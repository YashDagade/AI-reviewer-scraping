{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper for ICLR Papers through open review\n",
    "\n",
    "#TODO Improve the parsing logic for the introduction section - besides that I think it works grreat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementNotInteractableException,\n",
    ")\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from pdfminer.high_level import extract_text\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    filename='scraper.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    filename='scraper.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://openreview.net\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; DataScraper/1.0; +https://yourdomain.com/)\"\n",
    "}\n",
    "DOWNLOAD_DIR = \"ICLR_Papers_Scrape\"\n",
    "PDF_DIR = os.path.join(DOWNLOAD_DIR, \"PDFs\")\n",
    "MARKDOWN_DIR = os.path.join(DOWNLOAD_DIR, \"Markdown\")\n",
    "IMAGES_DIR = os.path.join(DOWNLOAD_DIR, \"Images\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [DOWNLOAD_DIR, PDF_DIR, MARKDOWN_DIR, IMAGES_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def setup_selenium():\n",
    "    \"\"\"Sets up Selenium with headless Chrome.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    # Initialize Service with ChromeDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    \n",
    "    # Initialize WebDriver with Service and Options\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def fetch_html(driver, url, timeout=30):\n",
    "    \"\"\"Fetches the fully rendered HTML content of a given URL using Selenium.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Fetching URL: {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait until the main content is loaded\n",
    "        WebDriverWait(driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"note\"))\n",
    "        )\n",
    "        \n",
    "        # Optional: Scroll to the bottom to ensure all lazy-loaded content is fetched\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for additional content to load\n",
    "        \n",
    "        html = driver.page_source\n",
    "        logging.info(f\"Successfully fetched URL: {url}\")\n",
    "        return html\n",
    "    except TimeoutException:\n",
    "        logging.error(f\"Timeout while loading {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_paper_info(soup):\n",
    "    \"\"\"Parses the paper's BeautifulSoup object to extract metadata.\"\"\"\n",
    "    paper_info = {}\n",
    "\n",
    "    # Extract Title\n",
    "    title_tag = soup.find('h2', class_='citation_title')\n",
    "    paper_info['title'] = title_tag.text.strip() if title_tag else \"N/A\"\n",
    "\n",
    "    # Extract Authors\n",
    "    authors_tag = soup.find('div', class_='forum-authors')\n",
    "    if authors_tag:\n",
    "        authors = [author.text.strip() for author in authors_tag.find_all('a')]\n",
    "        paper_info['authors'] = authors\n",
    "    else:\n",
    "        paper_info['authors'] = []\n",
    "\n",
    "    # Extract Publication Date\n",
    "    pub_date_tag = soup.find('span', class_='glyphicon-calendar')\n",
    "    if pub_date_tag and pub_date_tag.parent:\n",
    "        dates_text = pub_date_tag.parent.text.strip()\n",
    "        publication_date = re.search(r'Published:\\s*(.*?)(?:,|$)', dates_text)\n",
    "        paper_info['publication_date'] = publication_date.group(1) if publication_date else \"N/A\"\n",
    "    else:\n",
    "        paper_info['publication_date'] = \"N/A\"\n",
    "\n",
    "    # # Extract Decision - do it in reviewer responses instead\n",
    "    # decision = \"N/A\"\n",
    "    # decision_sections = soup.find_all('div', class_='note', attrs={'data-id': re.compile('.*')})\n",
    "    # for section in decision_sections:\n",
    "    #     heading = section.find('h4')\n",
    "    #     if heading and 'Paper Decision' in heading.text:\n",
    "    #         decision_field = section.find('strong', string='Decision:')\n",
    "    #         if decision_field and decision_field.next_sibling:\n",
    "    #             decision = decision_field.next_sibling.strip()\n",
    "    #             break\n",
    "    # paper_info['decision'] = decision\n",
    "\n",
    "    # Extract PDF URL\n",
    "    pdf_link_tag = soup.find('a', class_='citation_pdf_url')\n",
    "    if pdf_link_tag and 'href' in pdf_link_tag.attrs:\n",
    "        pdf_url = pdf_link_tag['href']\n",
    "        if not pdf_url.startswith('http'):\n",
    "            pdf_url = BASE_URL + pdf_url\n",
    "        paper_info['pdf_url'] = pdf_url\n",
    "    else:\n",
    "        paper_info['pdf_url'] = None\n",
    "\n",
    "    return paper_info\n",
    "\n",
    "def download_pdf(pdf_url, save_path):\n",
    "    \"\"\"Downloads the PDF from the given URL to the specified path.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Downloading PDF: {pdf_url}\")\n",
    "        response = requests.get(pdf_url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        logging.info(f\"Downloaded PDF to {save_path}\")\n",
    "        return True\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error downloading PDF from {pdf_url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_sections_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts the abstract and introduction from the PDF.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Extracting sections from PDF: {pdf_path}\")\n",
    "        text = extract_text(pdf_path)\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Regex patterns to accurately capture Abstract and Introduction\n",
    "        #TODO - add more patterns to capture Introduction \n",
    "        abstract_match = re.search(\n",
    "            r'(?is)abstract\\s*(.*?)\\s*(?:(introduction|1\\.\\s*Introduction|2\\.\\s*Methods|methods|conclusion|related work|acknowledgments|references|$))',\n",
    "            text\n",
    "        )\n",
    "        # introduction_match = re.search(\n",
    "        #     r'(?is)(introduction|1\\.\\s*Introduction)\\s*(.*?)\\s*(?:(conclusion|related work|methods|acknowledgments|references|2\\.\\s*Methods|$))',\n",
    "        #     text\n",
    "        # )\n",
    "        \n",
    "        \n",
    "        introduction_match = re.search(r'(?is)(introduction|1\\.\\s*Introduction)\\s*(.*?)\\s*(?:(\\n2\\s)|$)', text, re.DOTALL)\n",
    "\n",
    "        abstract = abstract_match.group(1).strip() if abstract_match else \"N/A\"\n",
    "        introduction = introduction_match.group(2).strip() if introduction_match else \"N/A\"\n",
    "\n",
    "        logging.info(f\"Extracted Abstract and Introduction from {pdf_path}\")\n",
    "        return abstract, introduction\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
    "        return \"N/A\", \"N/A\"\n",
    "\n",
    "def convert_to_markdown(text, header):\n",
    "    \"\"\"Converts plain text to Markdown with a specified header.\"\"\"\n",
    "    if text == \"N/A\":\n",
    "        markdown = f\"## {header}\\n\\nN/A\\n\"\n",
    "    else:\n",
    "        markdown = f\"## {header}\\n\\n{text}\\n\"\n",
    "    return markdown\n",
    "\n",
    "def save_markdown(content, filename):\n",
    "    \"\"\"Saves the given content to a Markdown file.\"\"\"\n",
    "    try:\n",
    "        path = os.path.join(MARKDOWN_DIR, filename)\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        logging.info(f\"Saved Markdown to {path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving Markdown file {filename}: {e}\")\n",
    "\n",
    "def extract_reviewer_responses(soup):\n",
    "    \"\"\"Extracts reviewer responses, comments, and other interactions.\"\"\"\n",
    "    responses = []\n",
    "\n",
    "    # Find all notes (reviews, meta-reviews, official comments)\n",
    "    note_divs = soup.find_all('div', class_='note', attrs={'data-id': re.compile('.*')})\n",
    "\n",
    "    for note in note_divs:\n",
    "        # Determine the type of note\n",
    "        invitation = note.find('span', class_='invitation')\n",
    "        if not invitation:\n",
    "            continue\n",
    "        invitation_type = invitation.text.strip()\n",
    "\n",
    "        # Extract author\n",
    "        signatures_span = note.find('span', class_='signatures')\n",
    "        author = \"N/A\"\n",
    "        if signatures_span:\n",
    "            # The last span usually contains the author\n",
    "            author_tags = signatures_span.find_all('span')\n",
    "            if author_tags:\n",
    "                author = author_tags[-1].text.strip()\n",
    "            else:\n",
    "                # Fallback if no span found\n",
    "                author = signatures_span.text.strip()\n",
    "        \n",
    "        # Extract content fields\n",
    "        content_div = note.find('div', class_='note-content')\n",
    "        content_dict = {}\n",
    "        if content_div:\n",
    "            # Each content field is typically within a div\n",
    "            content_fields = content_div.find_all('div', recursive=False)\n",
    "            for field in content_fields:\n",
    "                # Field name\n",
    "                field_name_tag = field.find('strong', class_='note-content-field')\n",
    "                if not field_name_tag:\n",
    "                    continue\n",
    "                field_name = field_name_tag.text.strip(':').strip()\n",
    "                # Field value\n",
    "                field_value_div = field.find('div', class_='note-content-value')\n",
    "                field_value_span = field.find('span', class_='note-content-value')\n",
    "                field_value = \"\"\n",
    "                if field_value_div:\n",
    "                    field_value = md(str(field_value_div)).strip()\n",
    "                elif field_value_span:\n",
    "                    field_value = md(str(field_value_span)).strip()\n",
    "                content_dict[field_name] = field_value\n",
    "\n",
    "        # If the note has nested replies (official comments by authors), handle them separately\n",
    "        nested_comments = note.find_all('div', class_='note', attrs={'data-id': re.compile('.*')})\n",
    "        for comment in nested_comments:\n",
    "            comment_invitation = comment.find('span', class_='invitation')\n",
    "            if comment_invitation and 'Official Comment' in comment_invitation.text.strip():\n",
    "                comment_author = \"Authors\"\n",
    "                comment_content_div = comment.find('div', class_='note-content')\n",
    "                comment_content = md(str(comment_content_div)) if comment_content_div else \"N/A\"\n",
    "                responses.append({\n",
    "                    'type': 'Official Comment',\n",
    "                    'author': comment_author,\n",
    "                    'content': {'Comment': comment_content}\n",
    "                })\n",
    "\n",
    "        # Avoid duplicating comments by checking if it's already appended\n",
    "        if not (invitation_type == 'Official Comment' and author == 'Authors'):\n",
    "            responses.append({\n",
    "                'type': invitation_type,\n",
    "                'author': author,\n",
    "                'content': content_dict\n",
    "            })\n",
    "\n",
    "    return responses\n",
    "\n",
    "def save_reviewer_responses(responses, filename):\n",
    "    \"\"\"Saves reviewer responses to a Markdown file.\"\"\"\n",
    "    try:\n",
    "        content = f\"## Reviewer Responses\\n\\n\"\n",
    "        for idx, response in enumerate(responses, 1):\n",
    "            content += f\"### {response['type']} {idx}\\n\"\n",
    "            content += f\"**Author:** {response['author']}\\n\\n\"\n",
    "            for field_name, field_value in response['content'].items():\n",
    "                content += f\"**{field_name}:**\\n{field_value}\\n\\n\"\n",
    "            content += \"\\n\"\n",
    "        save_markdown(content, filename)\n",
    "        logging.info(f\"Saved reviewer responses to {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving reviewer responses to {filename}: {e}\")\n",
    "\n",
    "def save_paper_metadata(paper_info, filename):\n",
    "    \"\"\"Saves paper metadata to a Markdown file.\"\"\"\n",
    "    try:\n",
    "        content = f\"# {paper_info['title']}\\n\\n\"\n",
    "        content += f\"**Authors:** {', '.join(paper_info['authors'])}\\n\\n\"\n",
    "        content += f\"**Publication Date:** {paper_info['publication_date']}\\n\\n\"\n",
    "        content += f\"**Decision:** {paper_info['decision']}\\n\\n\"\n",
    "        save_markdown(content, filename)\n",
    "        logging.info(f\"Saved metadata to {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving metadata to {filename}: {e}\")\n",
    "\n",
    "def process_paper(driver, paper_url):\n",
    "    \"\"\"Processes a single paper: downloads PDF, extracts sections, and scrapes HTML data.\"\"\"\n",
    "    logging.info(f\"Starting processing for paper: {paper_url}\")\n",
    "    html = fetch_html(driver, paper_url)\n",
    "    if not html:\n",
    "        logging.warning(f\"Failed to retrieve HTML for {paper_url}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    paper_info = parse_paper_info(soup)\n",
    "    paper_id_match = re.search(r'id=([A-Za-z0-9]+)', paper_url)\n",
    "    paper_id = paper_id_match.group(1) if paper_id_match else \"unknown\"\n",
    "\n",
    "    # Save metadata\n",
    "    metadata_filename = f\"{paper_id}_metadata.md\"\n",
    "    save_paper_metadata(paper_info, metadata_filename)\n",
    "\n",
    "    # Download PDF and extract sections\n",
    "    if paper_info['pdf_url']:\n",
    "        pdf_filename = f\"{paper_id}.pdf\"\n",
    "        pdf_path = os.path.join(PDF_DIR, pdf_filename)\n",
    "        success = download_pdf(paper_info['pdf_url'], pdf_path)\n",
    "        if success:\n",
    "            abstract, introduction = extract_sections_from_pdf(pdf_path)\n",
    "            abstract_md = convert_to_markdown(abstract, \"Abstract\")\n",
    "            introduction_md = convert_to_markdown(introduction, \"Introduction\")\n",
    "            combined_md = abstract_md + \"\\n\" + introduction_md\n",
    "            sections_filename = f\"{paper_id}_sections.md\"\n",
    "            save_markdown(combined_md, sections_filename)\n",
    "    else:\n",
    "        logging.warning(f\"No PDF URL found for {paper_url}. Skipping PDF download and extraction.\")\n",
    "\n",
    "    # Extract reviewer responses\n",
    "    responses = extract_reviewer_responses(soup)\n",
    "    if responses:\n",
    "        responses_filename = f\"{paper_id}_responses.md\"\n",
    "        save_reviewer_responses(responses, responses_filename)\n",
    "    else:\n",
    "        logging.info(f\"No reviewer responses found for {paper_url}.\")\n",
    "\n",
    "    logging.info(f\"Completed processing for paper ID: {paper_id}\")\n",
    "\n",
    "def process_papers_parallel(paper_urls, max_workers=4):\n",
    "    \"\"\"Processes multiple papers in parallel.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Each thread needs its own Selenium WebDriver\n",
    "        futures = []\n",
    "        for url in paper_urls:\n",
    "            driver = setup_selenium()\n",
    "            future = executor.submit(process_paper, driver, url)\n",
    "            futures.append((future, driver))\n",
    "        \n",
    "        for future, driver in futures:\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing a paper: {e}\")\n",
    "            finally:\n",
    "                driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with the scraper\n",
    "\n",
    "paper_urls = [\n",
    "    \"https://openreview.net/forum?id=KS8mIvetg2\",\n",
    "    \"https://openreview.net/forum?id=7Ttk3RzDeu\",\n",
    "    \"https://openreview.net/forum?id=ANvmVS2Yr0\",\n",
    "    \"https://openreview.net/forum?id=ekeyCgeRfC\"\n",
    "]\n",
    "\n",
    "\n",
    "process_papers_parallel(paper_urls, max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://openreview.net/forum?id=ANvmVS2Yr0', 'https://openreview.net/forum?id=7VPTUWkiDQ', 'https://openreview.net/forum?id=eepoE7iLpL', 'https://openreview.net/forum?id=iQHL76NqJT', 'https://openreview.net/forum?id=Iyve2ycvGZ', 'https://openreview.net/forum?id=SMZnJtkNX5', 'https://openreview.net/forum?id=lyoOWX0e0O', 'https://openreview.net/forum?id=yacRhge4zQ', 'https://openreview.net/forum?id=hcXfzlmg7Y', 'https://openreview.net/forum?id=buC4E91xZE', 'https://openreview.net/forum?id=hLT9cW4Afz', 'https://openreview.net/forum?id=IdibrApfps', 'https://openreview.net/forum?id=WNLAkjUm19', 'https://openreview.net/forum?id=lK2V2E2MNv', 'https://openreview.net/forum?id=Ebt7JgMHv1', 'https://openreview.net/forum?id=ApjY32f3Xr', 'https://openreview.net/forum?id=1FWDEIGm33', 'https://openreview.net/forum?id=ekeyCgeRfC', 'https://openreview.net/forum?id=BTKAeLqLMw', 'https://openreview.net/forum?id=BQvbL2sFQx', 'https://openreview.net/forum?id=IYxDy2jDFL', 'https://openreview.net/forum?id=5t57omGVMw', 'https://openreview.net/forum?id=dYjuJGTEbc', 'https://openreview.net/forum?id=7Ttk3RzDeu', 'https://openreview.net/forum?id=Ffjc8ApSbt', 'https://openreview.net/forum?id=hxAveMWogn', 'https://openreview.net/forum?id=RKw6AzP2BY', 'https://openreview.net/forum?id=tz9iIJtGgu', 'https://openreview.net/forum?id=23OEmHVkpq', 'https://openreview.net/forum?id=BXYZvcgVUv', 'https://openreview.net/forum?id=dER6OpDTnq', 'https://openreview.net/forum?id=7QlKLvfVge', 'https://openreview.net/forum?id=2DJMtdfgfH', 'https://openreview.net/forum?id=M6kpUtpQZQ', 'https://openreview.net/forum?id=E5CMyG6jl0', 'https://openreview.net/forum?id=AOSsLRKQrX', 'https://openreview.net/forum?id=H9DYMIpz9c', 'https://openreview.net/forum?id=hTEGyKf0dZ', 'https://openreview.net/forum?id=GzNaCp6Vcg', 'https://openreview.net/forum?id=o2IEmeLL9r', 'https://openreview.net/forum?id=9ceadCJY4B', 'https://openreview.net/forum?id=uNrFpDPMyo', 'https://openreview.net/forum?id=gYcft1HIaU', 'https://openreview.net/forum?id=rp5vfyp5Np', 'https://openreview.net/forum?id=kC5nZDU5zf', 'https://openreview.net/forum?id=cQgjz0mf0r', 'https://openreview.net/forum?id=bNt7oajl2a', 'https://openreview.net/forum?id=3SJE1WLB4M', 'https://openreview.net/forum?id=p4B7rl1UFA', 'https://openreview.net/forum?id=f3g5XpL9Kb', 'https://openreview.net/forum?id=vESNKdEMGp', 'https://openreview.net/forum?id=70PPJo3DwI', 'https://openreview.net/forum?id=AJBkfwXh3u', 'https://openreview.net/forum?id=kIZcruKmBg', 'https://openreview.net/forum?id=LzPWWPAdY4', 'https://openreview.net/forum?id=jUNSBetmAo', 'https://openreview.net/forum?id=LjivA1SLZ6', 'https://openreview.net/forum?id=gkfUvn0fLU', 'https://openreview.net/forum?id=M6XWoEdmwf', 'https://openreview.net/forum?id=TPZRq4FALB', 'https://openreview.net/forum?id=JHta7zuRNc', 'https://openreview.net/forum?id=BMZYh3IyAU', 'https://openreview.net/forum?id=oO6FsMyDBt', 'https://openreview.net/forum?id=csukJcpYDe', 'https://openreview.net/forum?id=oTRwljRgiv', 'https://openreview.net/forum?id=PdaPky8MUn', 'https://openreview.net/forum?id=10eQ4Cfh8p', 'https://openreview.net/forum?id=tQoGDHn2XO', 'https://openreview.net/forum?id=rhgIgTSSxW', 'https://openreview.net/forum?id=i8PjQT3Uig', 'https://openreview.net/forum?id=78iGZdqxYY', 'https://openreview.net/forum?id=NFqFA2vCQV', 'https://openreview.net/forum?id=xibcBSuuq0', 'https://openreview.net/forum?id=OkHHJcMroY', 'https://openreview.net/forum?id=iS7c9lkXuF', 'https://openreview.net/forum?id=JWrl5pJCnl', 'https://openreview.net/forum?id=Tigr1kMDZy', 'https://openreview.net/forum?id=mnyXZBa5dP', 'https://openreview.net/forum?id=H9D1revQeW', 'https://openreview.net/forum?id=4xXOc9nssp', 'https://openreview.net/forum?id=My7lkRNnL9', 'https://openreview.net/forum?id=UVSKuh9eK5', 'https://openreview.net/forum?id=anG8cNYQAs', 'https://openreview.net/forum?id=3DPTnFokLp', 'https://openreview.net/forum?id=JyXnsA8UAC', 'https://openreview.net/forum?id=IGzaH538fz', 'https://openreview.net/forum?id=4Ay23yeuz0', 'https://openreview.net/forum?id=w4abltTZ2f', 'https://openreview.net/forum?id=zX0LqTw80H', 'https://openreview.net/forum?id=7vVWiCrFnd', 'https://openreview.net/forum?id=AZGIwqCyYY', 'https://openreview.net/forum?id=fwCoLe3TAX', 'https://openreview.net/forum?id=miGpIhquyB', 'https://openreview.net/forum?id=aN4Jf6Cx69', 'https://openreview.net/forum?id=lOwkOIUJtx', 'https://openreview.net/forum?id=kmn0BhQk7p', 'https://openreview.net/forum?id=Rry1SeSOQL', 'https://openreview.net/forum?id=qBL04XXex6', 'https://openreview.net/forum?id=FMMF1a9ifL', 'https://openreview.net/forum?id=xuY33XhEGR']\n"
     ]
    }
   ],
   "source": [
    "def get_paper_urls_from_page(driver, page_url):\n",
    "    \"\"\"Extract all unique paper URLs from the given OpenReview page.\"\"\"\n",
    "    driver.get(page_url)\n",
    "    time.sleep(3)  # Give some time for page to load (adjust as necessary)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    paper_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Filter out URLs with '&noteId='\n",
    "    paper_urls = [link['href'] for link in paper_links if 'forum?id=' in link['href'] and '&noteId=' not in link['href']]\n",
    "    \n",
    "    return paper_urls\n",
    "\n",
    "def url_getter():\n",
    "    base_url = 'https://openreview.net/group?id=ICLR.cc/2024/Conference'  # The URL of the OpenReview ICLR page\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    all_paper_urls = []\n",
    "\n",
    " \n",
    "    for page_number in range(1):  \n",
    "        page_url = f\"{base_url}&page={page_number}\"\n",
    "        paper_urls = get_paper_urls_from_page(driver, page_url)\n",
    "        all_paper_urls.extend(paper_urls)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    # Create a list with the desired format\n",
    "    unique_urls = list(set(all_paper_urls))\n",
    "    formatted_urls = [f\"https://openreview.net{url}\" for url in unique_urls]\n",
    "    formatted_urls = formatted_urls[:100] # change the number to higher to get more\n",
    "    \n",
    "    return formatted_urls\n",
    "\n",
    "\n",
    "print(url_getter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this to get 100 different sample papers\n",
    "\n",
    "I ran it previously with 10 and the results are shown below - it took about 47 seconds to run 😭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_urls = url_getter()\n",
    "process_papers_parallel(paper_urls, max_workers=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
