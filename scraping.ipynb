{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementNotInteractableException,\n",
    ")\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from pdfminer.high_level import extract_text\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    filename='scraper.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://openreview.net\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; DataScraper/1.0; +https://yourdomain.com/)\"\n",
    "}\n",
    "DOWNLOAD_DIR = \"ICLR_Papers_Scrape\"\n",
    "PDF_DIR = os.path.join(DOWNLOAD_DIR, \"PDFs\")\n",
    "MARKDOWN_DIR = os.path.join(DOWNLOAD_DIR, \"Markdown\")\n",
    "IMAGES_DIR = os.path.join(DOWNLOAD_DIR, \"Images\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [DOWNLOAD_DIR, PDF_DIR, MARKDOWN_DIR, IMAGES_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "def setup_selenium():\n",
    "    \"\"\"Sets up Selenium with headless Chrome.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    # Initialize Service with ChromeDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    \n",
    "    # Initialize WebDriver with Service and Options\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def fetch_html(driver, url, timeout=30):\n",
    "    \"\"\"Fetches the fully rendered HTML content of a given URL using Selenium.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Fetching URL: {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait until the main content is loaded\n",
    "        WebDriverWait(driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"note\"))\n",
    "        )\n",
    "        \n",
    "        # Optional: Scroll to the bottom to ensure all lazy-loaded content is fetched\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for additional content to load\n",
    "        \n",
    "        html = driver.page_source\n",
    "        logging.info(f\"Successfully fetched URL: {url}\")\n",
    "        return html\n",
    "    except TimeoutException:\n",
    "        logging.error(f\"Timeout while loading {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_paper_info(soup):\n",
    "    \"\"\"Parses the paper's BeautifulSoup object to extract metadata.\"\"\"\n",
    "    paper_info = {}\n",
    "\n",
    "    # Extract Title\n",
    "    title_tag = soup.find('h2', class_='citation_title')\n",
    "    paper_info['title'] = title_tag.text.strip() if title_tag else \"N/A\"\n",
    "\n",
    "    # Extract Authors\n",
    "    authors_tag = soup.find('div', class_='forum-authors')\n",
    "    if authors_tag:\n",
    "        authors = [author.text.strip() for author in authors_tag.find_all('a')]\n",
    "        paper_info['authors'] = authors\n",
    "    else:\n",
    "        paper_info['authors'] = []\n",
    "\n",
    "    # Extract Publication Date\n",
    "    pub_date_tag = soup.find('span', class_='glyphicon-calendar')\n",
    "    if pub_date_tag and pub_date_tag.parent:\n",
    "        dates_text = pub_date_tag.parent.text.strip()\n",
    "        publication_date = re.search(r'Published:\\s*(.*?)(?:,|$)', dates_text)\n",
    "        paper_info['publication_date'] = publication_date.group(1) if publication_date else \"N/A\"\n",
    "    else:\n",
    "        paper_info['publication_date'] = \"N/A\"\n",
    "\n",
    "    # Extract Decision\n",
    "    decision = \"N/A\"\n",
    "    decision_sections = soup.find_all('div', class_='note', attrs={'data-id': re.compile('.*')})\n",
    "    for section in decision_sections:\n",
    "        heading = section.find('h4')\n",
    "        if heading and 'Paper Decision' in heading.text:\n",
    "            decision_field = section.find('strong', string='Decision:')\n",
    "            if decision_field and decision_field.next_sibling:\n",
    "                decision = decision_field.next_sibling.strip()\n",
    "                break\n",
    "    paper_info['decision'] = decision\n",
    "\n",
    "    # Extract PDF URL\n",
    "    pdf_link_tag = soup.find('a', class_='citation_pdf_url')\n",
    "    if pdf_link_tag and 'href' in pdf_link_tag.attrs:\n",
    "        pdf_url = pdf_link_tag['href']\n",
    "        if not pdf_url.startswith('http'):\n",
    "            pdf_url = BASE_URL + pdf_url\n",
    "        paper_info['pdf_url'] = pdf_url\n",
    "    else:\n",
    "        paper_info['pdf_url'] = None\n",
    "\n",
    "    return paper_info\n",
    "\n",
    "def download_pdf(pdf_url, save_path):\n",
    "    \"\"\"Downloads the PDF from the given URL to the specified path.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Downloading PDF: {pdf_url}\")\n",
    "        response = requests.get(pdf_url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        logging.info(f\"Downloaded PDF to {save_path}\")\n",
    "        return True\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error downloading PDF from {pdf_url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_sections_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts the abstract and introduction from the PDF.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Extracting sections from PDF: {pdf_path}\")\n",
    "        text = extract_text(pdf_path)\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Improved regex patterns to accurately capture Abstract and Introduction\n",
    "        abstract_match = re.search(\n",
    "            r'(?is)abstract\\s*(.*?)\\s*(?:(introduction|1\\.\\s*Introduction|2\\.\\s*Methods|methods|conclusion|related work|acknowledgments|references|$))',\n",
    "            text\n",
    "        )\n",
    "        # introduction_match = re.search(\n",
    "        #     r'(?is)(introduction|1\\.\\s*Introduction)\\s*(.*?)\\s*(?:(conclusion|related work|methods|acknowledgments|references|2\\.\\s*Methods|$))',\n",
    "        #     text\n",
    "        # )\n",
    "        \n",
    "        \n",
    "        introduction_match = re.search(r'(?is)(introduction|1\\.\\s*Introduction)\\s*(.*?)\\s*(?:(\\n2\\s)|$)', text, re.DOTALL)\n",
    "\n",
    "        abstract = abstract_match.group(1).strip() if abstract_match else \"N/A\"\n",
    "        introduction = introduction_match.group(2).strip() if introduction_match else \"N/A\"\n",
    "\n",
    "        logging.info(f\"Extracted Abstract and Introduction from {pdf_path}\")\n",
    "        return abstract, introduction\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
    "        return \"N/A\", \"N/A\"\n",
    "\n",
    "def convert_to_markdown(text, header):\n",
    "    \"\"\"Converts plain text to Markdown with a specified header.\"\"\"\n",
    "    if text == \"N/A\":\n",
    "        markdown = f\"## {header}\\n\\nN/A\\n\"\n",
    "    else:\n",
    "        markdown = f\"## {header}\\n\\n{text}\\n\"\n",
    "    return markdown\n",
    "\n",
    "def save_markdown(content, filename):\n",
    "    \"\"\"Saves the given content to a Markdown file.\"\"\"\n",
    "    try:\n",
    "        path = os.path.join(MARKDOWN_DIR, filename)\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        logging.info(f\"Saved Markdown to {path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving Markdown file {filename}: {e}\")\n",
    "\n",
    "def extract_reviewer_responses(soup):\n",
    "    \"\"\"Extracts reviewer responses, comments, and other interactions.\"\"\"\n",
    "    responses = []\n",
    "\n",
    "    # Find all notes (reviews, meta-reviews, official comments)\n",
    "    note_divs = soup.find_all('div', class_='note', attrs={'data-id': re.compile('.*')})\n",
    "\n",
    "    for note in note_divs:\n",
    "        # Determine the type of note\n",
    "        invitation = note.find('span', class_='invitation')\n",
    "        if not invitation:\n",
    "            continue\n",
    "        invitation_type = invitation.text.strip()\n",
    "\n",
    "        # Extract author\n",
    "        signatures_span = note.find('span', class_='signatures')\n",
    "        author = \"N/A\"\n",
    "        if signatures_span:\n",
    "            # The last span usually contains the author\n",
    "            author_tags = signatures_span.find_all('span')\n",
    "            if author_tags:\n",
    "                author = author_tags[-1].text.strip()\n",
    "            else:\n",
    "                # Fallback if no span found\n",
    "                author = signatures_span.text.strip()\n",
    "        \n",
    "        # Extract content fields\n",
    "        content_div = note.find('div', class_='note-content')\n",
    "        content_dict = {}\n",
    "        if content_div:\n",
    "            # Each content field is typically within a div\n",
    "            content_fields = content_div.find_all('div', recursive=False)\n",
    "            for field in content_fields:\n",
    "                # Field name\n",
    "                field_name_tag = field.find('strong', class_='note-content-field')\n",
    "                if not field_name_tag:\n",
    "                    continue\n",
    "                field_name = field_name_tag.text.strip(':').strip()\n",
    "                # Field value\n",
    "                field_value_div = field.find('div', class_='note-content-value')\n",
    "                field_value_span = field.find('span', class_='note-content-value')\n",
    "                field_value = \"\"\n",
    "                if field_value_div:\n",
    "                    field_value = md(str(field_value_div)).strip()\n",
    "                elif field_value_span:\n",
    "                    field_value = md(str(field_value_span)).strip()\n",
    "                content_dict[field_name] = field_value\n",
    "\n",
    "        # If the note has nested replies (official comments by authors), handle them separately\n",
    "        nested_comments = note.find_all('div', class_='note', attrs={'data-id': re.compile('.*')})\n",
    "        for comment in nested_comments:\n",
    "            comment_invitation = comment.find('span', class_='invitation')\n",
    "            if comment_invitation and 'Official Comment' in comment_invitation.text.strip():\n",
    "                comment_author = \"Authors\"\n",
    "                comment_content_div = comment.find('div', class_='note-content')\n",
    "                comment_content = md(str(comment_content_div)) if comment_content_div else \"N/A\"\n",
    "                responses.append({\n",
    "                    'type': 'Official Comment',\n",
    "                    'author': comment_author,\n",
    "                    'content': {'Comment': comment_content}\n",
    "                })\n",
    "\n",
    "        # Avoid duplicating comments by checking if it's already appended\n",
    "        if not (invitation_type == 'Official Comment' and author == 'Authors'):\n",
    "            responses.append({\n",
    "                'type': invitation_type,\n",
    "                'author': author,\n",
    "                'content': content_dict\n",
    "            })\n",
    "\n",
    "    return responses\n",
    "\n",
    "def save_reviewer_responses(responses, filename):\n",
    "    \"\"\"Saves reviewer responses to a Markdown file.\"\"\"\n",
    "    try:\n",
    "        content = f\"## Reviewer Responses\\n\\n\"\n",
    "        for idx, response in enumerate(responses, 1):\n",
    "            content += f\"### {response['type']} {idx}\\n\"\n",
    "            content += f\"**Author:** {response['author']}\\n\\n\"\n",
    "            for field_name, field_value in response['content'].items():\n",
    "                content += f\"**{field_name}:**\\n{field_value}\\n\\n\"\n",
    "            content += \"\\n\"\n",
    "        save_markdown(content, filename)\n",
    "        logging.info(f\"Saved reviewer responses to {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving reviewer responses to {filename}: {e}\")\n",
    "\n",
    "def save_paper_metadata(paper_info, filename):\n",
    "    \"\"\"Saves paper metadata to a Markdown file.\"\"\"\n",
    "    try:\n",
    "        content = f\"# {paper_info['title']}\\n\\n\"\n",
    "        content += f\"**Authors:** {', '.join(paper_info['authors'])}\\n\\n\"\n",
    "        content += f\"**Publication Date:** {paper_info['publication_date']}\\n\\n\"\n",
    "        content += f\"**Decision:** {paper_info['decision']}\\n\\n\"\n",
    "        save_markdown(content, filename)\n",
    "        logging.info(f\"Saved metadata to {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving metadata to {filename}: {e}\")\n",
    "\n",
    "def process_paper(driver, paper_url):\n",
    "    \"\"\"Processes a single paper: downloads PDF, extracts sections, and scrapes HTML data.\"\"\"\n",
    "    logging.info(f\"Starting processing for paper: {paper_url}\")\n",
    "    html = fetch_html(driver, paper_url)\n",
    "    if not html:\n",
    "        logging.warning(f\"Failed to retrieve HTML for {paper_url}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    paper_info = parse_paper_info(soup)\n",
    "    paper_id_match = re.search(r'id=([A-Za-z0-9]+)', paper_url)\n",
    "    paper_id = paper_id_match.group(1) if paper_id_match else \"unknown\"\n",
    "\n",
    "    # Save metadata\n",
    "    metadata_filename = f\"{paper_id}_metadata.md\"\n",
    "    save_paper_metadata(paper_info, metadata_filename)\n",
    "\n",
    "    # Download PDF and extract sections\n",
    "    if paper_info['pdf_url']:\n",
    "        pdf_filename = f\"{paper_id}.pdf\"\n",
    "        pdf_path = os.path.join(PDF_DIR, pdf_filename)\n",
    "        success = download_pdf(paper_info['pdf_url'], pdf_path)\n",
    "        if success:\n",
    "            abstract, introduction = extract_sections_from_pdf(pdf_path)\n",
    "            abstract_md = convert_to_markdown(abstract, \"Abstract\")\n",
    "            introduction_md = convert_to_markdown(introduction, \"Introduction\")\n",
    "            combined_md = abstract_md + \"\\n\" + introduction_md\n",
    "            sections_filename = f\"{paper_id}_sections.md\"\n",
    "            save_markdown(combined_md, sections_filename)\n",
    "    else:\n",
    "        logging.warning(f\"No PDF URL found for {paper_url}. Skipping PDF download and extraction.\")\n",
    "\n",
    "    # Extract reviewer responses\n",
    "    responses = extract_reviewer_responses(soup)\n",
    "    if responses:\n",
    "        responses_filename = f\"{paper_id}_responses.md\"\n",
    "        save_reviewer_responses(responses, responses_filename)\n",
    "    else:\n",
    "        logging.info(f\"No reviewer responses found for {paper_url}.\")\n",
    "\n",
    "    logging.info(f\"Completed processing for paper ID: {paper_id}\")\n",
    "\n",
    "def process_papers_parallel(paper_urls, max_workers=4):\n",
    "    \"\"\"Processes multiple papers in parallel.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Each thread needs its own Selenium WebDriver\n",
    "        futures = []\n",
    "        for url in paper_urls:\n",
    "            driver = setup_selenium()\n",
    "            future = executor.submit(process_paper, driver, url)\n",
    "            futures.append((future, driver))\n",
    "        \n",
    "        for future, driver in futures:\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing a paper: {e}\")\n",
    "            finally:\n",
    "                driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of Paper URLs\n",
    "    paper_urls = [\n",
    "        \"https://openreview.net/forum?id=KS8mIvetg2\",\n",
    "        \"https://openreview.net/forum?id=7Ttk3RzDeu\",\n",
    "        \"https://openreview.net/forum?id=ANvmVS2Yr0\",\n",
    "        \"https://openreview.net/forum?id=ekeyCgeRfC\"\n",
    "    ]\n",
    "\n",
    "    # Start parallel processing\n",
    "    process_papers_parallel(paper_urls, max_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = requests.get(\"https://openreview.net/forum?id=KS8mIvetg2\")\n",
    "\n",
    "# save the response to a file\n",
    "\n",
    "with open(\"KS8mIvetg2.html\", \"w\") as f:\n",
    "    f.write(a.text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
