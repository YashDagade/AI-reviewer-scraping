## Abstract

Zero-shot anomaly detection (ZSAD) requires detection models trained using aux- iliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, e.g., data privacy, yet it is challenging since the models need to generalize to anoma- lies across different domains where the appearance of foreground objects, abnor- mal regions, and background features, such as defects/tumors on different prod- ucts/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of Anoma- lyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from vari- ous defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP. 1

## Introduction

Anomaly detection (AD) has been widely applied in various applications, such as industrial defect inspection (Bergmann et al., 2019; 2020; Liznerski et al., 2020; Pang et al., 2021a; Roth et al., 2022; Huang et al., 2022; Mou et al., 2022; Chen et al., 2022; You et al., 2022; Ding et al., 2022; Reiss & Hoshen, 2023; Xie et al., 2023; Zhou et al., 2023; Cao et al., 2023) and medical image analysis (Pang et al., 2021a; Tian et al., 2021; Fernando et al., 2021; Qin et al., 2022; Ding et al., 2022; Liu et al., 2023; Tian et al., 2023). Existing AD approaches typically assume that training examples in a target application domain are available for learning the detection models (Pang et al., 2021b; Ruff et al., 2021). However, this assumption may not hold in various scenarios, such as i) when accessing training data violates data privacy policies (e.g., to protect the sensitive information of patients), or ii) when the target domain does not have relevant training data (e.g., inspecting defects in a manufacturing line of new products). Zero-shot anomaly detection (ZSAD) is an emerging task for AD in such scenarios, to which the aforementioned AD approaches are not viable, as it requires detection models to detect anomalies without any training sample in a target dataset. Since anomalies from different application scenarios typically have substantial variations in their visual appearance, foreground objects, and background features, e.g., defects on the surface of one product vs. that on the other products, lesions/tumors on different organs, or industrial defects tumors/lesions in medical images, detection models with strong generalization ability w.r.t. vs. such variations are needed for accurate ZSAD. Recently large pre-trained vision-language models (VLMs) (Radford et al., 2021; Kirillov et al., 2023) have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection (Jeong et al., 2023). Particularly, being ∗Equal contribution. † Corresponding authors. 1 Published as a conference paper at ICLR 2024 (a) (b) (c) (d) (e) (f) Figure 1: Comparison of ZSAD results on (b) test data using (c) original text prompts in CLIP (Radford et al., 2021), (d) tailored text prompts for AD in WinCLIP (Jeong et al., 2023), (e) learnable text prompts for general vision tasks in CoOp (Zhou et al., 2022a), and (f) object-agnostic text prompts in our AnomalyCLIP. (a) presents a set of auxiliary data we can use to learn the text prompts. The results are obtained by measuring the similarity between text prompt embeddings and image embeddings. The ground-truth anomaly regions are circled in red in (a) and (b). (c), (d), and (e) suffer from poor generalization across different domains, while our AnomalyCLIP in (f) can well generalize to anomalies in diverse types of objects from different domains. pre-trained using millions/billions of image-text pairs, CLIP (Radford et al., 2021) has been applied to empower various downstream tasks (Zhou et al., 2022b; Rao et al., 2022; Khattak et al., 2023; Sain et al., 2023) with its strong generalization capability. WinCLIP (Jeong et al., 2023) is a seminal work in the ZSAD line, which designs a large number of artificial text prompts to exploit the CLIP’s generalizability for ZSAD. However, the VLMs such as CLIP are primarily trained to align with the class semantics of foreground objects rather than the abnormality/normality in the images, and as a result, their generalization in understanding the visual abnormality/normality is restricted, leading to weak ZSAD performance. Further, the current prompting approaches, using either manually defined text prompts (Jeong et al., 2023) or learnable prompts (Sun et al., 2022; Zhou et al., 2022a), often result in prompt embeddings that opt for global features for effective object semantic alignment (Zhong et al., 2022; Wu et al., 2023), failing to capture the abnormality that often manifests in fine-grained, local features, as shown in Fig. 1d and Fig. 1e. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. AnomalyCLIP aims to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. It first devises a simple yet universally-effective learnable prompt template for the two general classes – normality and abnormality – and then utilizes both image-level and pixel-level loss functions to learn the generic normality and abnormality globally and locally in our prompt embeddings using auxiliary data. This allows our model to focus on the abnormal image regions rather the object semantics, enabling remarkable zero-shot capability of recognizing the abnormality that has similar abnormal patterns to those in auxiliary data. As shown in Fig. 1a and Fig. 1b, the foreground object semantics can be completely different in the fine-tuning auxiliary data and target data, but the anomaly patterns remain similar, e.g., scratches on metal nuts and plates, the misplacement of transistors and PCB, tumors/lesions on various organ surfaces, etc. Text prompt embeddings in CLIP fail to generalize across different domains, as illustrated in Fig. 1c, but object-agnostic prompt embeddings learned by AnomalyCLIP can effectively generalize to recognize the abnormality across different domain images in Fig. 1f. In summary, this paper makes the following main contributions. • We reveal for the first time that learning object-agnostic text prompts of normality and abnormality is a simple yet effective approach for accurate ZSAD. Compared to current text prompting approaches that are primarily designed for object semantic alignment (Zhou et al., 2022b; Jeong et al., 2023), our text prompt embeddings model semantics of generic abnormality and normality, allowing object-agnostic, generalized ZSAD performance. • We then introduce a novel ZSAD approach, called AnomalyCLIP, in which we utilize an object-agnostic prompt template and a glocal abnormality loss function (i.e., a combination of global and local loss functions) to learn the generic abnormality and normality prompts using auxiliary data. In doing so, AnomalyCLIP largely simplifies the prompt design and can effectively apply to different domains without requiring any change on its learned two 2 Capsule (MVTecAD)Metal nut (MVTecAD)Transistor(MVTecAD)Tile(MVTecAD)Leather(MVTecAD)Carpet(MVTecAD)Auxiliary dataClass1(DAGM)Colon(ColonDB)PCB(Visa)Skin(ISIC)Metal plate(Visa)Brian(Br35H)Test dataOriginal text promptsSimilarity map（CLIP ）Similarity map（WinCLIP）Tailored text promptsSimilarity map（CoOp）Learnable text promptsSimilarity map（AnomalyCLIP）Object-agnostic text prompts Published as a conference paper at ICLR 2024 Figure 2: Overview of AnomalyCLIP. To adapt CLIP to ZSAD, AnomalyCLIP introduces object- agnostic text prompt templates to capture generic normality and abnormality regardless of the object semantics. Then, we introduce glocal context optimization to incorporate global and fine-grained anomaly semantics into object-agnostic text prompt learning. Finally, textual prompt tuning and DPAM are used to enable the prompt learning in the textual and local visual spaces of CLIP. prompts, contrasting to existing methods like WinCLIP whose effectiveness relies heavily on extensive engineering on hundreds of manually defined prompts. • Comprehensive experiments on 17 datasets from various industrial and medical domains demonstrate that AnomalyCLIP achieves superior ZSAD performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from defect inspection and medical imaging domains. 2 PRELIMINARY CLIP consists of a text encoder and visual encoder denoted as T (·) and F (·), respectively. Both encoders are mainstream multi-layer networks such as ViT (Vaswani et al., 2017; Dosovitskiy et al., 2020). Using text prompts is a typical way to achieve the embeddings of different classes for zero- shot recognition. Particularly, a text prompt template G with the class name c can be passed through T (·) to obtain its corresponding textual embedding gc ∈ RD. The text prompt template commonly used in CLIP looks like A photo of a [cls], where [cls] represents the target class name. Then F (·) encodes an image xi to derive visual representations, where the class token fi ∈ RD i ∈ RH×W ×D is treated as its visual embedding (global visual embedding), and patch tokens f m are referred to as local visual embeddings. CLIP performs zero-shot recognition by measuring the similarity between textual and visual embeddings. In specific, given a target class set C and an image xi, CLIP predicts the probability of xi belonging to c as follows: p(y = c|xi) = P (gc, fi) = , (1) exp(< gc, fi > /τ ) c∈C exp(< gc, fi >)/τ ) (cid:80) where τ is a temperature hyperparameter, and the operator < ·, · > represents the computation of co- sine similarity. Unlike many vision tasks that involve many objects and use the name of the objects as the class name [cls], we posit that performing ZSAD tasks using CLIP should be object-agnostic, so we propose to design two classes of text prompts (i.e., normality and abnormality) and compute the possibility of these two classes according to Eq. 1. We denote the probability of being abnormal P (ga, fi) as the anomaly score. The computation is extended from global visual embeddings to local visual embeddings to derive the corresponding segmentation maps Sn ∈ RH×W and Sa ∈ RH×W , where each entry (j, k) are computed as P (gn, f m(j,k) ) and P (ga, f m(j,k) ). i i 3 ANOMALYCLIP: OBJECT-AGNOSTIC PROMPT LEARNING 3.1 APPROACH OVERVIEW In this paper, we propose AnomalyCLIP to adapt CLIP to ZSAD via object-agnostic prompt learn- ing. As shown in Fig. 2, AnomalyCLIP first introduces object-agnostic text prompt templates, where 3 Similarityscore…Layer NText encoderMLglobalVisual embeddingLocal visual embeddingAuxiliary imageLlocal…Layer 1……Layer 2…Textual embeddingGround truthLayer DPAM layerCosine similarityElement-wise sumMMaximumLearnableFrozenSimilarity map…V1objectVE…damaged objectW1WEgnNormal text promptgaAnomalous text promptVision encoder...Layer Layer Block 1......Layer Layer Block 1...Layer Layer Block 2...Layer Layer Block 4...Layer Layer Block 4...Layer Original layerLayer Layer Block 2...Object-agnostic learnable text prompt... Published as a conference paper at ICLR 2024 we design two generic object-agnostic text prompt templates of gn and ga to learn generalized em- bedding for the normality and abnormality classes, respectively (see Sec. 3.2). To learn such generic text prompt templates, we introduce global and local context optimization to incorporate global and fine-grained anomaly semantics into object-agnostic textual embedding learning. In addition, tex- tual prompt tuning and DPAM are used to support the learning in the textual and local visual spaces of CLIP. Finally, we integrate the multiple intermediate layers to provide more local visual details. During training, all modules are jointly optimized by the combination of global and local context optimization. During inference, we quantify the misalignment of textual and global/local visual embeddings to obtain the anomaly score and anomaly score map, respectively (see Sec. 3.3). 3.2 OBJECT-AGNOSTIC TEXT PROMPT DESIGN Commonly used text prompt templates in CLIP, like A photo of a [cls], primarily focus on object semantics. Consequently, they fail to generate textual embeddings that capture anomaly and normal semantics to query corresponding visual embeddings. To support the learning of anomaly-discriminative textual embeddings, we aim to incorporate prior anomaly semantics into text prompt templates. A trivial solution is to design the templates with specific anomaly types, such as A photo of a [cls] with scratches. However, the pattern of anomaly is typically unknown and diverse, so it is practically difficult to list all possible anomaly types. Therefore, it is important to define text prompt templates with generic anomaly semantics. For this purpose, we can adopt the text damaged [cls] to cover comprehensive anomaly semantics, facilitating the detection of diverse defects such as scratches and holes. Nevertheless, utilizing such text prompt templates poses challenges in generating generic anomaly-discriminating textual embeddings. This is because CLIP’s original pre-training focuses on aligning with object semantics instead of the ab- normality and normality within images. To address this limitation, we can introduce learnable text prompt templates and tune the prompts using auxiliary AD-relevant data. During the fine-tuning process, these learnable templates can incorporate both broad and detailed anomaly semantics, re- sulting in textual embeddings that are more discriminative between normality and abnormality. This helps avoid the need for manually defined text prompt templates that require extensive engineer- ing (Jeong et al., 2023). These text prompts are referred to as object-aware text prompt templates and defined as follows: gn = [V1][V2] . . . [VE][cls] ga = [W1][W2] . . . [WE][damaged][cls], where [V ]i and [W ]i (i ∈ {1, . . . , E}) are learnable word embeddings in normality and abnormality text prompt templates, respectively. ZSAD tasks require models to detect anomalies in previously unseen target datasets. These datasets often exhibit significant variations in object semantics among different objects, like various defects on one product vs. another, or discrepancies between industrial defects and medical imaging tu- mors. However, despite these substantial differences in object semantics, the underlying anomaly patterns could be similar. For instance, anomalies like scratches on metal nuts and plates, or the misplacement of transistors and PCB, as well as tumors on the surface of various organs, can share similar anomaly patterns. We hypothesize that the key of accurate ZSAD is to identify these generic anomaly patterns regardless of the varying semantics of different objects. Therefore, the inclusion of object semantics in object-aware text prompt templates is often unnecessary for ZSAD. It can even hinder the detection of anomalies in classes that have not been seen during the learning process. More importantly, excluding the object semantics from text prompt templates allows learnable text prompt templates to focus on capturing the characteristics of anomalies themselves, rather than the objects. Motivated by this, we introduce object-agnostic prompt learning, with the aim to capture generic normality and abnormality within images regardless of the object semantics. Different from object-aware text prompt templates, as shown below, the object-agnostic text prompt templates replace the class name in gn and ga with object, blocking out the class semantics of objects: gn = [V1][V2] . . . [VE][object] ga = [W1][W2] . . . [WE][damaged][object]. This design empowers the object-agnostic text prompt template to learn the shared patterns of dif- ferent anomalies. As a result, the generated textual embeddings are more generic and capable of identifying anomalies across diverse objects and different domains. Further, this prompt design is versatile and can be applied to different target domains without any modification, e.g., requiring no knowledge about the object name or anomaly types in a target dataset. 4 Published as a conference paper at ICLR 2024 3.3 LEARNING GENERIC ABNORMALITY AND NORMALITY PROMPTS Glocal context optimization To effectively learn the object-agnostic text prompts, we devise a joint optimization approach that enables the normality and abnormality prompt learning from both global and local perspectives, namely global and local context optimization. The global context op- timization aims to enforce that our object-agnostic textual embeddings are matched with the global visual embeddings of images of diverse objects. This helps effectively capture the normal/abnormal semantics from a global feature perspective. The local context optimization is introduced to enable object-agnostic text prompts to concentrate on fine-grained, local abnormal regions from M inter- mediate layers of the visual encoder, in addition to the global normal/abnormal features. Formally, let M be the set of intermediate layers used (i.e., M = |M|), our text prompts are learned by minimizing the following glocal loss function: Ltotal = Lglobal + λ (cid:80) Mc∈M LMc local, (2) S(j,k) n,Mc = P (gn, f m(j,k) where λ is a hyperparameter to balance the global and local losses. Lglobal is a cross-entropy loss that matches the cosine similarity between the object-agnostic textual embeddings and visual embed- dings of normal/abnormal images from auxiliary data. Let S ∈ RHimage×Wimage be the ground-truth segmentation mask, with Sjk = 1 if the pixel is as an anomaly and Sjk = 0 otherwise, then we have = P (ga, f m(j,k) Llocal = F ocal(U p([Sn,Mc , Sa,Mc]), S) + Dice(U p(Sn,Mc ), I − S) + Dice(U p(Sa,Mc), S), where F ocal(·, ·) and Dice(·, ·) denote a focal loss (Lin et al., 2017) and a Dice loss (Li et al., 2019) respectively. The operators U p(·) and [·, ·] represent the unsampling and concatenation along with the channel, and I represents the full-one matrix. Since the anomalous regions are typically smaller than the normal ones, we use focal loss to address the imbalance problem. Furthermore, to ensure that the model establishes an accurate decision boundary, we employ the Dice loss to measure the overlaps between the predicted segmentation U p(Sn,Mc)/U p(Sa,Mc) and the ground truth mask. ), where j ∈ [1, H], k ∈ [1, W ] ), S(j,k) a,Mc i,Mc i,Mc Refinement of the textual space To facilitate the learning of a more discriminative textual space via Eq. 2, inspired by Jia et al. (2022) and Khattak et al. (2023), we introduce text prompt tuning to refine the original textual space of CLIP by adding additional learnable token embeddings into its text encoder. Specifically, we first attach randomly initialized learnable token embeddings t′ m into Tm, the m-th layer of the frozen CLIP text encoder. Then, we concatenate t′ m and the original token embeddings tm along the dimension of the channel, and forward them to Tm to get the corresponding r′ m+1 and tm+1. To ensure proper calibration, we discard the obtained r′ m+1 and initialize new learnable token embeddings t′ m+1 is discarded, the updated gradients can still be backpropagated to optimize the learnable tokens t′ m due to the self-attention mechanism. We repeat this operation until we reach the designated layer M ′. During fine-tuning, these learnable token embeddings are optimized to refine the original textual space. More details see Appendix D. m+1. Note that even though the output r′ Refinement of the local visual space Since the visual encoder of CLIP is originally pre-trained to align global object semantics, the contrastive loss used in CLIP makes the visual encoder produce a representative global em- bedding for class recognition. Through the self-attention mechanism, the attention map in the visual encoder fo- cuses on the specific tokens highlighted within the red rectangle in Fig 3b. Although these tokens may contribute to global object recognition, they disrupt the local visual semantics, which directly hinders the effective learning of the fine-grained abnormality in our object-agnostic text prompts (Li et al., 2023b). We found empirically that a diagonally prominent attention map helps reduce the disturbance from other tokens, leading to im- proved local visual semantics. Therefore, we propose a mechanism called Diagonally Prominent Attention Map to refine the local visual space, with the visual encoder kept frozen during training. To this end, we replace the original Q-K attention in the visual encoder with diagonally prominent attention, such as Q-Q, K-K, and V -V self-attention schemes. As demonstrated in Fig.3c, Fig.3d, and Fig. 3e, the refined DPAM attention maps are more diagonally prominent, resulting in sub- stantially improved segmentation maps in both original CLIP and our AnomalyCLIP. Compared to Figure 3: DPAM visualization. (b) Q-K attention (d) K-K attention (c) Q-Q attention (e) V-V attention (a) Input 5 InputSimilarity map(CLIP)Q-K attention mapMarker 1Similarity map(AnomalyCLIP)Similarity map(CLIP)Q-Q attention mapSimilarity map(AnomalyCLIP)Similarity map(CLIP)K-K attention mapSimilarity map(AnomalyCLIP)Similarity map(CLIP)V-V attention mapSimilarity map(AnomalyCLIP) Published as a conference paper at ICLR 2024 CLIP that is based on global features and manually defined text prompts, the text prompts learned by AnomalyCLIP are more fine-grained, enabling substantially more accurate alignment between the normality/abnormality prompt embeddings and the local visual embeddings across four different self-attention schemes. This, in turn, allows AnomalyCLIP to generate accurate Sn and Sa for the joint optimization in Eq. 2. Unless otherwise specified, AnomalyCLIP utilizes V -V self-attention due to its superior overall performance. The performance of different self-attention mechanisms is analyzed in Sec. D. We also provide a detailed explanation about DPAM in Appendix C. Training and Inference During training, AnomalyCLIP minimizes the loss in Eq. 2 using an auxiliary AD-related dataset. As for inference, given a test image xi, we use the similarity score P (ga, fi) as the image-level anomaly score, with the anomaly score leaning toward one when the anomaly textual embedding ga is aligned with global visual embedding fi. For pixel-wise pre- dictions, we merge the segmentation Sn,Mc and Sa,Mc of all selected intermediate layers, fol- lowed by an interpolation and smoothing operation. Formally, our anomaly score map M ap ∈ RHimage×Wimage is computed as M ap = Gσ((cid:80) Mc∈M( 1 2 U p(Sa,Mc ))), where Gσ represents a Gaussian filter, and σ controls smoothing. 2 (I −U p(Sn,Mc))+ 1 4 EXPERIMENTS 4.1 EXPERIMENT SETUP Datasets and Evaluation Metrics We conducted extensive experiments on 17 publicly available datasets, covering various industrial inspection scenarios and medical imaging domains (including photography, endoscopy, and radiology) to evaluate the performance of AnomalyCLIP. In industrial inspection, we consider MVTec AD (Bergmann et al., 2019), VisA (Zou et al., 2022), MPDD (Jezek et al., 2021), BTAD (Mishra et al., 2021), SDD (Tabernik et al., 2020), DAGM (Wieler & Hahn, 2007), and DTD-Synthetic (Aota et al., 2023). In medical imaging, we consider skin cancer de- tection dataset ISIC (Gutman et al., 2016), colon polyp detection datasets CVC-ClinicDB (Bernal et al., 2015), CVC-ColonDB (Tajbakhsh et al., 2015), Kvasir (Jha et al., 2020), and Endo (Hicks et al., 2021), thyroid nodule detection dataset TN3k (Gong et al., 2021), brain tumor detection datasets HeadCT (Salehi et al., 2021), BrainMRI (Salehi et al., 2021), Br35H (Hamada., 2020), and COVID-19 detection dataset COVID-19 (Chowdhury et al., 2020; Rahman et al., 2021). The SOTA competing methods include CLIP (Radford et al., 2021), CLIP-AC (Radford et al., 2021), Win- CLIP (Jeong et al., 2023), VAND (Chen et al., 2023), and CoOp (Zhou et al., 2022b). We provide more details about the methods and data pre-processing in Appendix A. The anomaly detection per- formance is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUROC). Additionally, average precision (AP) for anomaly detection and AUPRO (Bergmann et al., 2020) for anomaly segmentation are also used to provide more in-depth analysis of the performance. Implementation details We use the publicly available CLIP model1 (VIT-L/14@336px) as our backbone. Model parameters of CLIP are all frozen. The length of learnable word embeddings E is set to 12. The learnable token embeddings are attached to the first 9 layers of the text encoder for refining the textual space, and their length in each layer is set to 4. We fine-tune AnomalyCLIP using the test data on MVTec AD and evaluate the ZSAD performance on other datasets. As for MVTec AD, we fine-tune AomalyCLIP on the test data of VisA. We report dataset-level results, which are averaged across their respective sub-datasets. All experiments are conducted in PyTorch-2.0.0 with a single NVIDIA RTX 3090 24GB GPU. More details can be found in Appendix A. 4.2 MAIN RESULTS ZSAD performance on diverse industrial inspection domains Table 1 shows the ZSAD results of AnomalyCLIP with five competing methods over seven industrial defect datasets of very different foreground objects, background, and/or anomaly types. AnomalyCLIP achieves superior ZSAD per- formance across the datasets, substantially outperforming the other five methods in most datasets. The weak performance of CLIP and CLIP-AC can be attributed to CLIP’s original pre-training, which focuses on aligning object semantics rather than anomaly semantics. By using manually defined text prompts, WinCLIP and VAND achieve better results. Alternatively, CoOp adopts learn- able prompts to learn the global anomaly semantics. However, those prompts focus on the global feature and ignore the fine-grained local anomaly semantics, leading to their poor performance on anomaly segmentation. To adapt CLIP to ZSAD, AnomalyCLIP learns object-agnostic text prompts to focus on learning the generic abnormality/normality using global and local context optimization, 1https://github.com/mlfoundations/open clip 6 Published as a conference paper at ICLR 2024 Table 1: ZSAD performance comparison on industrial domain. The best performance is highlighted in red, and the second-best is highlighted in blue. † denotes results taken from original papers. Task Category Obj &texture Image-level (AUROC, AP) Obj Pixel-level (AUROC, PRO) Texture Obj &texture Obj Texture Datasets MVTec AD VisA MPDD BTAD SDD DAGM DTD-Synthetic MVTec AD VisA MPDD BTAD SDD DAGM DTD-Synthetic |C| 15 12 6 3 1 10 12 15 12 6 3 1 10 12 CLIP (74.1, 87.6) (66.4, 71.5) (54.3, 65.4) (34.5, 52.5) (65.7, 45.2) (79.6, 59.0) (71.6, 85.7) (38.4, 11.3) (46.6, 14.8) (62.1, 33.0) (30.6, 4.4) (39.0, 8.9) (28.2, 2.9) (33.9, 12.5) CLIP-AC (71.5, 86.4) (65.0, 70.1) (56.2, 66.0) (51.0, 62.1) (65.2, 45.7) (82.5, 63.7) (66.8, 83.2) (38.2, 11.6) (47.8, 17.3) (58.7, 29.1) (32.8, 8.3) (32.5, 5.8) (32.7, 4.8) (23.7, 5.5) WinCLIP (91.8, 96.5)† (78.1, 81.2)† (63.6, 69.9) (68.2, 70.9) (84.3, 77.4) (91.8, 79.5) (93.2, 92.6) (85.1, 64.6)† (79.6, 56.8)† (76.4, 48.9) (72.7, 27.3) (68.8, 24.2) (87.6, 65.7) (83.9, 57.8) VAND (86.1, 93.5)† (78.0, 81.4)† (73.0, 80.2) (73.6, 68.6) (79.8, 71.4) (94.4, 83.8) (86.4, 95.0) (87.6, 44.0)† (94.2, 86.8)† (94.1, 83.2) (60.8, 25.0) (79.8, 65.1) (82.4, 66.2) (95.3, 86.9) CoOp (88.8, 94.8) (62.8, 68.1) (55.1, 64.2) (66.8, 77.4) (74.9, 65.1) (87.5, 74.6) (-, -) (33.3, 6.7) (24.2, 3.8) (15.4, 2.3) (28.6, 3.8) (28.9, 7.1) (17.5, 2.1) (-, -) AnomalyCLIP (91.5, 96.2) (82.1, 85.4) (77.0, 82.0) (88.3, 87.3) (84.7, 80.0) (97.5, 92.3) (93.5, 97.0) (91.1, 81.4) (95.5, 87.0) (96.5, 88.7) (94.2, 74.8) (90.6, 67.8) (95.6, 91.0) (97.9, 92.3) Table 2: ZSAD performance comparison on medical domain. The best performance is highlighted in red, and the second-best is highlighted in blue. Note that the image-level medical AD datasets do not contain segmentation ground truth, so the pixel-level medical AD datasets are different from the image-level datasets. Task Category Image-level (AUROC, AP) Pixel-level (AUROC, PRO) Brain Chest Skin Colon Thyroid Datasets HeadCT BrainMRI Br35H COVID-19 ISIC CVC-ColonDB CVC-ClinicDB Kvasir Endo TN3K |C| 1 1 1 1 1 1 1 1 1 1 CLIP (56.5, 58.4) (73.9, 81.7) (78.4, 78.8) (73.7, 42.4) (33.1, 5.8) (49.5, 15.8) (47.5, 18.9) (44.6, 17.7) (45.2, 15.9) (42.3, 7.3) CLIP-AC (60.0, 60.7) (80.6, 86.4) (82.7, 81.3) (75.0, 45.9) (36.0, 7.7) (49.5, 11.5) (48.5, 12.6) (45.0, 16.8) (46.6, 12.6) (35.6, 5.2) WinCLIP (81.8, 80.2) (86.6, 91.5) (80.5, 82.2) (66.4, 42.9) (83.3, 55.1) (70.3,32.5) (51.2,13.8) (69.7, 24.5) (68.2, 28.3) (70.7, 39.8) VAND (89.1, 89.4) (89.3, 90.9) (93.1, 92.9) (15.5, 8.5) (89.4, 77.2) (78.4, 64.6) (80.5, 60.7) (75.0, 36.2) (81.9, 54.9) (73.6, 37.8) CoOp (78.4, 78.8) (61.3, 44.9) (86.0, 87.5) (25.3, 9.2) (51.7, 15.9) (40.5, 2.6) (34.8, 2.4) (44.1, 3.5) (40.6, 3.9) (34.0, 9.5) AnomalyCLIP (93.4, 91.6) (90.3, 92.2) (94.6, 94.7) (80.1, 58.7) (89.7, 78.4) (81.9, 71.3) (82.9, 67.8) (78.9, 45.6) (84.1, 63.6) (81.5, 50.4) enabling the modeling of both global and local abnormality/normality. Our resulting prompts can also generalize to different datasets from various domains. To provide more intuitive results, we visualize the anomaly segmentation results of AnomalyCLIP, VAND, and WinCLIP across differ- ent datasets in Fig. 4. Compared to VAND and WinCLIP, AnomalyCLIP can perform much more accurate segmentation for the defects from different industrial inspection domains. Generalization from defect datasets to diverse medical domain datasets To evaluate the gen- eralization ability of our model, we further examine the ZSAD performance of AnomalyCLIP on 10 medical image datasets of different organs across different imaging devices. Table 2 shows the results, where learning-based methods, including AnomalyCLIP, VAND and CoOp, are all tuned using MVTec AD data. It is remarkable that methods like AnomalyCLIP and VAND obtain promis- ing ZSAD performance on various medical image datasets, even though they are tuned using a defect detection dataset. Among all these methods, AnomalyCLIP is the best performer due to its strong generalization brought by object-agnostic prompt learning. As illustrated in Fig. 4, Anoma- lyCLIP can accurately detect various types of anomalies in diverse medical images, such as skin cancer regions in photography images, colon polyps in endoscopy images, thyroid nodules in ultra- sound images, and brain tumors in MRI images, having substantially better performance in locating the abnormal lesion/tumor regions than the other two methods WinCLIP and VAND. This again demonstrates the superior ZSAD performance of AnomalyCLIP in datasets of highly diverse object semantics from medical imaging domains. Can we obtain better ZSAD performance if fine-tuned using medical image data? Comparing the promising performance in industrial datasets, AnomalyCLIP presents a relatively low perfor- mance in medical datasets. This is partly due to the impact of auxiliary data used in our prompt learning. So, then we examine whether the ZSAD performance on medical images can be improved if the prompt learning is trained on an auxiliary medical dataset. One challenge is that there are no available large 2D medical datasets that include both image-level and pixel-level annotations for our training. To address this issue, we create such a dataset based on ColonDB (More details see Appendix A), and then optimize the prompts in AnomalyCLIP and VAND using this dataset and evaluate their performance on the medical image datasets. The results are presented in Table 3. AnomalyCLIP and VAND largely improve their detection and segmentation performance compared to that fine-tuned on MVTec AD, especially for the colon polyp-related datasets such as CVC- ClincDB, Kvasir, and Endo (note that these datasets are all from different domains compared to the fine-tuning ColonDB dataset). AnomalyCLIP also exhibits performance improvement in detecting 7 Published as a conference paper at ICLR 2024 brain tumors in datasets such as HeadCT, BrainMRI, and Br35H. This is attributed to the visual similarities between colon polyps and brain tumors. Conversely, the symptom of the colon polyp differs significantly from that of diseased skin or chest, leading to performance degradation in ISIC and COVID-19. Overall, compared to VAND, AnomalyCLIP performs consistently better across all datasets of anomaly detection and segmentation. Table 3: ZSAD performance on medi- cal iamges when fine-tuned by medical image datasets. Category Classification Brain Chest Segmentation Skin Colon Thyroid Figure 4: Segmentation visualization. Datasets VAND AnomalyCLIP HeadCT BrainMRI Br35H COVID-19 (89.1, 89.4) (89.3, 90.9) (93.1, 92.9) (15.5, 8.5) (93.5, 95.1) (95.5, 97.2) (97.9, 98.0) (70.9, 33.7) ISIC (58.8, 31.2) CVC-ClinicDB (89.4, 82.3) (87.6, 39.3) (88.5, 81.9) (60.5, 16.8) Kvasir Endo TN3K (83.0, 63.8) (92.4, 82.9) (92.5, 61.5) (93.2, 84.8) (79.2, 47.0) Object-agnostic vs. object-aware prompt learning To study the ef- fectiveness of object-agnostic prompt learning in AnomalyCLIP, we com- pare AnomalyCLIP with its vari- ant that uses an object-aware prompt template. The performance gain of AnomalyCLIP to its object-aware prompt learning variant is shown in Fig. 5, where positive values indicate our object-agnostic prompt templates are better than the object-aware one. It is clear that our object-agnostic prompt learning performs much better than, or on par with, the object-aware version in both image- level and pixel-level anomaly detection. This indicates that having object-agnostic prompts helps better learn the generic abnormality and normality in images, as the object semantics are often not helpful, or can even become noisy features, for the ZSAD task. 4.3 ABLATION STUDY Figure 5: prompts compared to object-aware prompts. Performance gain of using object-agnostic Module ablation We first validate the effectiveness of different high-level modules of our Anoma- lyCLIP, including DPAM (T1), object-agnostic text prompts (T2), added learnable tokens in text encoders (T3), and multi-layer visual encoder features (T4). As shown in Table 4, each module contributes to the remarkable performance of AnomalyCLIP. DPAM improves the segmentation performance by enhancing local visual semantics (T1). Object-agnostic text prompts focus on the abnormality/normality within images instead of the object semantics, allowing AnomalyCLIP to de- tect anomalies in diverse unseen objects. Therefore, introducing object-agnostic text prompts (T2) significantly improves AnomalyCLIP. Furthermore, text prompt tuning (T3) also brings performance improvement via the refinement of original textual space. Finally, T4 integrates multi-layer visual semantics to provide more visual details, which further promotes the performance of ZSAD. Context optimization Next we examine key modules in detail. The object-agnostic prompt learn- ing is the most effective module, and it is driven by our glocal context optimization, so we consider two different optimization terms, local and global losses, in Eq. 2. The results are shown in Table 5. Both global and local context optimization contribute to the superiority of AnomalyCLIP. Global context optimization helps to capture global anomaly semantics, thus enabling more accurate image- level detection. Compared to global context optimization, local context optimization incorporates Table 4: Module ablation. VisA MVTec AD Pixel-level (46.8, 15.4) (68.4, 47.4) (89.5, 81.2) (90.0, 81.1) (91.1, 81.4) Image-level (66.3, 83.3) (66.3, 83.3) (90.8, 96.0) (91.0, 96.1) (91.5, 96.2) Pixel-level (47.9, 17.1) (54.8, 32.7) (95.0, 85.3) (95.2, 86.0) (95.5, 87.0) Image-level (54.4, 61.7) (54.4, 61.7) (81.7, 85.2) (81.9, 85.2) (82.1, 85.4) Module Base +T1 +T2 +T3 +T4 Table 5: Context optimization ablation. Local. Global. ✗ ✗ ✓ ✓ ✗ ✓ ✗ ✓ MVTec AD VisA Pixel-level (71.7, 57.7) (80.3, 77.8) (91.0, 80.4) (91.1, 81.4) Image-level (68.8, 85.8) (89.9, 95.4) (89.9, 96.0) (91.5, 96.2) Pixel-level (74.7, 62.1) (86.6, 78.1) (95.2, 86.5) (95.5, 87.0) Image-level (61.1, 69.1) (82.2, 84.9) (79.5, 83.2) (82.1, 85.4) 8 Medical domainSkinColonThyroidBrainOursVANDWinCLIPGroundTruthIndustrial domainTileScrewCapsuleMetal plateMVTec AD VisAMPDDBTADSDDDAGMDTDDataset01234Peformance gain 0.8 0.3 4.4 0.3 0.8 0.5 0.1 0.0 0.5 3.3 0.3 0.0 0.3 0.0 0.6 0.5 0.8 0.1 0.4 0.7 0.0 1.0 0.2 0.9 1.8 0.9 0.3 0.6Image-level metric Pixel-level metricImage-level AUROCAPPixel-level AUROCAUPRO Published as a conference paper at ICLR 2024 Figure 6: DPAM component ablation. local anomaly semantics, which improves pixel-level performance and complements image-level performance. By synthesizing these two optimization strategies, AnomalyCLIP generally achieves better performance than using them individually. DPAM strategy ablation AnomalyCLIP uses V -V self-attention by default. Here we study the effectiveness of using two other DPAM strategies, including Q-Q and K-K self-attention, result- ing in two AnomalyCLIP variants, namely AnomalyCLIPqq and AnomalyCLIPkk. The compari- son results are presented in Fig. 6. AnomalyCLIPqq achieves similar segmentation capabilities as AnomalyCLIP but suffers from degradation in detecting image-level anomalies. Conversely, while AnomalyCLIPkk performs well in anomaly classification, its segmentation performance is less ef- fective than AnomalyCLIP and AnomalyCLIPqq. The V -V self-attention is generally recommended in AnomalyCLIP. Detailed analysis of DPAM can be seen in Appendix C. 5 RELATED WORK Zero-shot anomaly detection ZSAD relies on the model’s strong transferability to handle un- seen anomalies (Aota et al., 2023). CLIP-AD (Liznerski et al., 2022) and ZOC (Esmaeilpour et al., 2022) are early studies in utilizing CLIP for ZSAD, but they mainly focus on the anomaly classi- fication task. ACR (Li et al., 2023a) requires tuning on target-domain-relevant auxiliary data for ZSAD on different target datasets, while AnomalyCLIP can be applied to different datasets after it is trained on one general dataset. A very recent approach WinCLIP (Jeong et al., 2023) presents a seminal work that leverages CLIP for zero-shot classification and segmentation. It uses a large number of hand-crafted text prompts and involves multiple forward passes of image patches for anomaly segmentation. To tackle this inefficiency, VAND (Chen et al., 2023) introduces learnable linear projection techniques to enhance the modeling of local visual semantics. However, these ap- proaches suffer from insufficiently generalized textual prompt embeddings, which degrades their performance in identifying anomalies associated with various unseen object semantics. Anomaly- CLIP utilizes only two object-agnostic learnable text prompts to optimize the generic text prompts of abnormality and normality, and it can obtain segmentation results with just a single forward pass. AnomalyGPT (Gu et al., 2023) is a concurrent work in utilizing foundation models for AD, but it is designed for unsupervised/few-shot AD with manually crafted prompts. Prompt learning Rather than resorting to full network fine-tuning, prompt learning emerges as a parameter-efficient alternative to achieve satisfactory results (Sun et al., 2022; Zhou et al., 2022a; Khattak et al., 2023; Kim et al., 2023). CoOp (Zhou et al., 2022b) introduces learnable text prompts for few-shot classification. On this basis, DenseCLIP (Rao et al., 2022) extends prompt learning to dense prediction tasks with an extra image decoder. Instead, AnomalyCLIP proposes object- agnostic prompt learning for anomaly detection, blocking out the potential adverse impact of the diverse object semantics on anomaly detection. Benefiting from the glocal context optimization, AnomalyCLIP can capture local anomaly semantics such that we can simultaneously perform clas- sification and segmentation tasks without an additional decoder network like Rao et al. (2022). 6 CONCLUSION In this paper, we tackle a challenging yet significant area of anomaly detection, ZSAD, in which there is no available data in the target dataset for training. We propose AnomalyCLIP to improve the weak generalization performance of CLIP for ZSAD. We introduce object-agnostic prompt learn- ing to learn generic abnormality/normality text prompts for generalized ZSAD on image datasets of diverse foreground objects. Further, to incorporate global and local anomaly semantics into Anoma- lyCLIP, we devise a joint global and local context optimization to optimize the object-agnostic text prompts. Extensive experimental results on 17 public datasets demonstrate that AnomalyCLIP achieves superior ZSAD performance. 9 MVTec ADVisAMPDDBTADSDDDAGMDTD7580859095100Pixel-level AUROCV-V attentionQ-Q attentionK-K attentionMVTec ADVisAMPDDBTADSDDDAGMDTD5060708090100AUPROMVTec ADVisAMPDDBTADSDDDAGMDTD7580859095100Image-level AUROCMVTec ADVisAMPDDBTADSDDDAGMDTD7580859095100AP Published as a conference paper at ICLR 2024 REPRODUCIBILITY STATEMENT To ensure the reproducibility and completeness of this paper, we have included an Appendix consist- ing of five main sections. In Appendix A, we provide more implementation details of AnomalyCLIP, as well as the reproduction of other baseline methods. Appendix B provides key statistics about the datasets used in our experiments and the implementation of the auxiliary medical dataset for prompt tuning. Appendix D supplements the main paper with additional results and ablations. Further vi- sualizations of similarity scores and maps are detailed in Appendix E. Additionally, the main paper presents only the average performance in each dataset that contains a number of data subsets, for which we present their fine-grained detection results in Appendix F. Our code will be made publicly accessible once the paper is accepted. REFERENCES Toshimichi Aota, Lloyd Teh Tzer Tong, and Takayuki Okatani. Zero-shot versus many-shot: Un- supervised texture anomaly detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 5564–5572, 2023. Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad–a comprehen- In Proceedings of the IEEE/CVF sive real-world dataset for unsupervised anomaly detection. conference on computer vision and pattern recognition, pp. 9592–9600, 2019. Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4183–4192, 2020. Jorge Bernal, F Javier S´anchez, Gloria Fern´andez-Esparrach, Debora Gil, Cristina Rodr´ıguez, and Fernando Vilari˜no. Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized medical imaging and graphics, 43:99–111, 2015. Tri Cao, Jiawen Zhu, and Guansong Pang. Anomaly detection under distribution shift. arXiv preprint arXiv:2303.13845, 2023. Xuhai Chen, Yue Han, and Jiangning Zhang. A zero-/few-shot anomaly classification and segmen- tation method for cvpr 2023 vand workshop challenge tracks 1&2: 1st place on zero-shot ad and 4th place on few-shot ad. arXiv preprint arXiv:2305.17382, 2023. Yuanhong Chen, Yu Tian, Guansong Pang, and Gustavo Carneiro. Deep one-class classification via interpolated gaussian descriptor. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 383–392, 2022. Muhammad E. H. Chowdhury, Tawsifur Rahman, Amith Khandakar, Rashid Mazhar, Muham- mad Abdul Kadir, Zaid Bin Mahbub, Khandakar Reajul Islam, Muhammad Salman Khan, Atif Iqbal, Nasser Al Emadi, Mamun Bin Ibne Reaz, and Mohammad Tariqul Islam. Can ai help IEEE Access, 8:132665–132676, 2020. doi: in screening viral and covid-19 pneumonia? 10.1109/ACCESS.2020.3010287. Hanqiu Deng and Xingyu Li. Anomaly detection via reverse distillation from one-class embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9737–9746, 2022. Choubo Ding, Guansong Pang, and Chunhua Shen. Catching both gray and black swans: Open-set supervised anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7388–7398, 2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 10 Published as a conference paper at ICLR 2024 Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detec- In Proceedings of the AAAI conference on artificial tion based on the pre-trained model clip. intelligence, volume 36, pp. 6568–6576, 2022. Tharindu Fernando, Harshala Gammulle, Simon Denman, Sridha Sridharan, and Clinton Fookes. Deep learning for medical anomaly detection–a survey. ACM Computing Surveys (CSUR), 54(7): 1–37, 2021. Haifan Gong, Guanqi Chen, Ranran Wang, Xiang Xie, Mingzhi Mao, Yizhou Yu, Fei Chen, and Guanbin Li. Multi-task learning for thyroid nodule segmentation with thyroid region prior. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), pp. 257–261. IEEE, 2021. Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, and Jinqiao Wang. Anoma- lygpt: Detecting industrial anomalies using large vision-language models, 2023. David Gutman, Noel C. F. Codella, Emre Celebi, Brian Helba, Michael Marchetti, Nabin Mishra, and Allan Halpern. Skin lesion analysis toward melanoma detection: A challenge at the inter- national symposium on biomedical imaging (isbi) 2016, hosted by the international skin imaging collaboration (isic), 2016. A. Hamada. Br35h: Brain tumor detection 2020. Online. Available: https://www.kaggle.com/datasets/ahmedhamada0/brain-tumor-detection, 2020. Steven A Hicks, Debesh Jha, Vajira Thambawita, P˚al Halvorsen, Hugo L Hammer, and Michael A Riegler. The endotect 2020 challenge: evaluation and comparison of classification, segmentation and inference time for endoscopy. In Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10-15, 2021, Proceedings, Part VIII, pp. 263–274. Springer, 2021. Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, and Yan-Feng Wang. Registration based few-shot anomaly detection. In European Conference on Computer Vision, pp. 303–319. Springer, 2022. Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and Onkar Dabeer. Winclip: Zero-/few-shot anomaly classification and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19606–19616, 2023. Stepan Jezek, Martin Jonak, Radim Burget, Pavel Dvorak, and Milos Skotak. Deep learning-based defect detection of metal parts: evaluating current methods in complex conditions. In 2021 13th International congress on ultra modern telecommunications and control systems and workshops (ICUMT), pp. 66–71. IEEE, 2021. Debesh Jha, Pia H Smedsrud, Michael A Riegler, P˚al Halvorsen, Thomas de Lange, Dag Johansen, and H˚avard D Johansen. Kvasir-seg: A segmented polyp dataset. In MultiMedia Modeling: 26th International Conference, MMM 2020, Daejeon, South Korea, January 5–8, 2020, Proceedings, Part II 26, pp. 451–462. Springer, 2020. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727. Springer, 2022. Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shah- baz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19113–19122, 2023. Kwanyoung Kim, Yujin Oh, and Jong Chul Ye. Zegot: Zero-shot segmentation through optimal transport of text prompts. arXiv preprint arXiv:2301.12171, 2023. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 11 Published as a conference paper at ICLR 2024 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, and Stephan Mandt. Zero-shot anomaly detection via batch normalization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, and Jiwei Li. Dice loss for data- imbalanced nlp tasks. arXiv preprint arXiv:1911.02855, 2019. Yi Li, Hualiang Wang, Yiqun Duan, and Xiaomeng Li. Clip surgery for better explainability with enhancement in open-vocabulary tasks. arXiv preprint arXiv:2304.05653, 2023b. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980–2988, 2017. Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. Clip-driven universal model for organ segmen- tation and tumor detection. arXiv preprint arXiv:2301.00785, 2023. Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Marius Kloft, and Klaus- Robert M¨uller. Explainable deep one-class classification. arXiv preprint arXiv:2007.01760, 2020. Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Klaus-Robert M¨uller, and Marius Kloft. Exposing outlier exposure: What can be learned from few, one, and zero outlier images. arXiv preprint arXiv:2205.11474, 2022. Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, and Gian Luca Foresti. Vt-adl: A vision transformer network for image anomaly detection and localization. In 2021 IEEE 30th International Symposium on Industrial Electronics (ISIE), pp. 01–06. IEEE, 2021. Shancong Mou, Xiaoyi Gu, Meng Cao, Haoping Bai, Ping Huang, Jiulong Shan, and Jianjun Shi. Rgi: robust gan-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection. In The Eleventh International Conference on Learning Representations, 2022. Guansong Pang, Choubo Ding, Chunhua Shen, and Anton van den Hengel. Explainable deep few- shot anomaly detection with deviation networks. arXiv preprint arXiv:2108.00462, 2021a. Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. ACM computing surveys (CSUR), 54(2):1–38, 2021b. Ziyuan Qin, Huahui Yi, Qicheng Lao, and Kang Li. Medical image understanding with pretrained vision language models: A comprehensive study. arXiv preprint arXiv:2209.15517, 2022. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763. PMLR, 2021. Tawsifur Rahman, Amith Khandakar, Yazan Qiblawey, Anas Tahir, Serkan Kiranyaz, Saad Bin Abul Kashem, Mohammad Tariqul Islam, Somaya Al Maadeed, Susu M Zughaier, Muhammad Salman Khan, et al. Exploring the effect of image enhancement techniques on covid-19 detection using chest x-ray images. Computers in biology and medicine, 132:104319, 2021. Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18082–18091, 2022. Tal Reiss and Yedid Hoshen. Mean-shifted contrastive loss for anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 2155–2162, 2023. 12 Published as a conference paper at ICLR 2024 Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch¨olkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14318–14328, 2022. Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Gr´egoire Montavon, Wojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert M¨uller. A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756–795, 2021. Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. Clip for all things zero-shot sketch-based image retrieval, fine-grained or not. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2765–2775, 2023. Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, and In Proceed- Hamid R Rabiee. Multiresolution knowledge distillation for anomaly detection. ings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14902–14912, 2021. Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast adaptation to multi-label recognition with limited annotations. Advances in Neural Information Processing Systems, 35:30569–30582, 2022. Domen Tabernik, Samo ˇSela, Jure Skvarˇc, and Danijel Skoˇcaj. Segmentation-based deep-learning Journal of Intelligent Manufacturing, 31(3):759–776, approach for surface-defect detection. 2020. Nima Tajbakhsh, Suryakanth R Gurudu, and Jianming Liang. Automated polyp detection in colonoscopy videos using shape and context information. IEEE transactions on medical imaging, 35(2):630–644, 2015. Yu Tian, Guansong Pang, Fengbei Liu, Yuanhong Chen, Seon Ho Shin, Johan W Verjans, Rajvin- der Singh, and Gustavo Carneiro. Constrained contrastive distribution learning for unsupervised anomaly detection and localisation in medical images. In Medical Image Computing and Com- puter Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pp. 128–140. Springer, 2021. Yu Tian, Fengbei Liu, Guansong Pang, Yuanhong Chen, Yuyuan Liu, Johan W Verjans, Rajvinder Singh, and Gustavo Carneiro. Self-supervised pseudo multi-class pre-training for unsupervised anomaly detection and segmentation in medical images. Medical Image Analysis, pp. 102930, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa- tion processing systems, 30, 2017. Matthias Wieler and Tobias Hahn. Weakly supervised learning for industrial optical inspection. In DAGM symposium in, volume 6, 2007. Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions for open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15254–15264, 2023. Guoyang Xie, Jingbao Wang, Jiaqi Liu, Feng Zheng, and Yaochu Jin. Pushing the limits of fewshot anomaly detection in industry vision: Graphcore. arXiv preprint arXiv:2301.12082, 2023. Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, and Xinyi Le. A unified model for multi-class anomaly detection. Advances in Neural Information Processing Systems, 35:4571– 4584, 2022. Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pp. 16793–16803, 2022. 13 Published as a conference paper at ICLR 2024 Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16816–16825, 2022a. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision- language models. International Journal of Computer Vision, 130(9):2337–2348, 2022b. Qihang Zhou, Shibo He, Haoyu Liu, Tao Chen, and Jiming Chen. Pull & push: Leveraging dif- ferential knowledge distillation for efficient unsupervised anomaly detection and localization. IEEE Transactions on Circuits and Systems for Video Technology, 33(5):2176–2189, 2023. doi: 10.1109/TCSVT.2022.3218587. Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer. Spot-the-difference self-supervised pre-training for anomaly detection and segmentation. In European Conference on Computer Vision, pp. 392–408. Springer, 2022. A IMPLEMENTATION DETAILS AND BASELINES A.1 IMPLEMENTATION DETAILS In this paper, we use the publicly available CLIP model (VIT-L/14@336px) as our backbone. Model parameters of CLIP are all frozen. The length of learnable text prompts M is set to 12. These trainable text tokens are attached to the first 9 layers of the text encoder, and each text token has a length of 4. We fine-tune AnomalyCLIP on the test data on MVTec AD and test the performance for other datasets. As for MVTec AD, we fine-tune AomalyCLIP on test data on VisA. To provide adequate visual details, we extract local visual embeddings vi m from the 6-th, 12-th, 18-th, and 24-th layers of the visual encoder. Starting from the 6-th layer, we apply DPAM to the architecture of the visual encoder according to Sec. 3.3. Additionally, we set the balanced weight λ to 1 in our loss function. The input images are resized to a size of 518 with batch size 8, and we use the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 0.001 to update model parameters. During testing, we apply a Gaussian filter with σ = 4 to smooth the anomaly score map. The epoch is 15 for all experiments, which are performed in PyTorch-2.0.0 with a single NVIDIA RTX 3090 24GB GPU. A.2 BASELINES To demonstrate the superiority of Anomlay-CLIP, we compare AnomlayCLIP with broad SOTA baselines. Implementation and reproduction details are given as follows: • CLIP (Radford et al., 2021). CLIP is a powerful zero-shot classification method. To perform the anomaly detection task, we use two classes of text prompt templates A photo of a normal [cls] and A photo of an anomalous [cls], where cls denotes the target class name. The anomaly score is computed according to Eq. 1. As for anomaly segmentation, we extend the above computation to local visual embedding to derive the segmentation. • CLIP-AC (Radford et al., 2021). Different from CLIP, CLIP-AC employs an ensemble of text prompt templates that are recommended for ImageNet dataset (Radford et al., 2021). We average the generated textual embeddings of normal and anomaly classes respectively, and compute the probability and segmentation in the same way as CLIP. • WinCLIP (Jeong et al., 2023). WinCLIP is a SOTA ZSAD method. They design a large set of hand-crafted text prompt templates specific to anomaly detection and use a window scaling strategy to obtain anomaly segmentation. All parameters are kept the same as in their paper. • VAND (Chen et al., 2023). VAND is an improved version of WinCLIP. They first adjust the text prompt templates and then introduce learnable linear projections to improve local visual semantics to derive more accurate segmentation. All parameters are kept the same as in their paper. 14 Published as a conference paper at ICLR 2024 Table 6: Key statistics on the datasets used. Dataset Category Modalities |C| Obj &texture Obj Texture Skin MVTec AD VisA MPDD BTAD SDD DAGM DTD-Synthetic ISIC CVC-ClinicDB CVC-ColonDB Kvasir Endo TN3K Thyroid HeadCT BrainMRI Br35H Brain COVID-19 Chest Photography Photography Photography Photography Photography Photography Photography Photography Endoscopy Endoscopy Endoscopy Endoscopy Radiology (Utralsound) Radiology (CT) Radiology (MRI) Radiology (MRI) Radiology (X-ray) 15 12 6 3 1 10 12 1 1 1 1 1 1 1 1 1 1 Normal and anomalous samples (467, 1258) (962, 1200) (176, 282) (451, 290) (181, 74) (6996, 1054) (357, 947) (0, 379) (0, 612) (0, 380) (0, 1000) (0, 200) Usage Industrial defect detection Industrial defect detection Industrial defect detection Industrial defect detection Industrial defect detection Industrial defect detection Industrial defect detection Skin cancer detection Colon polyp detection Colon polyp detection Colon polyp detection Colon polyp detection (0, 614) Thyroid nodule detection (100, 100) Brain tumor detection (98, 155) Brain tumor detection (1500, 1500) Brain tumor detection (1341, 219) COVID-19 detection • CoOp (Zhou et al., 2022b). CoOp is a representative method for prompt learning. To adapt CoOp to ZSAD, we replace its learnable text prompt templates [V1][V2]...[VN ][cls] with normality and abnormality text prompt templates, where Vi is the learnable word em- beddings. The normality text prompt template is defined as [V1][V2]...[VN ][normal][cls], and the abnormality one is defined as [V1][V2]...[VN ][anomalous][cls]. Anomaly proba- bilities and segmentation are obtained in the same way as for AnomalyCLIP. All parameters are kept the same as in their paper. B DATASET More dataset details In this paper, we conduct extensive experiments on 17 public datasets span- ning two domains and three modalities to validate the effectiveness of our methods. Since we just use the test data of Datasets, we present the relevant information of their test sets in Table 6. We apply the default normalization of OpenCLIP to all datasets. After normalization, we resize the images to a resolution of (518, 518) to obtain an appropriate visual feature map resolution. It should be noted that the original image size of SDD has a width of 500 and a height ranging from 1,240 to 1,270. Before processing, we vertically divide the original 500 × 1,250 image into two images and assign pixel-wise annotations to each image. Fine-tuning medical dataset We cannot find publicly available 2D medical AD datasets that in- clude both category labels and segmentation ground truths simultaneously. To fill the blank, in this paper, we create such a medical dataset by combining two existing 2D medical datasets. Particularly, we use the colon polyp detection dataset ColonDB (Tajbakhsh et al., 2015) to provide pixel-level an- notations. Meanwhile, considering the normal samples in the same domain, we choose the test split of Endo classification dataset (Hicks et al., 2021) to combine with ColonDB. As a result, the new medical dataset contains 163 normal samples and 380 anomaly samples, supporting both anomaly classification and segmentation tasks. 15 Published as a conference paper at ICLR 2024 C DETAILED ANALYSIS OF DPAM Since the visual encoder of CLIP is originally pre-trained to align global object semantics, such as cat and dog, the contrastive loss used in CLIP makes the visual encoder produce a representative global embedding for recognizing semantic classes. Through the self-attention mechanism, the attention map in the visual encoder focuses on the specific tokens highlighted within the red rectangle in Fig. 3b. Although these tokens may contribute to global object recognition, they disrupt the local visual semantics, which directly hinders the effective learning of the fine-grained abnormality in our object-agnostic text prompts. For segmentation purposes, it’s crucial for the visual feature map to emphasize the surrounding context to capture more local visual semantics. Formally, let aij be an attention score in the attention score matrix, where i, j ∈ [1, h × w], then the i-th output of Q-K attention can be written as: Attention(Q, K, V )i = sof tmax (cid:19) (cid:18) qiK ⊤ √ D V = n (cid:80) j=1 n (cid:80) j=1 aijvj aij , aij = e qik⊤ j √ D . Note that vectors (i.e., qi, ki, vi) are represented as row vectors. Attention(Q, K, V )i can be re- garded as the weighted average of vj using aij as the weight. Assuming that the original attention map focuses on the specific tokens at index m, it is clear that qi only produces the large attention score with km in all kj. Therefore, aim is the largest score among other aij so Attention(Q, K, V )i is dominated by vm, which causes the local visual embedding at index i to be disturbed by the lo- cal visual embedding at index m. In Figure 3(b), the attention score map presents vertical activa- tion and suggests that every qi produces a large attention score with km. In such a case, several Attention(Q, K, V )i is dominated by vm and results in weak anomaly segmentation in Figure 3(b) even though vm may be important for original class recognition. Some prior studies (Rao et al., 2022; Gu et al., 2023) use an additional decoder to recover the local visual semantics. In this paper, we directly use local visual embeddings for segmentation and point out that an ideal attention map for local visual semantics should exhibit a more pronounced diagonal pattern. For this purpose, DPAM is proposed to replace the original Q-K attention with analogous components, including Q-Q, K-K, and V -V self-attention. Therefore, aij is changed into: aqq ij = e qiq⊤ j √ D , akk ij = e kik⊤ j √ D , avv ij = e viv⊤ j √ D . qi, ki, that and Attention(V, V, V )i, and vi hold significant weight in forming This modification ensures Attention(Q, Q, V )i, Attention(K, K, V )i, thereby preserving local visual semantics. As a result, the produced attention maps exhibit a more diagonal prominence compared to the original Q-K attention, leading to improved performance in anomaly segmen- tation, as shown in Fig.3c, Fig.3d, and Fig. 3e. However, since Q and K consist of the original attention map, other important tokens at index n for class recognition within themselves may also produce relatively large scores (ain) (e.g., qi has strong relevance with qn besides qi) to disturb Attention(Q, Q, V )i and Attention(K, K, V )i Fig.3c and Fig.3d. In contrast to Q-Q and K-K, V -V does not participate in computing the original attention map, reducing the unexpected bias to different tokens in V for the purpose of anomaly segmentation. Therefore, vi does not produce a large weight (aij) with vj and generates a larger weight (aii) to form Attention(V, V, V )i, preserving more information of vi and experiencing diagonally prominent attention map (minimal disturbance), as depicted in Fig. 3e. This is the reason why V -V achieves the best results. D ADDITIONAL RESULTS AND ABLATIONS Module ablation by removing modules. We dive into the effectiveness of each module in AnomalyCLIP in Table 7. We test the contribution of one module by removing one module and maintaining the rest module. 1. The effectiveness of DPAM (T1). When we remove DPAM, the results show a decrease from 91.1% AUROC to 87.9% AUROC in pixel-level performance and from 91.5% AU- ROC to 80.7% AUROC in image-level performance. This performance decline indicates 16 Published as a conference paper at ICLR 2024 Table 7: Study on the effect of each module. T1: DPAM, T2: Object-agnostic prompt learning, T3: Textual prompt tuning, and T4: Integration of multi-scale local visual feature. T1 T2 T3 T4 ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✗ MVTec AD VisA Pixel-level (91.1, 81.4) (87.9, 80.0) (84.3, 75.3) (90.6, 80.5) (90.0, 81.1) Image-level (91.5, 96.2) (80.7, 89.5) (65.6, 85.3) (90.4, 95.3) (91.0, 96.1) Pixel-level (95.5, 87.0) (91.9, 84.9) (89.5, 83.2) (94.5, 85.6) (95.2, 86.0) Image-level (82.1, 85.4) (73.0, 77.7) (68.1, 72.9) (81.6, 85.1) (81.9, 85.2) Table 8: The study of computation overhead among baselines. Methods CLIP WinCLIP VAND CoOp AnomalyCLIP AnomalyCLIP without DPAM Training time per epoch (min) - - 13.56 min 12.25 min 13.71 min 12.98 min FPS 13.23 1.20 9.23 12.75 8.92 10.21 Performance (VisA) Pixel-level (46.6, 14.8) (79.6, 56.8) (94.2, 86.8) (24.2, 3.8) (95.5, 87.0) (91.9, 84.9) Image-level (66.4, 71.5) (78.1, 81.2) (78.0, 81.4) (62.8, 68.1) (82.1, 85.4) (73.0, 77.7) the importance of DPAM, which enhances local visual semantics by modifying the atten- tion mechanism. However, the decrease in performance at the image level is more pro- nounced than that at the pixel level. This discrepancy is attributed to the fact that the total loss places greater emphasis on local context optimization, driven by a larger local loss compared to the case with DPAM. 2. The effectiveness of object-agnostic prompt learning (T2). Excluding object-agnostic prompt learning makes AnomalyCLIP suffer from the huge performance gap (i.e., 91.1% AUROC to 84.3% AUROC in pixel-level and 91.5% AUROC to 65.6% AUROC in image- level). This performance decline illustrates that the object-agnostic text prompt template plays a significant role in improving the performance of AnomalyCLIP at both pixel and image levels. 3. The effectiveness of textual prompt tuning (T3). When removing textual prompt tuning, the performance of AnomalyCLIP declines from 91.1% AUROC to 90.6% AUROC in pixel- level performance and from 91.5% AUROC to 90.4% AUROC in image-level performance. This demonstrates the importance of adapting original textual space by adding learnable textual tokens in the text encoder. 4. The effectiveness of the integration of multi-layer local visual semantics (T4). When re- moving multi-layer local visual semantics, the outcomes reveal a decrease from 91.1% AUROC to 90.0% AUROC in pixel-level performance and from 91.5% AUROC to 91.0% AUROC in image-level performance. This performance decline indicates the importance of incorporating multi-layer local visual semantics. Study of computation overhead In addition to performance, computation overhead is also an important metric to evaluate the model. Therefore, we assess the time taken during training (train- ing time per epoch) and the inference speed (frames per second, FPS). For a fair comparison, all experiments are conducted in a single 3090 NVIDIA RTX 3090 24GB GPU, and the GPU is kept free before evaluation. In Table 8, AnomalyCLIP takes 13.71 min per epoch on MVTec AD (The total number of samples is 1725) and only requires a total of 15 epochs for the whole fine-tuning. Once AnomalyCLIP finishes fine-tuning, AnomalyCLIP can be applied to different datasets and do- mains without additional training. We also compare AnomalyCLIP with other baselines that need auxiliary data (i.e., CoOp and VAND). The minimum training time per epoch is 12.25 min of CoOp, and hence the training time taken is similar for fine-tuning methods. As for inference speed, CLIP achieves the 13.23 FPS. However, it suffers from weak detection performance. Although WinCLIP achieves better performance, WinCLIP has only 1.2 FPS because it needs multiple forward image patches to derive the segmentation. AnomalyCLIP outperforms WinCLIP and obtains 8.92 FPS. We also evaluated the computation overhead of DPAM separately. In Table 8, without DPAM, AnomalyCLIP takes 12.98 min to train per epoch. Compared to the 13.71 min for AnomalyCLIP with DPAM, we observe that introducing DPAM does not significantly increase the time complexity. 17 Published as a conference paper at ICLR 2024 (a) Successful case of de- tecting scratch-like defects. (b) Failure case of detect- ing scratch-like defects. (c) Successful case of de- tecting color stain. (d) Failure case of detect- ing color stain Figure 7: Analysis for successful and failure cases. Table 9: Study on the effect of shared and unshared learnable word embeddings. MVTec AD VisA Shared Unshared Pixel-level (90.5, 80.1) (91.1, 81.4) Image-level (90.9, 95.2) (91.5, 96.2) Pixel-level (95.0, 86.4) (95.5, 87.0) Image-level (81.5, 84.4) (82.1, 85.4) This is attributed to the fact that DPAM only creates the two paths during the computation of the attention map and is frozen during fine-tuning, thereby avoiding the computationally expensive pro- cess of gradient computation. Meanwhile, DPAM also does not result in large computation overhead during inference: AnomalyCLIP w/ DPAM gets 8.92 FPS vs. 10.21 FPS for w/o using DPAM. Discussion of successful and failure case Although AnomalyCLIP achieves superior results in ZSAD, we find that there still exist some potential failures. In Fig. 7a, AnomalyCLIP accurately detects scratch-like patterns on the product, even when they typically appear in the texture. How- ever, false detection occurs when scratch-like patterns are situated in the background, as depicted in Fig. 7b. Meanwhile, we also show the color stain pattern. As shown in Fig. 7c, AnomalyCLIP suc- cessfully detects the color stain, which exhibits subtle visual differences from the detected entities. However, AnomalyCLIP may face challenges when the normal region displays patterns that are in- distinguishable to the naked eye from anomalies. For instance, in skin cancer detection, the normal regions falsely detected as anomalies are actually visually similar to the disease region in Fig. 7c. Also, the stain interference in the background is also a problem. These failure cases illustrate the importance of mitigating background interference and achieving fine-grained discrimination, espe- cially in cases of visually similar abnormalities. Exploring these challenges for enhancing ZSAD is a valuable direction for future research. Study on the effect of shared and unshared learnable word embeddings As presented in Ta- ble 9, when sharing the learnable word embeddings of gn and ga, AnomalyCLIP achieves 90.5% AUROC in pixel level and 90.9% in image-level on MVTec AD and 95.0% AUROC in pixel level and 81.5% AUROC in image level on VisA. The results show that AnomalyCLIP without sharing also works well for ZSAD and the efficiency of our object-agnostic prompt learning. However, the shared prompt performs slightly worse than the unshared prompts (used in the original paper). The performance decrease is 0.6%AUROC and 0.6%AUROC in image level on MVTec AD and Visa, and 0.5%AUROC and 0.6%AUROC in pixel level. We believe that the separate learning for these two prompts helps discriminate the generic normality and abnormality because when we share the parameters of Vi and Wi, the learned semantics of normal and anomaly may be confused. Study on the effect of local visual features Here, we examine the impact of various types of local visual semantics. We explore two ensemble methods, namely AnomalyCLIPensemble1 and AnomalyCLIPensemble2, involving the ensemble of Q-Q, K-K, and V -V and the ensemble of Q- K, Q-Q, K-K, and V -V , respectively. In addition to Q-K and V -V features, we average the logit output of different features for the ensemble. As shown in Table 10, AnomalyCLIPensemble1 shows performance improvement by leveraging the advantages of three DPAM features. How- ever, while AnomalyCLIPensemble2 outperforms the Q-K feature version, it experiences a perfor- 18 Published as a conference paper at ICLR 2024 Table 10: Study on the effect of local visual features. Module Q-K V-V Q-Q, K-K, V-V (ensemble) Q-K, Q-Q, K-K, V-V(ensemble) MVTec AD VisA Pixel-level (87.9, 80.0) (91.1, 81.4) (91.2, 83.3) (90.7, 80.5) Image-level (80.7, 89.5) (91.5, 96.2) (91.4, 96.1) (90.8, 96.0) Pixel-level (91.9, 84.9) (95.5, 87.0) (95.8, 87.6) (94.9, 86.6) Image-level (73.0, 77.7) (82.1, 85.4 ) (82.3, 85.7) (80.7, 83.7) mance decrease compared to V -V from 91.1%AUROC to 90.7%AUROC and 91.5%AUROC to 90.8%AUROC on MVTec AD. There is also a decline from 95.5%AUROC to 94.9%AUROC and 82.1%AUROC to 80.7%AUROC on VisA. The decline in performance upon adding Q-K features to AnomalyCLIPensemble1 suggests that the Q-K feature fails to provide valid local visual semantics to facilitate ZSAD. Note that the original CLIP exploits Q-K features and gets the weak segmentation performance. The seemingly good pixel-level performance of Q-K in AnomalyCLIP is attributed to local optimization, where the object-agnostic prompt helps alleviate the disrupted local visual semantics of Q-K. Table 11: Ablation on the effect of the Focal and Dice loss. Focal loss Dice loss ✗ ✗ ✓ ✓ ✗ ✓ ✗ ✓ MVTec AD VisA Pixel-level (80.3, 77.8) (87.2, 78.6) (90.6, 78.1) (91.1, 81.4) Image-level (89.9, 95.4) (89.5, 95.2) (91.0, 96.0) (91.5, 96.2) Pixel-level (86.6, 78.1) (90.1, 79.8) (94.9, 86.1) (95.5, 87.0) Image-level (82.2, 84.9) (81.8, 85.2) (81.2, 84.6) (82.1, 85.4) Focal and Dice loss ablation Focal and Dice loss play a crucial role in optimizing local context. They are introduced to empower object-agnostic text prompts to focus on fine-grained, local ab- normal regions from intermediate layers of the visual encoder. As mentioned in Section 3.2, Focal loss addresses the imbalance between anomaly and normal pixels, typically caused by the smaller size of anomalous regions. Meanwhile, Dice loss aims to precisely constrain the anomaly boundary by measuring the overlap between the predicted segmentation (Sn/Sa) and the ground truth mask. To provide a more comprehensive analysis, we have included an ablation study on Focal and Dice loss in Table 11. Compared to scenarios without local context optimization, Dice loss improves the pixel-level and image-level performance from 80.3%AUROC to 87.2%AUROC and 86.6%AUROC to 90.1%AUROC in pixel level on MVTec AD and VisA. Focal loss also brings the performance gain of 10.3%AUROC and 8.3%AUROC. Combining Focal and Dice loss, AnomalyCLIP achieves the best results (i.e., 91.1%AUROC and 95.5%AUROC). Note that the global context optimization is always used during the ablation, since we need at least one loss function to drive the optimization. Table 12: Comparison of ZSAD performance between AnomalyCLIP and SOTA full-shot methods. The best performance is highlighted in red, and the second-best is highlighted in blue. Task Category Obj &texture Image-level (AUROC, AP) Obj Pixel-level (AUROC, PRO) Texture Obj &texture Obj Texture Datasets MVTec AD VisA MPDD BTAD SDD DAGM MVTec AD VisA MPDD BTAD SDD DAGM |C| 15 12 6 3 1 10 15 12 6 3 1 10 AnomalyCLIP (91.5, 96.2) (82.1, 85.4) (77.0, 82.0) (88.3, 87.3) (84.7, 80.0) (97.5, 92.3) (91.1, 81.4) (95.5, 87.0) (96.5, 88.7) (94.2, 74.8) (90.6, 67.8) (95.6, 91.0) PatchCore (99.0, 99.7) (94.6, 95.9) (94.1, 96.3) (93.2, 98.6) (64.9, 48.3) (92.7, 81.3) (98.1, 92.8) (98.5, 92.2) (98.8, 94.9) (97.4, 74.4) (87.9, 46.3) (95.9, 87.9) RD4AD (98.7, 99.4) (95.3, 95.7) (91.6, 93.8) (93.8, 96.8) (86.8, 81.3) (92.9, 79.1) (97.8, 93.6) (98.4, 91.2) (98.4, 95.2) (97.5, 75.1) (92.2, 72.0) (96.8, 91.9) Comparison with SOTA full-shot methods In this section, we are interested in the performance gap between AnomalyCLIP and the recently published SOTA full-shot methods, such as Patch- Core (Roth et al., 2022) and RD4AD (Deng & Li, 2022). Since some datasets do not provide normal training data, we conduct experiments on six public datasets. As shown in Table 12, AnomalyCLIP achieves comparable anomaly detection and segmentation performance compared to PatchCore and RD4AD, and it even outperforms them in some datasets. This illustrates that the generic prompt 19 Published as a conference paper at ICLR 2024 (a) (c) (b) (d) (c)L ablation (d)N ablation. Figure 8: Hyparameter analysis. Pixel/image-level (AUPRO, AP) performances are shown on the left and right sides of each subplot, respectively. (b) M ablation. (a)E ablation. embeddings empower AnomalyCLIP to effectively capture the normality and abnormality so that AnomalyCLIP can surpass the performance boundary decided by the training data. Refinement of the textual space A representative embedding is not only decided by the well- designed text prompt, it also depends on the appropriate textual space. During fine-tuning, randomly initialized learnable token embeddings are introduced in the text encoder to refine the textual space for the adaption to AD. To control the degree of refining the textual space, we choose to insert the learnable token embeddings into the text encoder from its bottom to the top layer. In particular, the trainable and original tokens are denoted as t′ m and tm, respectively, where m represents the layer of the text encoder. To integrate the original textual representations, for the layer m, we concatenate t′ m and tm along the dimension of the channel and then forward them into Tm to get r′ m+1 and tm+1. Due to the self-attention mechanism, the output of tm+1 contains the information of t′ m. In order to provide adequate calibration, we discard the obtained r′ m+1 and initialize new learnable token embeddings t′ m+1. Through this operation, t′ m+1 further refines textual representations of the layer m + 1. We repeat this operation until we reach the designated layer M ′. This procedure is given by: [r′ [r′ m+1, tm+1] = Tm([t′ m+2, tm+2] = Tm+1([t′ m, tm]) m+1, tm+1]) (3) . . . tM ′+1 = TM ′(tM ′), where the operator [·, ·] represents the concatenation along the channel. Hyparameter analysis We study the length of learnable text prompts E, depth of learnable token embeddings M , length of learnable token embeddings M , and number of used layers in visual encoder N . As shown in Fig. 8b, we observe that the detection and segmentation performance initially improves with an increase in the value of E. However, within the range of lengths from 12 to 16, we notice a decline in performance, which suggests that excessively long learnable text prompts could involve redundant information. Therefore, an appropriate value for E, such as E = 12, is beneficial to accurate learning of object-agnostic text prompts. Besides, we also investigate the depth of the attached learnable token embeddings in Fig. 8b. The degree of refining of the initial text space becomes more pronounced as the depth increases, enabling more discriminative textual embeddings for normal and anomaly. However, the performance drops when the refinement is excessive and impairs the generalization of AnomlayCLIP, as seen in the case when M equals 9. After selecting the depth, we proceed to investigate the influence of the length of learnable token embeddings. As illustrated in Fig. 8c, we find that the length of token embeddings also involves a similar tradeoff between the model generalization and calibration of textual space in Fig. 8d. AnomalyCLIP achieves the overall performance gain when we provide the most local visual semantics (N = 4). 20 1012141680.082.585.087.5MVTecVisA10121416859095100MVTecVisAAblation on the length of learnable text prompt E5791180.082.585.087.5MVTecVisA57911859095100MVTecVisAAblation on the depth of learnable text prompt M246880.082.585.087.5MVTecVisA246880859095MVTecVisAAblation on the length of learnable token embedding L01234758085MVTecVisA0123480859095MVTecVisAAblation on used layer of visial encoder N Published as a conference paper at ICLR 2024 Table 13: Ablation on the robustness of the abnormality-related token in our prompt template on industrial defect datasets. Task Category Obj &texture Image-level (AUROC, AP) Obj Pixel-level (AUROC, PRO) Texture Obj &texture Obj Texture Datasets MVTec AD VisA MPDD BTAD SDD DAGM DTD-Synthetic MVTec AD VisA MPDD BTAD SDD DAGM DTD-Synthetic damaged (91.5, 96.2) (82.1, 85.4) (77.0, 82.0) (88.3, 87.3) (84.7, 80.0) (97.5, 92.3) (93.5, 97.0) (91.1, 81.4) (95.5, 87.0) (96.5, 88.7) (94.2, 74.8) (90.6, 67.8) (95.6, 91.0) (97.9, 92.3) anomalous (91.4, 96.2) (80.7, 84.5) (78.0, 83.9) (84.8, 86.7) (82.3, 76.3) (97.7, 92.6) (93.3, 96.9) (91.0, 81.4) (95.5, 86.5) (96.6, 88.7) (94.3, 74.3) (89.6, 66.8) (95.6, 91.2) (97.9, 92.3) flawed (91.3, 96.2) (80.7, 84.5) (77.9, 83.6) (85.2, 87.4) (82.6, 76.8) (97.5, 92.4) (93.2, 96.9) (90.7, 81.4) (95.5, 86.5) (96.7, 89.0) (94.4, 75.1) (89.5, 66.5) (95.6, 91.3) (97.9, 92.1) defective (91.4, 96.2) (80.9, 84.6) (77.8, 83.5) (84.8, 86.2) (82.8, 77.2) (97.5, 92.3) (93.4, 97.0) (91.0, 81.7) (95.5, 86.2) (96.7, 89.2) (94.3, 75.2) (89.5, 64.8) (95.5, 90.9) (97.9, 92.5) blemished (91.5, 96.2) (80.7, 84.5) (78.6. 84.1) (85.9, 67.1) (82.7, 77.0) (97.5, 92.4) (93.5, 97.0) (90.9, 81.2) (95.6, 86.5) (96.6, 88.8) (94.3, 73.7) (89.6, 64.6) (95.6, 90.9) (97.9, 92.2) Figure 9: Object ablation. Prompt template ablation Here, we study the robustness of AnomalyCLIP to prior anomaly semantics in the object-agnostic text prompt template. We replace damaged in the object-agnostic text prompt with other words having similar anomaly semantics, such as anomalous, flawed, defective, blemished. The results are presented in Table 13 and Table 14. The steady results indicate that AnomalyCLIP is not sensitive to the prior anomaly semantics introduced by the object- agnostic text prompt template. Object ablation To investigate what the object-agnostic text prompts have learned, we replace object in object-agnostic text prompts with specific target [cls], resulting in AnomalyCLIPre. In Fig. 9, AnomalyCLIPre still performs well in ZSAD, even as we block out the object seman- tics during fine-tuning. This suggests that the knowledge learned by object-agnostic text prompts is the underlying anomaly patterns, allowing them to provide discriminative textual embeddings even when specific object semantics are incorporated. Furthermore, compared to AnomalyCLIP, AnomalyCLIPre shows a performance decay, which can be attributed to the inclusion of redun- dant/noisy object semantics. These results once again demonstrate the generalization ability of object-agnostic prompt learning. Table 14: Ablation on the robustness of the abnormality-related token in our prompt template on medical image datasets. Task Category Image-level (AUROC, AP) Pixel-level (AUROC, PRO) Brain Chest Skin Colon Thyroid Datasets HeadCT BrainMRI Br35H COVID-19 ISIC CVC-ColonDB CVC-ClinicDB Kvasir Endo TN3K damaged (93.4, 91.6) (90.3, 92.2) (94.6, 94.7) (80.1, 58.7) (89.7, 78.4) (81.9, 71.3) (82.9, 67.8) (78.9, 45.6) (84.1, 63.6) (81.5, 50.4) anomalous (93.1, 90.6) (87.8, 90.4) (93.1, 93.0) (80.0, 58.5) (90.1, 80.1) (82.2, 71.5) (83.0, 68.1) (79.4, 45.1) (84.3, 63.5) (81.5, 51.7) flawed (93.3, 90.8) (87.7, 90.0) (92.9, 92.8) (80.2, 58.8) (90.1, 80.1) (82.3, 71.6) (83.1, 68.4) (79.4, 45.2) (84.2, 63.5) (81.3, 50.9) defective (93.5, 91.0) (88.3, 90.5) (93.1, 93.0) (80.6, 59.0) (90.4, 81.0) (82.1, 71.1) (82.9, 67.9) (79.3, 44.9) (84.2, 62.9) (81.3, 50.3) blemished (93.8, 91.5) (88.6, 90.7) (93.2, 93.1) (82.1, 61.4) (90.2, 80.6) (82.2, 71.5) (83.1, 68.2) (79.5, 45.8) (84.3, 63.4) (81.6, 51.1) 21 MVTec AD VisA91.582.189.182.0Image-level AUROC AUROCAnomalyCLIPAnomalyCLIPreMVTec AD VisA96.285.494.585.0APAnomalyCLIPAnomalyCLIPreMVTec AD VisA91.195.590.595.0Pixel-level AUROCAnomalyCLIPAnomalyCLIPreMVTec AD VisA81.487.080.686.6AUPROAnomalyCLIPAnomalyCLIPre Published as a conference paper at ICLR 2024 E VISUALIZATION Similarity score between textual and visual embeddings. We present visualizations of the sim- ilarity scores generated by both CLIP and AnomalyCLIP. These visualizations aim to provide an intuitive illustration of the effective adaptation made by AnomalyCLIP in comparison to CLIP. As shown in Fig. 10 and Fig. 11, we present the similarity score of CLIP on MVTec AD and VisA. The normal and anomaly scores are severely overlapped. Further, the range of scores is centered at 0.5. These show that the textual and visual space of CLIP originally aligned for object semantics are not desired for ZSAD. Also, we visualize the similarity scores of AnomalyCLIP in Fig. 12 and Fig. 13. Compared to CLIP, there is a significant overlap between the scores assigned to normal and anomaly instances, and at the same time, the score range is considerably wider. These results indicate that AnomalyCLIP achieves a significant improvement in adapting CLIP to ZSAD. Anomaly score map for different datasets. In addition to the similarity score for anomaly clas- sification, we also visualize the anomaly score maps to present the strong anomaly segmentation ability of AnomalyCLIP. Specifically, we visualize the industrial object class: hazelnut, pill, and screw from MVTec AD; candle, chewinggum, capsule, cashew, pcb, and pip fryum from Visa; bracket, metal plate, and tube from MPDD. We also visualize the industrial texture: grid, leather, carpet, tile, wood, and zipper. In addition, we visualize the segmentation in medical domain across photography, endoscopy, and radiology images: skin cancer detection from ISIC; thyroid nodule detection from TN3K; colon polyp detection from Kvasir; brain tumor detection from Br35H. Figure 10: Similarity scores of CLIP on MVTec AD. Each sub-figure represents the visualization of one object. F FINE-GRAINED ZSAD PERFORMANCE In this section, we present the fine-grained data subset-level ZSAD performance in details. 22 0.00.51.0carpetNormalAnomaly0.00.51.0bottleNormalAnomaly0.00.51.0hazelnutNormalAnomaly0.00.51.0leatherNormalAnomaly0.00.51.0cableNormalAnomaly0.00.51.0capsuleNormalAnomaly0.00.51.0gridNormalAnomaly0.00.51.0pillNormalAnomaly0.00.51.0transistorNormalAnomaly0.00.51.0metal_nutNormalAnomaly0.00.51.0screwNormalAnomaly0.00.51.0toothbrushNormalAnomaly0.00.51.0zipperNormalAnomaly0.00.51.0tileNormalAnomaly0.00.51.0woodNormalAnomaly Published as a conference paper at ICLR 2024 Table 15: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly segmen- tation on MVTec AD. Object name Carpet Bottle Hazelnut Leather Cable Capsule Grid Pill Transistor Metal nut Screw Toothbrush Zipper Tile Wood Mean CLIP 11.5 17.5 25.2 9.9 37.4 50.9 8.7 55.8 51.1 43.9 80.1 36.3 51.5 49.9 45.7 38.4 CLIP-AC 10.7 23.3 34.0 5.6 37.5 49.1 11.9 60.8 48.5 53.6 76.4 35.0 44.7 39.1 42.4 38.2 WinCLIP 95.4 89.5 94.3 96.7 77.0 86.9 82.2 80.0 74.7 61.0 89.6 86.9 91.6 77.6 93.4 85.1 VAND 98.4 83.4 96.1 99.1 72.3 92.0 95.8 76.2 62.4 65.4 97.8 95.8 91.1 92.7 95.8 87.6 CoOp 6.7 23.1 30.2 11.7 49.7 35.5 7.8 46.5 50.1 49.3 17.0 64.9 33.4 41.7 31.4 33.3 AnomalyCLIP 98.8 90.4 97.1 98.6 78.9 95.8 97.3 92 71 74.4 97.5 91.9 91.4 94.6 96.5 91.1 Table 16: Fine-grained data-subset-wise performance comparison (PRO) for anomaly segmentation on MVTec AD. Object name Carpet Bottle Hazelnut Leather Cable Capsule Grid Pill Transistor Metal nut Screw Toothbrush Zipper Tile Wood Mean CLIP 2.9 1.4 2.8 0.2 7.3 13.2 0.9 6.0 15.3 2.9 57.8 5.8 17.7 21.5 13.7 11.3 CLIP-AC 1.9 4.9 9.4 0.0 6.9 14.9 2.4 8.2 11.2 10.3 56.2 5.2 15.2 16.3 10.3 11.6 WinCLIP 84.1 76.4 81.6 91.1 42.9 62.1 57.0 65.0 43.4 31.8 68.5 67.7 71.7 51.2 74.1 64.6 VAND 48.5 45.6 70.3 72.4 25.7 51.3 31.6 65.4 21.3 38.4 67.1 54.5 10.7 26.7 31.1 44.0 CoOp 0.5 4.5 4.7 1.8 12.2 5.7 1.0 3.2 9.3 7.0 6.4 16.6 11.6 10.1 5.1 6.7 AnomalyCLIP 90.1 80.9 92.4 92.2 64.4 87.2 75.6 88.2 58.1 71.0 88.0 88.5 65.3 87.6 91.2 81.4 Table 17: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly classifica- tion on MVTec AD. Object name Carpet Bottle Hazelnut Leather Cable Capsule Grid Pill Transistor Metal nut Screw Toothbrush Zipper Tile Wood Mean CLIP 96 45.9 88.7 99.4 58.1 71.4 72.5 73.6 48.8 62.8 78.2 73.3 60.1 88.5 94 74.1 CLIP-AC 93.1 46.1 91.1 99.5 46.6 68.8 63.7 73.8 51.2 63.4 66.7 89.2 36.1 89.0 94.9 71.5 VAND 99.5 92.0 89.6 99.7 88.4 79.9 86.3 80.5 80.8 68.4 84.9 53.8 89.6 99.9 99.0 86.1 CoOp 99.9 87.7 93.5 99.9 56.7 81.1 94.7 78.6 92.2 85.3 88.9 77.5 98.8 99.7 97.7 88.8 AnomalyCLIP 100.0 89.3 97.2 99.8 69.8 89.9 97.0 81.8 92.8 93.6 81.1 84.7 98.5 100.0 96.8 91.5 WinCLIP 100.0 99.2 93.9 100.0 86.5 72.9 98.8 79.1 88.0 97.1 83.3 88.0 91.5 100.0 99.4 91.8 23 Published as a conference paper at ICLR 2024 Table 18: Fine-grained data-subset-wise performance comparison (AP) on for anomaly classification MVTec AD. Object name Carpet Bottle Hazelnut Leather Cable Capsule Grid Pill Transistor Metal nut Screw Toothbrush Zipper Tile Wood Mean CLIP 98.8 78.9 94.6 99.8 70.8 92.1 87.1 93.4 48.1 87.7 91.4 90.7 87.4 95.9 97.9 87.6 CLIP-AC 97.8 79.8 95.9 99.8 64.3 90.9 83.9 93.6 49.9 89.2 86.6 96.0 73.9 96.2 98.3 86.4 WinCLIP 100.0 99.8 96.9 100.0 91.2 91.5 99.6 95.7 87.1 99.3 93.1 95.6 97.5 100.0 99.8 96.5 VAND 99.8 97.7 94.8 99.9 93.1 95.5 94.9 96.0 77.5 91.9 93.6 71.5 97.1 100.0 99.7 93.5 CoOp 100.0 96.4 96.7 100.0 69.4 95.7 98.1 94.2 90.2 96.3 96.2 90.4 99.7 99.9 99.4 94.8 AnomalyCLIP 100.0 97.0 98.6 99.9 81.4 97.9 99.1 95.4 90.6 98.5 92.5 93.7 99.6 100.0 99.2 96.2 Table 19: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly segmen- tation on VisA. Object name Candle Capsules Cashew Chewinggum Fryum Macaroni1 Macaroni2 Pcb1 Pcb2 Pcb3 Pcb4 Pipe fryum Mean CLIP 33.6 56.8 64.5 43.0 45.6 20.3 37.7 57.8 34.7 54.6 52.1 58.7 46.6 CLIP-AC 50.0 61.5 62.5 56.5 62.7 22.9 28.8 51.6 38.4 44.6 49.9 44.7 47.8 WinCLIP 88.9 81.6 84.7 93.3 88.5 70.9 59.3 61.2 71.6 85.3 94.4 75.4 79.6 VAND 97.8 97.5 86.0 99.5 92.0 98.8 97.8 92.7 89.7 88.4 94.6 96.0 94.2 CoOp 16.3 47.5 32.5 3.4 21.7 36.8 27.5 19.8 22.9 18.0 14.0 29.2 24.2 AnomalyCLIP 98.8 95.0 93.8 99.3 94.6 98.3 97.6 94.1 92.4 88.4 95.7 98.2 95.5 Table 20: Fine-grained data-subset-wise performance comparison (PRO) for anomaly segmentation on VisA. Object name Candle Capsules Cashew Chewinggum Fryum Macaroni1 Macaroni2 Pcb1 Pcb2 Pcb3 Pcb4 Pipe fryum Mean CLIP 3.6 15.8 9.6 17.8 12.1 8.1 20.9 11.7 12.8 31.7 17.1 16.7 14.8 CLIP-AC 6.0 22.4 10.9 30.2 29.3 13.4 18.4 12.5 13.9 23.6 20.3 6.0 17.3 WinCLIP 83.5 35.3 76.4 70.4 77.4 34.3 21.4 26.3 37.2 56.1 80.4 82.3 56.8 VAND 92.5 86.7 91.7 87.3 89.7 93.2 82.3 87.5 75.6 77.8 86.8 90.9 86.8 CoOp 1.1 18.4 1.7 0.1 2.6 18.1 2.7 0.1 0.7 0.0 0.0 0.6 3.8 AnomalyCLIP 96.2 78.5 91.6 91.2 86.8 89.8 84.2 81.7 78.9 77.1 91.3 96.8 87.0 Table 21: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly classifica- tion on VisA. Object name Candle Capsules Cashew Chewinggum Fryum Macaroni1 Macaroni2 Pcb1 Pcb2 Pcb3 Pcb4 Pipe fryum Mean CLIP 37.9 69.7 69.1 77.5 67.2 64.4 65 54.9 62.6 52.2 87.7 88.8 66.4 CLIP-AC 33.0 75.3 72.7 76.9 60.9 67.4 65.7 43.9 59.5 49.0 89.0 86.4 65.0 VAND 83.8 61.2 87.3 96.4 94.3 71.6 64.6 53.4 71.8 66.8 95.0 89.9 78.0 CoOp 46.2 77.2 75.7 84.9 80.0 53.6 66.5 24.7 44.6 54.4 66.0 80.1 62.8 AnomalyCLIP 79.3 81.5 76.3 97.4 93.0 87.2 73.4 85.4 62.2 62.7 93.9 92.4 82.1 WinCLIP 95.4 85.0 92.1 96.5 80.3 76.2 63.7 73.6 51.2 73.4 79.6 69.7 78.1 24 Published as a conference paper at ICLR 2024 Table 22: Fine-grained data-subset-wise performance comparison (AP) for anomaly classification on VisA. Object name Candle Capsules Cashew Chewinggum Fryum Macaroni1 Macaroni2 Pcb1 Pcb2 Pcb3 Pcb4 Pipe fryum Mean CLIP 42.9 81.0 83.4 90.4 82.0 56.8 65.0 56.9 63.2 53.0 88.0 94.6 71.5 CLIP-AC 40.0 84.3 86.1 90.2 76.6 58.7 65.8 48.4 59.8 47.6 90.6 93.7 70.1 WinCLIP 95.8 90.9 96.4 98.6 90.1 75.8 60.3 78.4 49.2 76.5 77.7 82.3 81.2 VAND 86.9 74.3 94.1 98.4 97.2 70.9 63.2 57.2 73.8 70.7 95.1 94.8 81.4 CoOp 52.9 85.3 87.1 93.1 90.2 52.3 62.2 36.0 47.3 54.8 66.3 89.7 68.1 AnomalyCLIP 81.1 88.7 89.4 98.9 96.8 86.0 72.1 87.0 64.3 70.0 94.4 96.3 85.4 Table 23: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly segmen- tation on MPDD. Object name Bracket black Bracket brown Bracket white Connector Metal plate Tubes Mean CLIP 85.3 26.9 83.5 56.5 64.3 56.4 62.1 CLIP-AC 86.4 31.5 77.4 52.9 52.5 51.5 58.7 WinCLIP 57.8 72.2 79.5 79.0 92.6 77.6 76.4 VAND 96.3 86.2 99.0 90.6 93.1 99.1 94.1 CoOp 9.3 20.2 8.3 7.6 14.1 33.2 15.4 AnomalyCLIP 95.7 94.4 99.8 97.2 93.8 98.1 96.5 Table 24: Fine-grained data-subset-wise performance comparison (PRO) for anomaly segmentation on MPDD. Object name Bracket black Bracket brown Bracket white Connector Metal plate Tubes Mean CLIP 62.6 2.8 47.9 22.8 31.5 30.4 33.0 CLIP-AC 58.9 4.0 41.6 20.2 27.0 22.9 29.1 WinCLIP 43 25.0 57.6 44.6 78.2 44.7 48.9 VAND 89.7 70.3 93.1 74.5 74.5 96.9 83.2 CoOp 1.5 0.4 0.0 0.0 0.2 11.5 2.3 AnomalyCLIP 85.2 77.7 98.8 89.8 86.9 93.6 88.7 Table 25: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly classifica- tion on MPDD. Object name Bracket black Bracket brown Bracket white Connector Metal plate Tubes Mean CLIP 32.4 50.9 45.4 75 34.9 87.3 54.3 CLIP-AC 32.8 57.9 42.6 76.2 54.8 72.8 56.2 WinCLIP 41.5 48.6 40.2 79.3 93.4 78.7 63.6 VAND 66.1 64.0 79.6 78.8 53.8 95.9 73.0 CoOp 36.9 43.9 48.9 38.3 77.0 85.4 55.1 AnomalyCLIP 67.3 62.2 64.9 86.9 85.2 95.5 77.0 Table 26: Fine-grained data-subset-wise performance comparison (AP) for anomaly classification on MPDD. Object name Bracket black Bracket brown Bracket white Connector Metal plate Tubes Mean CLIP 47.8 66.2 51.2 62.2 70.6 94.4 65.4 CLIP-AC 48.6 72.0 47.3 61.4 78.5 88.2 66.0 WinCLIP 56.9 69.5 45.1 61.3 97.6 89.1 69.9 VAND 71.7 79.0 82.3 71.8 78.3 98.1 80.2 CoOp 50.0 65.7 57.5 26.4 92.0 93.6 64.2 AnomalyCLIP 72.9 80.8 68.5 76.8 94.7 98.1 82.0 25 Published as a conference paper at ICLR 2024 Figure 11: Similarity scores of CLIP on VisA. Each sub-figure represents the visualization of one object. Figure 12: Similarity scores of AnomalyCLIP on MVTec AD. Each sub-figure represents the visu- alization of one object. 26 0.000.250.500.751.00candleNormalAnomaly0.000.250.500.751.00capsulesNormalAnomaly0.000.250.500.751.00cashewNormalAnomaly0.000.250.500.751.00chewinggumNormalAnomaly0.000.250.500.751.00fryumNormalAnomaly0.000.250.500.751.00macaroni1NormalAnomaly0.000.250.500.751.00macaroni2NormalAnomaly0.000.250.500.751.00pcb1NormalAnomaly0.000.250.500.751.00pcb2NormalAnomaly0.000.250.500.751.00pcb3NormalAnomaly0.000.250.500.751.00pcb4NormalAnomaly0.000.250.500.751.00pipe_fryumNormalAnomaly0.00.51.0carpetNormalAnomaly0.00.51.0bottleNormalAnomaly0.00.51.0hazelnutNormalAnomaly0.00.51.0leatherNormalAnomaly0.00.51.0cableNormalAnomaly0.00.51.0capsuleNormalAnomaly0.00.51.0gridNormalAnomaly0.00.51.0pillNormalAnomaly0.00.51.0transistorNormalAnomaly0.00.51.0metal_nutNormalAnomaly0.00.51.0screwNormalAnomaly0.00.51.0toothbrushNormalAnomaly0.00.51.0zipperNormalAnomaly0.00.51.0tileNormalAnomaly0.00.51.0woodNormalAnomaly Published as a conference paper at ICLR 2024 Figure 13: Similarity scores of AnomalyCLIP on VisA. Each sub-figure represents the visualization of one object. Figure 14: Anomaly score maps for the data subset, hazelnut, in MVTec AD. The first row rep- resents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 15: Anomaly score maps for the data subset, pill, in MVTec AD. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. 27 0.00.20.40.60.81.0candleNormalAnomaly0.00.20.40.60.81.0capsulesNormalAnomaly0.00.20.40.60.81.0cashewNormalAnomaly0.00.20.40.60.81.0chewinggumNormalAnomaly0.00.20.40.60.81.0fryumNormalAnomaly0.00.20.40.60.81.0macaroni1NormalAnomaly0.00.20.40.60.81.0macaroni2NormalAnomaly0.00.20.40.60.81.0pcb1NormalAnomaly0.00.20.40.60.81.0pcb2NormalAnomaly0.00.20.40.60.81.0pcb3NormalAnomaly0.00.20.40.60.81.0pcb4NormalAnomaly0.00.20.40.60.81.0pipe_fryumNormalAnomaly Published as a conference paper at ICLR 2024 Figure 16: Anomaly score maps for the data subset, metal nut, in MVTec AD. The first row rep- resents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 17: Anomaly score maps for the data subset, capsule, in MVTec AD. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmenta- tion results from AnomalyCLIP. Figure 18: Anomaly score maps for the data subset, screw, in MVTec AD. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmenta- tion results from AnomalyCLIP. Figure 19: Anomaly score maps for the data subset candle. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. 28 Published as a conference paper at ICLR 2024 Figure 20: Anomaly score maps for the data subset chewinggum. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 21: Anomaly score maps for the data subset capusle. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 22: Anomaly score maps for the data subset cashew. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 23: Anomaly score maps for the data subset pcb. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. 29 Published as a conference paper at ICLR 2024 Figure 24: Anomaly score maps for the data subset pip fryum. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 25: Similarity scores for the data subset bracket. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 26: Anomaly score maps for the data subset metal plate. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 27: Anomaly score maps for the data subset tube. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. 30 Published as a conference paper at ICLR 2024 Figure 28: Anomaly score maps for the data subset grid. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 29: Anomaly score maps for the data subset leather. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 30: Anomaly score maps for the data subset carpet. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 31: Anomaly score maps for the data subset tile. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. 31 Published as a conference paper at ICLR 2024 Figure 32: Anomaly score maps for the data subset wood. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 33: Anomaly score maps for the data subset zipper. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 34: Similarity scores for the data subset skin. Figure 35: Anomaly score maps for the data subset thyroid. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. 32 Published as a conference paper at ICLR 2024 Figure 36: Anomaly score maps for the data subset colon. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. Figure 37: Anomaly score maps for the data subset brain. The first row represents the input, and we circle the anomaly regions in the second row. The last row presents the segmentation results from AnomalyCLIP. 33
