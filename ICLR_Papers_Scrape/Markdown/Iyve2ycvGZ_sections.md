## Abstract

Flow matching is a powerful framework for generating high-quality samples in various applications, especially image synthesis. However, the intensive com- putational demands of these models, especially during the finetuning process and sampling processes, pose significant challenges for low-resource scenarios. This paper introduces Bellman Optimal Stepsize Straightening (BOSS) technique it aims specifically for a few- for distilling flow-matching generative models: step efficient image sampling while adhering to a computational budget con- straint. First, this technique involves a dynamic programming algorithm that optimizes the stepsizes of the pretrained network. Then, it refines the veloc- ity network to match the optimal step sizes, aiming to straighten the genera- tion paths. Extensive experimental evaluations across image generation tasks demonstrate the efficacy of BOSS in terms of both resource utilization and im- age quality. Our results reveal that BOSS achieves substantial gains in efficiency while maintaining competitive sample quality, effectively bridging the gap be- tween low-resource constraints and the demanding requirements of flow-matching generative models. Our paper also fortifies the responsible development of ar- tificial intelligence, offering a more sustainable generative model that reduces computational costs and environmental footprints. Our code can be found at https://github.com/nguyenngocbaocmt02/BOSS. 1

## Introduction

There have been impressive advancements in deep generative models in recent years, which con- stitute an appealing set of approaches capable of approximating data distributions and generating high-quality samples, as showcased in influential works such as Ramesh et al. (2022); Saharia et al. (2022); Rombach et al. (2022). They are primarily driven by a category of time-dependent genera- tive models that utilize a predefined probability path, denoted as {πt}t∈[0,1]. This probability path is a process that interpolates between the initial noise distribution π0 and the target data distribution π1. The training for these models can be broadly characterized as a regression task involving a neural network function vθ and a target ideal velocity vt(x): L(θ) := Et∈[0,1], Xt∼πt[ℓ(vθ(Xt, t), vt(Xt))]. Here, the velocity network vθ maps any input data x at time t ∈ [0, 1) to a vector-valued velocity quantity vθ(x, t) and plays a crucial role in the generation of samples from the interpolation process through the relationship: (cid:90) 1 X1 = X0 + vθ(Xt, t)dt, 0 which is the solution of the ODE dXt = vθ(xt, t)dt with the boundary condition Xt=0 = X0. A noteworthy class of algorithms that fits within this framework includes denoising diffusion mod- els (Ho et al., 2020; Sohl-Dickstein et al., 2015; Song et al., 2020b) and the more recent flow- 1 Published as a conference paper at ICLR 2024 matching/rectified-flow models (Liu et al., 2022b; Lipman et al., 2022; Albergo & Vanden-Eijnden, 2022; Neklyudov et al., 2023). The latter type of model extends the principles employed in training diffusion models to simulation- free continuous normalizing flows (CNF, Chen et al. 2018). It is particularly attractive because it fixes the suboptimal alignment between noises and images of diffusion models by introducing a straight trajectory formula connecting them. This leads to (empirically observed) faster training and inference time than diffusion models. The rectified flow framework Liu et al. (2022b) also includes a technique called reflow, which gradually rectifies the probability paths. It significantly reduces the number of function evaluations needed for sampling and thus belongs to the family of distillation methods. However, the standard reflow technique proposed in Liu et al. (2022b) requires a significant amount of computational budget: on a small-dimensional dataset such as CIFAR-10 (32×32 pixel images), it takes at least an additional 300,000 retraining iterations of the pretrained velocity networks to reach FID (Fr´echet Inception Distance, Heusel et al. 2017) of 4.85 for 1-step generation. The additional retraining time can reach approximately 200 days of A100 GPU for distilling models for 1-step sampling on higher-dimensional scale datasets to achieve competitive FID, as stated in an extension of the rectified flow framework (Liu et al., 2023). Motivated to fix this problem, in this work, we aim to distill the Rectified Flow model while satisfying the following objective. Given a pretrained flow-matching velocity vθ and a target of K number of function evalua- tions (NFEs), how can we adapt vθ with a proper sampling schedule to generate high-fidelity images using only a modest computational resource? Contributions. We propose BOSS, the Bellman Optimal Stepsize Straightening method, to finetune pretrained flow-matching models. Our proposal includes two phases. The first phase seeks the optimal K-element sequence ∆∗ for the initial model vθ. The second phase utilizes ∆∗ to retrain vθ such that the retrained model vθ∗ performs better. With the proposed procedure, we straighten the velocity network with just about 10,000 retraining iterations while outperforming the standard reflow strategies regarding image quality. Quantitatively, our procedure consistently achieves lower FID in unconditional image generation with four different datasets. Furthermore, as the additional results in Appendix E show, the straightening procedure using Low-Rank Adaptation (LoRA) can finetune only 2% of the model’s parameters, yet it performs competitively to that of full-rank finetuning. Related works. Our approach is directly related to existing works on improving the sampling ef- ficiency of diffusion and flow-matching models with training-based algorithms. In Salimans & Ho (2021), the authors proposed an approach to enhance the sampling speed of unguided diffusion models through iterative distillation. This is extended to the case of classifier-free guided diffusion models in Meng et al. (2023). In Wang et al. (2023), the authors propose a method leveraging re- inforcement learning to automatically search for an optimal sampling schedule for Diffusion Prob- abilistic Models (DPMs), addressing limitations in hand-crafted schedules and the assumption of uniformity across instances. Our work has a few common features and motivations with Watson et al. (2021), which achieved significant speed-ups through dynamic programming and decomposed loss terms. However, their focus on individual Kullback–Leibler divergence loss that neglects the cumulative information loss during sampling. This results in images with reduced overall quality. In contrast, we focus on minimizing the local truncation error during the sampling procedure, which improves the image quality consistently across all budgets of NFEs. Moreover, we propose a fine- tuning method that allows faster sampling with just a few NFEs. Recent work by Song et al. (2023) introduced a framework that learns a model capable of mapping any point at any time to the tra- jectory’s starting point, called the consistency model. After submitting this work, we discovered a concurrent study by Li et al. (2023). This work pointed out that using uniform stepsizes is subopti- mal for diffusion model sampling and instead used evolutionary algorithms to search for the optimal stepsizes and score network architectures, with the FID score being the optimized metric. Within the context of Rectifed Flow/Flow Matching, Liu et al. (2022b) proposed a reflow method that uses retraining to straighten the probability sampling path. This results in a low NFE sampling with favorable image quality. The recent work of Liu et al. (2023) takes this framework to a larger scale, demonstrating impressive results on high-resolution image datasets. However, both rely on computational intensive retraining procedures, which we improve in our work. 2 Published as a conference paper at ICLR 2024 The other direction that aims to accelerate the sampling process of diffusion/flow matching models is training-free samplers (Song et al., 2020a; Bao et al., 2022; Liu et al., 2022a; Tachibana et al., 2021; Zhang & Chen, 2022; Karras et al., 2022; Lu et al., 2022; Zheng et al., 2023). Although required no additional training step, these works mainly relied on the properties of the SDE/probability flow ODE dynamics to propose heuristic solvers/diffusion noise schedulers. Therefore, verifying whether the proposed sampling stepsizes are optimal is hard. 2 BACKGROUND Suppose we are given a (pretrained) model vθ(Xt, t) with parameter θ, which is an estimator of the function v from an ordinary differentiable equation (ODE) on the span t ∈ [0, 1]: dXt = v(Xt, t)dt. (1) In generative modeling with diffusion/flow matching models, this dynamic system is called the probability flow ODE (Song et al., 2020b; Lipman et al., 2022). The estimator vθ allows us to flow from the distribution π0 (noises) to the distribution π1 (real images) via the equation: X1 = X0 + (cid:90) 1 0 vθ(Xt, t)dt, (2) where X0 ∼ π0 and X1 ∼ π1. In the context of our problem, X0 is observable. X1 is only determined by the equation (2) which is a deterministic process that for each X0 = x0, there is only value X1 = x1 coupling with it through the following equation: x1 = x0 + (cid:90) 1 0 vθ(xt, t)dt. We are interested in the low-cost estimate of the integral (cid:82) 1 0 vθ(xt, t)dt with respect to t over the interval [0, 1]. It is an essential concern when calculating the velocity field is computationally ex- pensive. The amount of times calling vθ is defined as the number of function evaluations (NFE). First Order Sampling scheme. To solve for the integration that appears in the sampling equa- tion (2), it is necessary to invoke a numerical integrator that uses discretized time steps. Any numer- ical integration scheme will induce truncation errors, which can be quantified in two forms: first, when we have the value at the previous time step Xτ −δ, the solver estimates the subsequent true value Xτ as ˆXτ , causing a local truncation error Xτ − ˆXτ . These local errors accumulate over the number of intervals, eventually resulting in a cumulative error known as the global truncation error. The most popular discretization scheme is perhaps Euler’s method: given a budget K number of function evaluations (NFEs), the Euler uniform sampling computes the interval ∆ = 1/K, and the sample successively xi k/K = xi (k−1)/K + vθ(xi (k−1)/K, (k − 1)∆) × ∆ ∀k = 1, . . . , K, 0 ∼ π0. We denote this uniform sampling scheme by E U (K). Euler’s with the initial condition xi method with uniform stepsizes ∆ has local truncation error O(∆2), and global truncation error O(∆). One can generalize the Euler sampling with non-uniform intervals by dividing the time domain [0, 1] into unequal intervals with timestamps 0 = τ0 < τ1 . . . < τK = 1, and sample successively (3) xi τk = xi τk−1 + vθ(xi τk−1 , τk−1) × (τk − τk−1) ∀k = 1, . . . , K, (4) with the initial condition xi 0 ∼ π0. The timestamps equivalently determine the stepsize τk − τk−1 for each sampling iteration. We denote this scheme by E({τ0, τ1, . . . , τK}). If the timestamps τk are equally spaced in [0, 1], then we obtain the equivalence E({τ0, τ1, . . . , τK}) ≡ E U (K). Since the reflow procedure in Liu et al. (2022b) deals exclusively with Euler’s method for being the fastest with a fixed computational budget, we focus only on this method in our paper. 3 OPTIMAL SAMPLING STEPSIZES The objective presented in Section 1 can be defined more rigorously as follows. Given a pretrained model vθ and an insignificant value of K, we aim to find the optimal value θ∗ and sequence ∆∗ such 3 Published as a conference paper at ICLR 2024 that E(., ∆∗) is a reasonable estimate for the coupling sample x1 of any x0. This can be posed as an integer optimization problem of finding the best schedule for sampling. Given a fixed budget of K NFEs, we find a schedule {τ0, τ1, . . . , τK} satisfying 0 = τ0 < τ1 < . . . < τK = 1 and that the associated Euler non-uniform sampling scheme E({τ0, τ1, . . . , τK}) has minimal sampling error for the pretrained velocity vθ. In Section 3.1, we describe our estimate of the sampling error for any valid schedule. Section 3.2 presents an integer programming formulation to find the optimal stepsizes for sampling, and Section 3.3 provides a dynamic programming algorithm to find the optimal schedule. 3.1 SAMPLING ERROR ESTIMATION Given any two arbitrary timestamps 0 ≤ tj < tk ≤ 1, we are interested in estimating the local Euler truncation error, i.e., measuring the discrepancy between the true value Xtk = Xtj + (cid:90) tk tj vθ(Xt, t)dt and the one-step Euler sampling value X E tk = Xtj + vθ(Xtj , tj) × (tk − tj), where Xtj is sampled from the distribution πtj which is induced by the initial distribution π0 of X0 and the ODE (1). This local truncation error can be formalized as ctruncation jk := EXtj ∼πtj (cid:104)(cid:13) (cid:13) (cid:13) (cid:90) tk tj (cid:13) 2 (cid:13) vθ(Xt, t)dt − vθ(Xtj , tj)(tk − tj) (cid:13) 2 (cid:105) . Unfortunately, computing ctruncation operator and the integration. We instead employ the following two simplifications: is computationally intensive because of both the expectation jk 1. We fix the possible choice of time-stamps: for a sufficiently large number K max, the anchoring timestamps are {tk}k=0,...,Kmax with tk = k/K max. In doing so, we have restrained the space of all possible sampling schedules to the combinations of finite anchoring timestamps {tk}. Later in the numerical experiments, we choose K max = 100, leading to the anchoring timestamps {0, 0.01, 0.02, . . . , 0.99, 1}. 2. We approximate the local truncation error ctruncation for any two anchoring timestamps tj < tk jk by a sample average estimator cjk that is constructed as follows: cjk = 1 N N (cid:88) i=1 ci jk, where jk = ∥xi ci tk − xi tj − vθ(xi tj , tj) × (tk − tj)∥2 2, (5) where for sample i, the noise xi uniform sampling path starting from xi 0 is drawn from π0, and xi tj and xi tk are extracted from Euler 0, taken at time tj and tk, respectively. Figure 1 illustrates how the sampling error ci jk are calculated for the simple case with K max = 5 (or equivalently, with a uniform interval ∆ = 0.2). First, noise xi 0 is drawn from π0, and the blue curve depicts the nonlinear trajectory following the ODE (1). The piecewise linear trajectory is the path generated by the uniform Euler sampling with K max NFEs, leading to the observed trajectory {xi k}k=0,...,Kmax. For a concrete example of computing ci 25, we mea- sure the difference between the value of a one- step Euler sampling from t2 to t5 with a step- size t5−t2 = 3∆ to obtain xi 2, t2)×3∆, and the observed value xi 5. Intuitively, we can view cjk as the difference between the Euler one-step and the Euler (k − j)-step uniform sampling between tj and tk. 2+vθ(xi Figure 1: An example with K max = 5 to illustrate the computation of the sampling error. 4 Published as a conference paper at ICLR 2024 Figure 2: A network flow formulation to find the optimal sampling schedule for image generation. Time t0 = 0 represents noise, while tKmax = 1 is the terminal data (images). Each discretized timestamp is represented by a node, with edges reflecting the one-dimensional flow of time from noise to image. The cost cjk associated with each edge is the sampling error estimate, measured by the average difference between the Euler one-step and the Euler (k − j)-step sampling between tj and tk, see Section 3.1. jk One may observe that cjk is only an approximation of the true local truncation error ctruncation because cjk is computed based on the Euler trajectory (red piecewise linear path in Figure 1), while the truncation error ctruncation should be computed based on the nonlinear trajectory of the ODE (blue curve in Figure 1). One downside of using cjk is that for any two consecutive timestamps tj and tj+1, we have cj,(j+1) = 0. This downside can be mitigated by taking K max sufficiently large. On the other hand, as K max gets large, calculating all the values cjk is computationally intensive because there are, in total, K max(K max − 1)/2 pair of timestamps whose errors are to be computed. Nevertheless, we demonstrate empirically in Section 5 that even when cjk is computed using a small number N of samples, the resulting optimal schedule already demonstrates a superior performance vis-`a-vis competing methods. jk 3.2 INTEGER PROGRAMMING FORMULATION As we now describe, finding the optimal sampling schedule can be formulated as a network-flow- based problem (Ahuja et al., 1993). First, construct a graph of K max +1 nodes; each node represents one timestamp, see Figure 2. There is an edge connecting node tj to node tk if tj < tk, and this edge is associated with a sampling error cost cjk, computed in Section 3.1. For a target of K NFEs, the optimal sampling schedule is a path that traverses from the source node t0 to the sink node tKmax that is comprised of exactly K edges. This path can be recovered from the optimal solution of the problem Kmax−1 (cid:88) Kmax (cid:88) min cjkzjk j=0 k=j+1 s. t. (cid:80)Kmax−1 (cid:80)Kmax j=0 (cid:80)Kmax k=j+1 zjk = K k=1 z0k = 1, (cid:80)Kmax−1 k=0 zkj = (cid:80)Kmax k=j+1 zjk j=0 (cid:80)j−1 zjk ∈ {0, 1} zjKmax = 1 (6) 1, K max−1 ∀j ∈ ∀0 ≤ j < k ≤ K max. (cid:74) (cid:75) Above, zjk ∈ {0, 1} is a binary decision variable, zjk = 1 if the path takes a one-step sampling from time tj to time tk. The objective function of (6) minimizes the path’s accumulated sampling error, which approximates the global truncation error of the Euler sampling with the corresponding step sizes. The first constraint indicates that the path should consist of exactly K edges; the second constraint imposes that t0 and tKmax are the source and sink nodes, respectively. Finally, the last set of constraints is the flow conservation on each intermediary node between t0 and tKmax. 5 Published as a conference paper at ICLR 2024 3.3 DYNAMIC PROGRAMMING ALGORITHM While the integer programming problem (6) can be solved using commercial solvers such as GUROBI or using network flow algorithms (see Skiena (2008, §6) for an example), there are prac- tical cases in which we need to find optimal paths for multiple values of the budget K NFEs. A convenient way to address this computation is to leverage a dynamic programming formulation, which successively builds up the error-to-go function at each node and for each number of remain- ing NFEs. To this end, for any timestamp t , we define the error-to-go function as (cid:98)j and any number of remaining NFEs (cid:98)k ∈ 1, K max (cid:74) (cid:75) V ((cid:98)j, (cid:98)k) :=    Kmax−1 (cid:88) Kmax (cid:88) min cjkzjk j=(cid:98)j k=j+1 s. t. (cid:80)Kmax−1 j=(cid:98)j (cid:80)Kmax k=(cid:98)j+1 z (cid:80)Kmax k=j+1 zjk = (cid:98)k (cid:98)jk = 1, (cid:80)Kmax−1 j=(cid:98)j k=j+1 zjk zkj = (cid:80)Kmax (cid:80)j−1 k=(cid:98)j zjk ∈ {0, 1} zjKmax = 1 (7) (cid:98)j + 1, K max−1 ∀j ∈ (cid:75) (cid:74) ∀(cid:98)j ≤ j < k ≤ K max. The error-to-go V ((cid:98)j, (cid:98)k) is the minimal sampling error accumulated from time t (cid:98)j to the terminal time tKmax = 1 using exactly (cid:98)k NFEs. It is easy to see that the optimal value of problem (6) is equal to V (0, K). At initialization, we set the base case The dynamic programming update step rolls backward following V ((cid:98)j, 1) = c (cid:98)jKmax ∀ (cid:98)j ∈ 1, K max . (cid:75) (cid:74) ∀k = 2, . . . , K max : V (j, k) = min j<(cid:98)j≤Kmax cj(cid:98)j + V ((cid:98)j, k − 1). (8) The output of the dynamic programming algorithm is the error function V , and one can assess the Bellman optimal schedule by tracing the minimizing path following (8) for each value K of NFEs. 4 STRAIGHTENING FLOWS WITH BELLMAN STEPSIZE Given the Bellman optimal stepsizes, we de- scribe a piecewise linear straightening of the sampling curve. The straightening procedure aims to re-align the velocity network vθ to re- duce the accumulated sampling error at the ter- minal timestamps. For a fixed number of NFEs K, let {τ0, . . . , τK} be the optimal timestamps found in Section 3 with τ0 = 0 and τK = 1, which corresponds to K stepsizes defined by τk − τk−1 for k = 1, . . . , K. We now mod- ify the network weights to straighten the sam- pling path on each interval [τk, τk+1]. An in- tuitive explanation for the straightening pro- cedure is illustrated in Figure 3: here, sup- pose that K = 2, and the optimal schedule is {τ0 = 0, τ1 = 0.4, τ2 = 1}. For the sample i drawn in Figure 3, the Bellman sampling in- 2 → xi duces a two-piece linear path xi 5 (dashed line). If the velocity vectors evaluated at xi 2 align with the dashed line, then the Bellman optimal Euler sampling with K = 2 incurs zero loss compared to the Euler uniform 0 and xi 0 → xi Figure 3: Continued example following Figure 1 for straightening with K = 2 NFEs, evaluated at time t0 = 0 and time t2 = 0.4. Blue arrows are velocity vectors given by the pretrained model, and purple arrows following the dashed lines are the ideal straight path. The straightening proce- dure in Section 4 aims to align the blue arrows towards the purple arrows. Arrows illustrate di- rections and are not drawn with proper scale. 6 Published as a conference paper at ICLR 2024 sampling with K max = 5. This motivates the following alignment procedure: min θ EX0∼π0 (cid:104) K−1 (cid:88) k=0 (cid:13) (cid:13)vθ(X E(Kmax) (cid:13) τk , τk) − X E(Kmax) τk+1 − X E(Kmax) τk τk+1 − τk (cid:105) , (cid:13) 2 (cid:13) (cid:13) 2 where X E(Kmax) is obtained by the Euler uniform sampling with K max NFEs of the initial condition τk X0 ∼ π0. Replacing the expectation with n empirical paths obtained by E(K max), we have the sample averaging optimization problem for straightening min θ 1 n n (cid:88) K−1 (cid:88) i=1 k=0 ∥vθ(xi τk , τk) − − xi xi τk τk+1 τk+1 − τk ∥2 2. (9) We straighten the velocity model using a stochastic gradient descent algorithm to solve the above In the main paper, we train all parameters θ of the pretrained model, whereas in Ap- problem. pendix E, we employ Low-Rank Adaptations to reduce the number of trainable parameters while preserving the performance of the straightening process. 5 NUMERICAL EXPERIMENTS Settings. We evaluate our methods on unconditioned image generation tasks. In particular, we use the CIFAR-10 (Krizhevsky et al., 2009) and three high-resolution (256x256) datasets CelebA-HQ (Karras et al., 2018), LSUN-Church, LSUN-Bedroom (Yu et al., 2015), and AFHQ-Cat. We take the checkpoints of pretrained velocity networks v from the official implementation1 of Rectified Flow (Liu et al., 2022b), which is based on the U-Net architecture of DDPM++ (Song et al., 2020b). If not mentioned otherwise, we evaluate the sample schemes with NFE={4, 6, 8}. The quality of image samples is with Frechet inception distance (FID) score (Heusel et al., 2017). Baselines. A comparison between Bellman optimal stepsizes and the conventional first/second order methods using uniform stepsizes is presented in Section 5.1. We also include an adaptive strategy, the Runge-Kutta method of order 5(4) from SciPy (Virtanen et al., 2020). In Section 5.2, our fine- tuning procedure, Bellman Optimal Stepsize Straightening (BOSS), is compared with two baselines including the Uniform-Reflow, and Distill-k-Reflow introduced in (Liu et al., 2022b). 5.1 IMPROVEMENTS OF FIRST ORDER AND SECOND ORDER SAMPLING SCHEME USING BELLMAN OPTIMAL STEPSIZE First, we benchmark pretrained Euler samplers with uniform and Bellman optimal stepsizes, calcu- lated following Section 3.3. The quantitative results are demonstrated in Table 1. The FID is much lower for samples generated by Euler’s method with the Bellman step, which shows that with Bell- man’s optimal step size, the generated images are of much higher quality in general. Specifically, for sampling on the three larger dimension datasets (256x256), Bellman steps can help drastically reduce FID compared to the uniform step size. This trend is also reflected in Figure 5, which shows our qualitative results. Table 1: FID (↓) of Euler’s sampling method with uniform stepsizes vs. Bellman optimal stepsizes on unconditional image generation task across different datasets. Dataset Euler 51.95 158.95 106.94 68.95 LSUN-Bedroom 84.35 CIFAR-10 CelebA-HQ LSUN-Church AFHQ-Cat 4 NFE 6 NFE 8 NFE Bellman 47.57 92.03 80.91 45.54 61.60 Euler 25.69 127.01 53.85 61.50 39.19 Bellman 23.35 72.54 45.09 36.15 35.35 Euler 16.82 109.42 34.74 56.96 32.15 Bellman 15.74 49.80 33.22 33.94 25.80 We also empirically analyze the effect of Bellman optimal stepsizes on popular ODE solvers includ- ing Euler and Heun (second-order version). To avoid the confusion between schemes, we denote compared methods as follows: 1https://github.com/gnobitab/RectifiedFlow/ 7 Published as a conference paper at ICLR 2024 • Uniform Euler and Uniform Heun are the conventional Euler and Heun methods that use uniform sampling steps. • Bellman Euler and Bellman Heun are two variants of the above methods, but using our proposed Bellman step sizes. • RK45 is an adaptive strategy, the Runge-Kutta method of order 5(4) from Scipy. The quantitative results are displayed in Figure 4. It is evident that employing Bellman optimal stepsizes can significantly improve FID scores (image quality) compared to using uniform stepsizes across all pretrained models on four distinct datasets. The Bellman Heun methods approach the performance of RK45 with substantially fewer NFEs. To elaborate, the Bellman Heun method achieves approximately a 1% gap compared to RK45 with just 20 sampling steps and fully recovers the performance of RK45 with 50 steps. (a) CIFAR-10 (b) CelebA-HQ (c) AFHQ-Cat (d) LSUN-Church (e) LSUN-Bedroom Figure 4: The FID score of sampling methods with different numbers of function evaluations (step sizes). Images generated by samplers using Bellman stepsizes clearly show lower FID than conven- tional ones that use uniform step sizes. Note that Uniform Heun and Bellman Heun are second-order sampling methods that use twice the NFEs. 5.2 EFFECTS OF REFLOW WITH BELLMAN STEPSIZE After calculating the Bellman optimal step size, we follow the procedure described in Section 4 to straighten the probability path. The results, seen in Table 2, suggest that BOSS performs al- most equally with the reflow procedure on CIFAR-10 but markedly better on the other four higher- dimension datasets. This is consistent with the visible improvements in sampled image quality observed in Figure 5. Table 2: FID (↓) of different retraining methods on the unconditional image generation task across different datasets. Distill-K-Reflow is a distillation technique that relies on the reflow of the velocity network on a discrete grid of K uniform stepsizes between 0 and 1, as elaborated in Liu et al. (2022b). Dataset CIFAR-10 CelebA-HQ LSUN-Church AFHQ-Cat LSUN-Bedroom Distill-6-Reflow BOSS-6 Uniform-Reflow 4.80 18.67 17.43 26.10 18.45 4.35 34.56 34.52 46.24 41.17 4.33 43.57 40.45 51.24 45.13 8 4102050131Number of Steps (log-scale)5.010.020.050.0100.02.6FID (log-scale)Uniform EulerUniform HeunBellman EulerBellman HeunRK454102050221Number of Steps (log-scale)20.050.0100.011.5FID (log-scale)Uniform EulerUniform HeunBellman EulerBellman HeunRK454102050205Number of Steps (log-scale)50.0100.021.9FID (log-scale)Uniform EulerUniform HeunBellman EulerBellman HeunRK454102050206Number of Steps (log-scale)20.050.0100.011.4FID (log-scale)Uniform EulerUniform HeunBellman EulerBellman HeunRK454102050209Number of Steps (log-scale)20.050.0100.011.6FID (log-scale)Uniform EulerUniform HeunBellman EulerBellman HeunRK45 Published as a conference paper at ICLR 2024 (a) Euler (6 NFEs) (b) Bellman (6 NFEs) (c) BOSS (6 NFEs) (d) RK45 (208 NFEs) Figure 5: Qualitative results on unconditional image generation task. From first to last row: CelebA- (a)-(b): Comparisons of Euler stepsizes HQ/LSUN-Bedroom/LSUN-Church/AFHQ-Cat dataset. between uniform (a) and the Bellman optimal stepsizes (b); (c)-(d): Comparisons of BOSS retraining and Runge-Kutta-45 sampling. Notice our proposed BOSS sampling has comparably similar visual quality to RK45 while requiring only 6 NFEs, compared to 208 NFEs of RK45. 6 CONCLUSIONS This paper proposed BOSS, the Bellman Optimal stepsize Straightening method, to adapt pretrained flow-matching models under low computational resource constraints. Our method consists of two phases: first, find optimal sampling stepsizes for the pretrained model, then straighten out the veloc- ity network on each interval of the sampling schedule. We demonstrate empirically that BOSS performs competitively in adapting pretrained models in the image generation task. Similar to training-based samplers for diffusion and flow matching models, a limitation of our method is the additional training cost to output the optimal sample step sizes. There are many potential extensions to our proposed framework to distill a guided velocity network, similar to Meng et al. (2023), or a computationally cheaper algorithm for calculating the Bellman sampling step sizes. Acknowledgments. Viet Anh Nguyen gratefully acknowledges the generous support from the CUHK’s Improvement on Competitiveness in Hiring New Faculties Funding Scheme and the CUHK’s Direct Grant Project Number 4055191. The work of Binh Nguyen is supported by Sin- gapore’s Ministry of Education grant A-0004595-00-00. 9 Published as a conference paper at ICLR 2024 REFERENCES R.K. Ahuja, T.L. Magnanti, and J.B. Orlin. Network Flows: Theory, Algorithms, and Applications. Prentice Hall, 1993. Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2022. Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-DPM: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. arXiv preprint arXiv:2201.06503, 2022. Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in Neural Information Processing Systems, 31, 2018. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for im- proved quality, stability, and variation. In International Conference on Learning Representations, 2018. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion- based generative models. Advances in Neural Information Processing Systems, 35:26565–26577, 2022. Alex Krizhevsky et al. Learning multiple layers of features from tiny images. cs.toronto.edu, 2009. Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji. Autodiffusion: Training-free optimization of time steps and architec- tures for automated diffusion model acceleration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7105–7114, 2023. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow match- ing for generative modeling. In The Eleventh International Conference on Learning Representa- tions, 2022. Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022a. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022b. Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. arXiv preprint arXiv:2309.06380, 2023. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775–5787, 2022. Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and In Proceedings of the IEEE/CVF Tim Salimans. On distillation of guided diffusion models. Conference on Computer Vision and Pattern Recognition, pp. 14297–14306, 2023. Kirill Neklyudov, Rob Brekelmans, Daniel Severo, and Alireza Makhzani. Action matching: Learn- ing stochastic dynamics from samples. In Proceedings of the 40th International Conference on Machine Learning, 2023. Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in GAN evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 10 Published as a conference paper at ICLR 2024 Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text- conditional image generation with CLIP latents. arXiv preprint arxiv:2204.06125, 7, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High- resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Informa- tion Processing Systems, 35:36479–36494, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2021. Steven S. Skiena. The Algorithm Design Manual. Springer, 2008. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learn- ing, pp. 2256–2265. PMLR, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna- tional Conference on Learning Representations, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben In Interna- Poole. Score-based generative modeling through stochastic differential equations. tional Conference on Learning Representations, 2020b. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proceed- ings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 32211–32252. PMLR, 23–29 Jul 2023. Hideyuki Tachibana, Mocho Go, Muneyoshi Inahara, Yotaro Katayama, and Yotaro Watanabe. Quasi-Taylor samplers for diffusion generative models based on ideal derivatives. arXiv preprint arXiv:2112.13339, 2021. Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Courna- peau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. SciPy 1.0: Fundamental algorithms for scientific computing in Python. Nature Methods, 17(3):261–272, 2020. Yunke Wang, Xiyu Wang, Anh-Dung Dinh, Bo Du, and Charles Xu. Learning to schedule in diffu- sion probabilistic models. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 2478–2488, 2023. Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efficiently sam- ple from diffusion probabilistic models. arXiv preprint arXiv:2106.03802, 2021. Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In The Eleventh International Conference on Learning Representations, 2022. Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. DPM-solver-v3: Improved diffusion ODE solver with empirical model statistics. In Thirty-seventh Conference on Neural Information Pro- cessing Systems, 2023. 11 Published as a conference paper at ICLR 2024 A DETAILS OF EXPERIMENTS We use the following checkpoints that are downloaded from the GitHub folder 2: • CIFAR-10: at iteration 800,000; • CelebA-HQ: at iteration 1,000,000; • LSUN-Church: at iteration 1,200,000; • LSUN-Bedroom: at iteration 1,000,000; • AFHQ-Cat: at iteration 1,000,000. The pretrained models are finetuned in 12,000 iterations. One iteration is the passing and back- propagation process for a batch including 15 samples. Due to the similar cost of training between finetuning methods, we report the average GPU hours consumed on each pretrained model up to 12000 iterations, using NVIDIA RTX A5000. • CIFAR-10: 3.56 training hours. • CelebA-HQ: 10.35 training hours. • LSUN-Church: 13.43 training hours. • LSUN-Bedroom: 14.23 training hours. • AFHQ-Cat: 9.30 training hours. With this limited budget of resources, the proposed method, BOSS, achieves significantly better performance than other methods in terms of FID score. The value of N in Equation (5) and K max are fixed at 100, and 100 in all experiments if not mentioned. This setup N = 100 means we only use one batch sampling to calculate the truncation errors between timestamps in equation (5). It is worth noting that this setup highlights the low-cost and limited-resources requirement of our proposal. FID Calculation. All the FID metrics of related works are either cited from previous baselines or are calculated (when there are no such figures reported) based on the Clean-FID paper (Parmar et al., 2022), which unifies the FID calculation to make a fair comparison between papers. These four datasets were downloaded following the instructions from their original papers. We then create the stats file by the clean-fid project. The FID score is calculated based on 50,000 generated images and the stats file. B DESCRIPTION OF THE DYNAMIC PROGRAMMING ALGORITHM This section presents the pseudocode in Algorithm 1 for the practical implementation of the dynamic programming algorithm designed to determine the Bellman optimal stepsizes. The algorithm takes a cost matrix, denoted as c, as input, where cjk is computed using Equation (5) and the specified number of function evaluations (NFEs). The most resource-intensive aspect of this code is the nested loop responsible for calculating κ(j, k), incurring a time complexity of O((K max)2 × K). The choice of K max is crucial, aiming for the Euler sampling method with K max stepsizes to precisely replicate the trajectory of the Ordinary Differential Equation (ODE). Typically, K max falls within the range of 100 to 1,000, ensuring accuracy. Given this range for K max and the variable K ranging from 2 to 1,000, the algorithm executes within milliseconds in all scenarios. 2https://github.com/gnobitab/RectifiedFlow/ 12 Published as a conference paper at ICLR 2024 Algorithm 1 Minimum Cost Path Computation Input: Cost matrix c(j, k) = cjk(j, k = 0, . . . , K max) , the target NFEs K. Set κ(i, j) ← +∞ ∀0 ≤ i ≤ K max, 0 ≤ j ≤ K Set κ(i, 1) ← c(i, K max) ∀0 ≤ i ≤ K max for k = 2 to K do for j = 0 to K max − 1 do for i = j + 1 to K max − 1 do κ(j, k) ← min{κ(j, k), c(j, i) + κ(i, k − 1)} end for end for end for Initialize ψ ← [0], ω ← 0 for k = K to 1 do for j = ω + 1 to K max do if κ(ω, k) = c(ω, j) + κ(j, k − 1) then Append j to ψ and set ω ← j break end if end for end for Append K max to ψ. return ψ, κ(0, K) C EMPIRICAL ANALYSIS ABOUT BELLMAN OPTIMAL STEPSIZES C.1 A COMMON TREND IN BELLMAN OPTIMAL STEPSIZES FOR PRETRAINED MODELS ON DIFFERENT DATASETS We plot the Bellman Optimal stepsizes in Figure 6. It shows a common trend of sampling with small initial steps and then larger stepsizes for intermediate iterations. At K = 6, we can observe that the last step is smaller than the penultimate step, hinting that the sampling process aims to take smaller final steps to refine the output. This refining trend is more evident for K = 8. Figure 6: Optimal Bellman stepsizes for CIFAR-10 and CelebA-HQ at K = 2, 4 and 6. One can identify a common pattern of smaller steps at the beginning and the end of the sampling procedure C.2 EMPIRICAL EVIDENCE FOR THE STEPSIZES TREND This section aims to experimentally explain the trend of smaller stepsizes at the beginning and the end of the sampling procedure. Given a velocity network vθ, we empirically estimate the curvature using the following procedure: 1. Sample N noise instances xi 2. Forward each noise instance using vθ to obtain xi 3. Compute the local curvature for each trajectory: 0 i.i.d. from a Gaussian distribution (we set N ≈ 1000). t for t = 1, . . . , K max. Curvi t = ∥xi t − for t = 2, . . . , K max − 1. t+1 + xi xi 2 t−1 ∥2 2 13 Published as a conference paper at ICLR 2024 4. Calculate the average curvature: (cid:91)Curvt = 1 N N (cid:88) i=1 Curvi t ∀t = 2, . . . , K max − 1. Subsequently, we plot (cid:91)Curvt and investigate whether the Bellman stepsizes coincide with the straightness of the curve, as demonstrated in Figure 7. The curvature of all pretrained rectified models is significant at timestamps close to zero (near the space of noises) and one (near the space of real images). This observation indicates a variant stepsize trajectory, notably at initial and final timestamps. It can be intuitively explained that additional steps are required to match the high- curvature region accurately. Consequently, the outcome of our proposal is sensible, as it correctly identifies the high curvature levels at the beginning and end of the sampling trajectories. (a) CIFAR-10 (b) CelebA-HQ (c) AFHQ-Cat (d) LSUN-Church (e) LSUN-Bedroom Figure 7: Curvature measurement (blue curve) and Bellman optimal timestamps (black horizontal line) for different datasets. We observe that the timestamps are denser at regions with higher esti- mated curvature. C.3 FINDING OPTIMAL STEPSIZES IS A LIGHT-WEIGHT PROCESS In this section, we elaborate further on the efficiency of our framework. This efficiency arises because we only need to pass a single batch of noise through the forward process to obtain the values for each intermediate timestamp. Subsequently, we calculate the local truncation error for any two timestamps. These local truncation errors have in total K max × (K max − 1)/2, and they can be efficiently stored without requiring large memory. The dynamic programming involved in this process is also time-efficient, as elaborated in the Appendix B. For added credibility, we provide a running time of the entire stepsizes calculation process with NFE = 10, detailing the running time of each component across all our datasets in Table 3. All running times are for the Nvidia A5000, an old-generation GPU launched in April 2021. The whole process takes around 115 seconds to complete for the 256x256 datasets. 14 0.00.20.40.60.81.0t012345671e4Bellman stepsizesCurvt0.00.20.40.60.81.0t0.00.51.01.52.01e1Bellman stepsizesCurvt0.00.20.40.60.81.0t0.00.20.40.60.81.01.21.41e1Bellman stepsizesCurvt0.00.20.40.60.81.0t0.00.51.01.52.02.53.03.51e2Bellman stepsizesCurvt0.00.20.40.60.81.0t012341e2Bellman stepsizesCurvt Published as a conference paper at ICLR 2024 Table 3: Running time (seconds) for different datasets CIFAR-10 CelebA-HQ LSUN-Church LSUN-Bedroom AFHQ-Cat One batch forward Local truncation errors Dynamic programming The whole process 8.5977 10.066 0.0134 18.6771 47.7143 67.9143 0.0138 115.6424 47.4024 67.5273 0.0138 114.9435 48.4324 65.9875 0.0139 114.4338 45.5489 66.7532 0.0138 112.3159 Table 4: Bellman Optimal Stepsizes for K = 2 and K = 4. The total sum of stepsizes equals one. K = 2 K = 4 CIFAR-10 CelebA-HQ LSUN-Church [0.28, 0.72] [0.13, 0.87] [0.20, 0.80] [0.12, 0.22, 0.31, 0.35] [0.05, 0.17, 0.32, 0.46] [0.08, 0.21, 0.34, 0.37] Table 5: Bellman Optimal Stepsizes for K = 6 and K = 8. The total sum of stepsizes equals one. K = 6 K = 8 CIFAR-10 CelebA-HQ LSUN-Church [0.09, 0.13, 0.17, 0.21, 0.22, 0.18] [0.03, 0.08, 0.17, 0.26, 0.28, 0.18] [0.06, 0.15, 0.24, 0.26, 0.20, 0.09] [0.07, 0.09, 0.12, 0.14, 0.17, 0.17, 0.14, 0.10] [0.02, 0.05, 0.11, 0.17, 0.21, 0.20, 0.16, 0.08] [0.04, 0.09, 0.15, 0.17, 0.18, 0.17, 0.13, 0.07] The Bellman steps being far from uniform also means that the probability path of the pretrained models is far from straight, and performing the straightening operation would be beneficial. We report in this section the Bellman optimal stepsizes obtained in Section 3.3. We want to focus on the case K = 8 to see the common trend of sampling step sizes. Initially, the sampling process takes small step sizes, possibly for structural determination of the images. The stepsizes become larger for the intermediate steps. The last two stepsizes show a decreasing trend: the sampling process takes small stepsizes at the end to refine and potentially make the final output less noisy. D THE TRANSFER OF OPTIMAL STEPSIZES ACROSS DATASETS To verify the generalization of optimized stepsizes, we transferred the optimized stepsizes from LSUN-Church to the pretrained models on CelebA-HQ and LSUN-Bedroom. The FID scores ob- tained with 4, 6, and 8 NFEs for CelebA-HQ resulting from this transfer are presented in Table 6. Table 6: FID scores for CelebA-HQ with different methods and NFEs Method 4 NFEs 6 NFEs 8 NFEs Uniform Euler Bellman Euler Bellman-transfer 158.95 92.03 132.04 127.01 72.54 100.68 109.42 49.80 72.88 Uniform Euler uses uniform step sizes, Bellman Euler uses optimal stepsizes for CelebA-HQ, while Bellman-transfer uses the stepsizes taken from LSUN-Church. Bellman Euler is still the optimal method. However, what is important here is that Bellman-transfer is better than Uniform Euler. This hints that there is a certain degree of transferability of the step sizes.3 In the empirical realm, we can attribute this transferability to a comparable curvature pattern exhibited by pretrained rectified models, as discussed in Section C.2. 3This is an empirical claim; we do not impose any theoretical claim. 15 Published as a conference paper at ICLR 2024 Table 7 is for the LSUN-Bedroom dataset. We observe the same trend here, empirically confirming that the stepsizes have a certain degree of transferability. Nevertheless, optimizing the stepsizes using Bellman Euler would still obtain the best performance. Table 7: FID scores for LSUN-Bedroom with different methods and NFEs Method 4 NFE 6 NFE 8 NFE Uniform Euler Bellman Euler Bellman-transfer 84.35 61.60 70.23 39.19 35.35 38.01 32.15 25.80 29.14 E LOW-RANK ADAPTATION FOR STRAIGHTENING In this section, we expertiment with adding a low-rank adaptation to the linear and convolutional layers of the velocity network vθ . For instance, the tth linear layer represented by an m × n matrix Wt is adapted to (cid:99)Wt = Wt + AtB⊤ t , where At is an m × r matrix, and Bt is an n × r matrix. The value r ≪ min{m, n} represents the rank of the adaptation. This adaptation is similarly applied to convolutional layers, with a slight adjustment: a convolutional layer is first reshaped into a two-dimensional matrix before incorporat- ing the adaptation term. We keep all original parameters of the models fixed and only update the At and Bt matrices during the straightening process. We have four versions of straightening named LoRA-r, where r is chosen from the set {1, 4, 16, 64}. Their FID scores on the CelebA-HQ dataset over training iterations are plotted in Figure 8, while the number of trainable parameters for each method is presented in Table 8. The FULL-RANK variant described in Section 4 is the method that finetuning all parameters of the original model. It is noticeable that versions LoRA-4, LoRA-16, and LoRA-64 can almost match the FID score of the FULL-RANK version, which is 33.86. Specifically, at 175,000 training iterations, LoRA-4, LoRA-16, and LoRA-64 achieve FID scores of 36.70, 34.52, and 34.20 respectively. Furthermore, LoRA-4 only finetunes 2% of the parameters of the original model but still achieves competitive results compared to the FULL-RANK version. Figure 8: The FID score of straightening procedures along the training iterations on the CelebA-HQ dataset. 16 0255075100125150175Number of training iterations (103)406080100120FIDFID vs Number of Training StepsLoRA-1LoRA-4LoRA-16LoRA-64FULL-RANK Published as a conference paper at ICLR 2024 Table 8: The number of trainable parameters of straightening procedures compared to the full-rank straightening on the CelebA-HQ dataset Method Number of Trainable parameters Percentage on the FULL-RANK version (%) LoRA-1 LoRA-4 LoRA-16 LoRA-64 FULL-RANK 330,562 1,322,248 5,288,992 21,155,968 65,574,549 0.5 2.02 8.07 32.26 100 F ADDITIONAL QUALITATIVE RESULTS Figure 9: Comparison between images generated from an identical noise with different sampling methods and the number of stepsizes on the LSUN-Church dataset. 17 Published as a conference paper at ICLR 2024 Figure 10: Comparison between images generated from an identical noise with different sampling methods and the number of stepsizes on the LSUN-Bedroom dataset. 18 Published as a conference paper at ICLR 2024 (a) BOSS (NFE = 6, FID = 17.3) (b) RK45 (NFE = 203.4, FID = 11.4) Figure 11: Comparative qualitative outcomes of BOSS with NFE = 6. The image on the right showcases the generated images referenced by RK45. Figure 12: Uncurated images generated from the model finetuned by BOSS (NFE = 10, FID=13.89) 19 Published as a conference paper at ICLR 2024 (a) Uniform stepsizes with 4 NFE (b) Bellman optimal stepsizes with 4 NFE (c) BOSS with 2 NFE (d) BOSS with 4 NFE (e) BOSS with 10 NFE (f) RK45 with 211.2 NFE Figure 13: Comparative qualitative outcomes of BOSS with different NFEs on the LSUN-Bedroom dataset. 20 Published as a conference paper at ICLR 2024 (a) Uniform stepsizes with 4 NFE (b) k-step reflow with 4 NFE (c) Bellman optimal sizes with 4 NFE (d) Uniform Reflow with 4 NFE (e) RK45 (f) BOSS with 4 NFE Figure 14: Samples from LSUN-Bedroom. All corresponding samples use the same initial noise. 21
