## Abstract

In this paper, we study inverse game theory (resp. inverse multiagent learning) in which the goal is to ﬁnd parameters of a game’s payoff functions for which the expected (resp. sampled) behavior is an equilibrium. We formulate these problems as generative-adversarial (i.e., min-max) optimization problems, which we develop polynomial-time algorithms to solve, the former of which relies on an exact ﬁrst- order oracle, and the latter, a stochastic one. We extend our approach to solve inverse multiagent simulacral learning in polynomial time and number of samples. In these problems, we seek a simulacrum, meaning parameters and an associated equilibrium that replicate the given observations in expectation. We ﬁnd that our approach outperforms the widely-used ARIMA method in predicting prices in Spanish electricity markets based on time-series data. 1

## Introduction

Game theory provides a mathematical framework, called games, which is used to predict the outcome of the interaction of preference-maximizing agents called players. Each player in a game chooses a strategy from its strategy space according to its preference relation, often represented by a payoff function over possible outcomes, implied by a strategy proﬁle (i.e., a collection of strategies, one- per-player). The canonical outcome, or solution concept, prescribed by game theory is the Nash equilibrium (NE) (Nash, 1950): a strategy proﬁle such that each player’s strategy, ﬁxing the equilib- rium strategies of its opponents, is payoff-maximizing (or more generally, preference-maximizing). In many applications of interest, such as contract design (Holmström, 1979; Grossman & Hart, 1992) and counterfactual prediction (Peysakhovich et al., 2019), the payoff functions (or more generally, preference relations) of the players are not available, but the players’ strategies are. In such cases, we are concerned with estimating payoff functions for which these observed strategies are an equilibrium. This estimation task serves to rationalize the players’ strategies (i.e., we can interpret the observed strategies as solutions to preference-maximization problems). Estimation problems of this nature characterize inverse game theory (Waugh et al., 2013; Bestick et al., 2013). The primary object of study of inverse game theory is the inverse game, which comprises a game with the payoff functions omitted, and an observed strategy proﬁle. The canonical solution concept prescribed for an inverse game is the inverse Nash equilibrium, i.e., payoff functions for which the observed strategy proﬁle corresponds to a Nash equilibrium. If the set of payoff functions in an inverse game is unrestricted, the set of inverse Nash equilibria can contain a wide variety of spurious solutions, e.g., in all inverse games, the payoff function that assigns zero payoffs to all outcomes is an inverse Nash equilibrium, because any strategy proﬁle is a Nash equilibrium of a constant game: i.e., one whose payoffs are constant across strategies. To meaningfully restrict the class of payoff functions over which to search for an inverse Nash equilibrium, one common approach (Kuleshov & Schrijvers, 2015; Syrgkanis et al., 2017) is to assume that the inverse game includes in addition to all the aforementioned objects, a parameter-dependent payoff function for each player, in which case an inverse Nash equilibrium is simply deﬁned as parameter values such that the observed strategy proﬁle is a Nash equilibrium of the parameter-dependent payoff functions evaluated at those values. ⇤Research conducted while the author was an intern at JP Morgan Chase & Co. 1 Published as a conference paper at ICLR 2024 If one assumes exact oracle access to the payoffs of the game (i.e., if there exists a function which, for any strategy proﬁle, returns the players’ payoffs1), the problem of computing an inverse Nash equilibrium is one of inverse multiagent planning. In many games, however, a more appropriate assumption is stochastic oracle access, because of inherent stochasticity in the game (Shapley, 1953) or because players employ randomized strategies Nash (1950). The problem of computing an inverse Nash equilibrium assuming stochastic oracle access is one of inverse multiagent learning. One important class of inverse games is that of inverse Markov games, in which the underlying game is a Markov game (Shapley, 1953; Fink, 1964; Takahashi, 1964), i.e., the game unfolds over an inﬁnite time horizon: at each time period, players observe a state, take an action (simultaneously), receive a reward, and transition onto a new state. In such games, each player’s strategy,2 also called a policy, is a mapping from states to actions describing the action the player takes at each state, with any strategy proﬁle inducing a history distribution over histories of play i.e., sequences of (state, action proﬁle) tuples. The payoff for any strategy proﬁle is then given by its expected cumulative reward over histories of play drawn from the history distribution associated with the strategy proﬁle. Excluding rare instances,3 the payoff function in Markov games is only accessible via a stochastic oracle, typically implemented via a game simulator that returns estimates of the value of the game’s rewards and transition probabilities. As such, the computation of an inverse Nash equilibrium in an inverse Markov game is an inverse multiagent learning problem, which is often called inverse multiagent reinforcement learning (inverse MARL) (Natarajan et al., 2010). In many real-world applications of inverse Markov games, such as robotics control (Coates et al., 2009), one does not directly observe Nash equilibrium strategies but rather histories of play, which we assume are sampled from the history distribution associated with some Nash equilibrium. In these applications, we are given an inverse simulation—an inverse Markov game together with sample histories of play—based on which we seek parameter values which induce payoff functions that rationalize the observed histories. As a Nash equilibrium itself is not directly observed in this setting, we aim to compute parameter values that induce a Nash equilibrium that replicates the observed histories in expectation. We call the solution of such an inverse simulation (i.e., parameter values together with an associated Nash equilibrium) a simulacrum. Not only does a simulacrum serve to explain (i.e., rationalize) observations, additionally, it can provide predictions of unobserved behavior. We study two simulacral learning problems, a ﬁrst-order version in which samples histories of play are faithful, and a second-order version in which they are not—a (possibly stochastic) function of each history of play is observed rather than the history itself. Here, the use of the term “ﬁrst-order” refers to the fact that the simulacrum does not necessarily imitate the actual equilibrium that generated the histories of play, since multiple equilibria can generate the same histories of play (Baudrillard, 1994). More generally, if the simulacrum is “second-order,” it is nonfaithful, meaning some information about the sample histories of play is lost. We refer to the problems of computing ﬁrst-order (resp. second-order) simulacra as ﬁrst-order (resp. second-order) simulacral learning: i.e., build a ﬁrst-order (resp. second-order) simulacrum from faithful (resp. non-faithful; e.g., aggregate agent behavior) sample histories of play. We summarize the problems characterizing inverse game theory in Table 1a. Contributions The algorithms introduced in this paper extend the class of games for which an inverse Nash equilibrium can be computed efﬁciently (i.e., in polynomial-time) to the class of normal- form concave games (which includes normal-form ﬁnite action games), ﬁnite state and action Markov games, and a large class of continuous state and action Markov games. While our focus is on Markov games in this paper, the results apply to normal-form (Nash, 1950), Bayesian (Harsanyi, 1967; 1968), and extensive-form games (Zermelo, 1913). The results also extend to other equilibrium concepts, beyond Nash, such as (coarse) correlated Aumann (1974); Moulin & Vial (1978), and more generally,  -equilibrium (Greenwald & Jafari, 2003) mutatis mutandis. First, regarding inverse multiagent planning, we provide a min-max characterization of the set of inverse Nash equilibria of any inverse game for which the set of inverse Nash equilibria is non-empty, assuming an exact oracle (Theorem 3.1). We then show that for any inverse concave game, when the 1Throughout this work, we assume that the oracle evaluations are constant time and measure computational complexity in terms of the number of oracle calls. 2Throughout this paper, a strategy refers to the complete description of a players’ behavior at any state or time of the game, while an action refers to a speciﬁc realization of a strategy at a given state and time. 3For simple enough games, one can express the expected cumulative reward in closed form, and then solve the inverse (stochastic) game assuming exact oracle access. 2 Published as a conference paper at ICLR 2024 Equilibrium Access Direct Faithful Samples Nonfaithful Samples Exact Oracle Inverse Multiagent Planning First-order Simulacral Planning Second-order Simulacral Planning Stochastic Oracle Inverse Multiagent Learning First-order Simulacral Learning Second-order Simulacral Learning (a) Taxonomy of inverse game theory problems. First- order simulacral learning is more commonly known as multiagent apprenticeship learning (Abbeel & Ng, 2004; Yang et al., 2020). Reference (Fu et al., 2021) (Yu et al., 2019) (Lin et al., 2019) (Song et al., 2018) (Syrgkanis et al., 2017) (Kuleshov & Schrijvers, 2015) (Waugh et al., 2013) (Bestick et al., 2013) (Natarajan et al., 2010) This work Game Type Finite Markov Finite Markov Finite Zero-sum Markov Finite Markov Finite Bayesian Finite Normal-Form Finite Normal-Form Finite Normal-Form Finite Markov Finite/Concave Normal-form Finite/Concave Markov Solution Concept Nash Quantal Response Various Quantal Response Bayes-Nash Correlated Correlated Correlated Cooperative Nash/Correlated Any Other Polytime? 7 7 7 7 3 3 3 7 7 3 (b) A comparison of our work and prior work on inverse game theory and inverse MARL. regret of each player is convex in the parameters of the inverse game, an assumption satisﬁed by a large class of inverse games such as inverse normal-form games, this min-max optimization problem is convex-concave, and can thus be solved in polynomial time (Theorem 3.2) via standard ﬁrst-order
