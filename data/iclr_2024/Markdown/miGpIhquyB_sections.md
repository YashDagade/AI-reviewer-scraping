## Abstract

There has been increased interest in using Large Language Models (LLMs) for text dataset generation subject to a desired attribute, e.g., for use in downstream fine-tuning or training. These works generally focus on a single quality metric of the generated text, typically accuracy on a downstream task. However, this fails to consider whether the model even has the ability to faithfully model the data distribution of the desired real-world domain. In contrast, in this work, we additionally focus on important distributional metrics agnostic to the downstream task, such as data diversity and faithfulness. We show that even in simple domains, generated datasets reveal inherent trade-offs between these metrics across models and training regimes. Further, we find that our metrics not only describe the generated dataset, but also capture key aspects of the underlying model. This allows us to characterize the generated datasets, individual models and by comparison the properties of different model families and training paradigms. By focusing on sub-distributions well-represented in the training data of LLMs, we can, for example, show that popular instruction-tuning techniques strongly decrease the LLMâ€™s text generation abilities, with respect to distributional aspects like diversity. 1

## Introduction

In recent years, large language models (large LMs, LLMs), often called foundation models, have become the state-of-the-art on many NLP tasks and beyond. These models can achieve outstanding performance on many tasks, often without any adaption or only with minimal prompting (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022; Touvron et al., 2023a; Bommasani et al., 2021). Need for Task-Specific Data And Dataset Generation The direct application of LLMs can be effective and has the advantage that users do not have to do any additional training or data collection beforehand. However, in practice, smaller custom models that were trained on task-specific data still outperform LLMs, both in terms of task accuracy and hardware efficiency (Ye et al., 2022a; Hsieh et al., 2023; Gao et al., 2023). Recent work also focuses on fine-tuning LLMs themselves on task- specific data, either via standard training (Hu et al., 2022; 2023; Chen et al., 2021), self-improvement (Bai et al., 2022b; Wang et al., 2022b; Haluptzok et al., 2023; Wang et al., 2022a), reinforcement learning from human feedback (RLHF) (Stiennon et al., 2020; Bai et al., 2022a; Ouyang et al., 2022) or even via in-context learning (Brown et al., 2020). Fundamentally, however, all of these
