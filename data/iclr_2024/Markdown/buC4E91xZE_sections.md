## Abstract

Zero-shot anomaly detection (ZSAD) requires detection models trained using aux- iliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, e.g., data privacy, yet it is challenging since the models need to generalize to anoma- lies across different domains where the appearance of foreground objects, abnor- mal regions, and background features, such as defects/tumors on different prod- ucts/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of Anoma- lyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from vari- ous defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP. 1

## Introduction

Anomaly detection (AD) has been widely applied in various applications, such as industrial defect inspection (Bergmann et al., 2019; 2020; Liznerski et al., 2020; Pang et al., 2021a; Roth et al., 2022; Huang et al., 2022; Mou et al., 2022; Chen et al., 2022; You et al., 2022; Ding et al., 2022; Reiss & Hoshen, 2023; Xie et al., 2023; Zhou et al., 2023; Cao et al., 2023) and medical image analysis (Pang et al., 2021a; Tian et al., 2021; Fernando et al., 2021; Qin et al., 2022; Ding et al., 2022; Liu et al., 2023; Tian et al., 2023). Existing AD approaches typically assume that training examples in a target application domain are available for learning the detection models (Pang et al., 2021b; Ruff et al., 2021). However, this assumption may not hold in various scenarios, such as i) when accessing training data violates data privacy policies (e.g., to protect the sensitive information of patients), or ii) when the target domain does not have relevant training data (e.g., inspecting defects in a manufacturing line of new products). Zero-shot anomaly detection (ZSAD) is an emerging task for AD in such scenarios, to which the aforementioned AD approaches are not viable, as it requires detection models to detect anomalies without any training sample in a target dataset. Since anomalies from different application scenarios typically have substantial variations in their visual appearance, foreground objects, and background features, e.g., defects on the surface of one product vs. that on the other products, lesions/tumors on different organs, or industrial defects tumors/lesions in medical images, detection models with strong generalization ability w.r.t. vs. such variations are needed for accurate ZSAD. Recently large pre-trained vision-language models (VLMs) (Radford et al., 2021; Kirillov et al., 2023) have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection (Jeong et al., 2023). Particularly, being ∗Equal contribution. † Corresponding authors. 1 Published as a conference paper at ICLR 2024 (a) (b) (c) (d) (e) (f) Figure 1: Comparison of ZSAD results on (b) test data using (c) original text prompts in CLIP (Radford et al., 2021), (d) tailored text prompts for AD in WinCLIP (Jeong et al., 2023), (e) learnable text prompts for general vision tasks in CoOp (Zhou et al., 2022a), and (f) object-agnostic text prompts in our AnomalyCLIP. (a) presents a set of auxiliary data we can use to learn the text prompts. The results are obtained by measuring the similarity between text prompt embeddings and image embeddings. The ground-truth anomaly regions are circled in red in (a) and (b). (c), (d), and (e) suffer from poor generalization across different domains, while our AnomalyCLIP in (f) can well generalize to anomalies in diverse types of objects from different domains. pre-trained using millions/billions of image-text pairs, CLIP (Radford et al., 2021) has been applied to empower various downstream tasks (Zhou et al., 2022b; Rao et al., 2022; Khattak et al., 2023; Sain et al., 2023) with its strong generalization capability. WinCLIP (Jeong et al., 2023) is a seminal work in the ZSAD line, which designs a large number of artificial text prompts to exploit the CLIP’s generalizability for ZSAD. However, the VLMs such as CLIP are primarily trained to align with the class semantics of foreground objects rather than the abnormality/normality in the images, and as a result, their generalization in understanding the visual abnormality/normality is restricted, leading to weak ZSAD performance. Further, the current prompting approaches, using either manually defined text prompts (Jeong et al., 2023) or learnable prompts (Sun et al., 2022; Zhou et al., 2022a), often result in prompt embeddings that opt for global features for effective object semantic alignment (Zhong et al., 2022; Wu et al., 2023), failing to capture the abnormality that often manifests in fine-grained, local features, as shown in Fig. 1d and Fig. 1e. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. AnomalyCLIP aims to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. It first devises a simple yet universally-effective learnable prompt template for the two general classes – normality and abnormality – and then utilizes both image-level and pixel-level loss functions to learn the generic normality and abnormality globally and locally in our prompt embeddings using auxiliary data. This allows our model to focus on the abnormal image regions rather the object semantics, enabling remarkable zero-shot capability of recognizing the abnormality that has similar abnormal patterns to those in auxiliary data. As shown in Fig. 1a and Fig. 1b, the foreground object semantics can be completely different in the fine-tuning auxiliary data and target data, but the anomaly patterns remain similar, e.g., scratches on metal nuts and plates, the misplacement of transistors and PCB, tumors/lesions on various organ surfaces, etc. Text prompt embeddings in CLIP fail to generalize across different domains, as illustrated in Fig. 1c, but object-agnostic prompt embeddings learned by AnomalyCLIP can effectively generalize to recognize the abnormality across different domain images in Fig. 1f. In summary, this paper makes the following main contributions. • We reveal for the first time that learning object-agnostic text prompts of normality and abnormality is a simple yet effective approach for accurate ZSAD. Compared to current text prompting approaches that are primarily designed for object semantic alignment (Zhou et al., 2022b; Jeong et al., 2023), our text prompt embeddings model semantics of generic abnormality and normality, allowing object-agnostic, generalized ZSAD performance. • We then introduce a novel ZSAD approach, called AnomalyCLIP, in which we utilize an object-agnostic prompt template and a glocal abnormality loss function (i.e., a combination of global and local loss functions) to learn the generic abnormality and normality prompts using auxiliary data. In doing so, AnomalyCLIP largely simplifies the prompt design and can effectively apply to different domains without requiring any change on its learned two 2 Capsule (MVTecAD)Metal nut (MVTecAD)Transistor(MVTecAD)Tile(MVTecAD)Leather(MVTecAD)Carpet(MVTecAD)Auxiliary dataClass1(DAGM)Colon(ColonDB)PCB(Visa)Skin(ISIC)Metal plate(Visa)Brian(Br35H)Test dataOriginal text promptsSimilarity map（CLIP ）Similarity map（WinCLIP）Tailored text promptsSimilarity map（CoOp）Learnable text promptsSimilarity map（AnomalyCLIP）Object-agnostic text prompts Published as a conference paper at ICLR 2024 Figure 2: Overview of AnomalyCLIP. To adapt CLIP to ZSAD, AnomalyCLIP introduces object- agnostic text prompt templates to capture generic normality and abnormality regardless of the object semantics. Then, we introduce glocal context optimization to incorporate global and fine-grained anomaly semantics into object-agnostic text prompt learning. Finally, textual prompt tuning and DPAM are used to enable the prompt learning in the textual and local visual spaces of CLIP. prompts, contrasting to existing
