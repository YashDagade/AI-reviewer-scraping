## Abstract

When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate outputs). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game—which we term the CONSENSUS GAME—in which a GENERATOR seeks to communicate an abstract correctness parameter using natural language sentences to a DISCRIMINATOR. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call EQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading comprehension, commonsense reasoning, mathematical problem-solving, and dialog), EQUILIBRIUM-RANKING consistently, and sometimes substantially, improves performance over existing LM decoding procedures—on multiple benchmarks, we observe that applying EQUILIBRIUM- RANKING to LLaMA-7B outperforms the much larger LLaMA-65B and PaLM- 540B models. These results highlight the promise of game-theoretic tools for addressing fundamental challenges of truthfulness and consistency in LMs. 1

## Introduction

Current language models (LMs) perform quite well on some tasks involving generation or verification of factual assertions—including question answering, fact-checking, and even unconditional text generation. But they are far from perfectly reliable, and there is increasing evidence that LMs actually grow more prone to generating false but frequently repeated statements with increasing scale (McKenzie et al., 2023). Further complicating matters, LMs offer multiple affordances for solving factual generation tasks. They may be used both generatively (e.g. by asking for the most probable answer to a question) or discriminatively (e.g. by presenting a (question, answer) pair and asking whether the answer is acceptable) and, these two procedures do not always produce consistent results: generative procedures may fail when probability mass is spread across multiple contradicting answers (Wang et al., 2022; Mitchell et al., 2022), while discriminative procedures may fail due to miscalibration (Han et al., 2022; Chen et al., 2022) or subtle dependence on question wording (Jiang et al., 2020). Given these noisy and often-conflicting signals, how should we distill out an LM’s best guess at the truth? This paper presents an approach for reconciling generative and discriminative LM decoding proce- dures by formulating decoding as a signaling game (Lewis, 2008) that we call the CONSENSUS GAME. At a high level, this game features a GENERATOR agent that must communicate an abstract correct or incorrect value to a DISCRIMINATOR agent, but may only do so using a set of candidate natural language strings (Fig. 1). Intuitively, an effective strategy for this game (i.e. a joint policy) is one in which the GENERATOR and DISCRIMINATOR agree on the assignment of strings to correctness values. Given such a strategy, we may inspect it to identify candidates agreed by consensus to be correct. Doing so requires solving a multi-step game with a complex (string-valued) action space. In recent years, no-regret learning algorithms have emerged as the preferred technique to compute effective ∗Correspondence to: apjacob@mit.edu 1 Published as a conference paper at ICLR 2024 Figure 1: (Left) Overview of our approach. Differing LM queries fail to exhibit consensus about the answer to a factual question. By reconciling predictions between generative and discriminative LM queries using the CONSENSUS GAME, we obtain an accurate prediction. (Right) Structure of the CONSENSUS GAME, a two- player sequential signaling game with imperfect information. First, the environment (N) uniformly samples a correctness parameter. A GENERATOR (G) conditioned on this parameter produces a natural language string from a set of candidates. The DISCRIMINATOR (D) only observes this string and must predict the correctness parameter sampled by environment. If the DISCRIMINATOR correctly identifies this parameter, then both players receive a reward of 1. The dashed line connects nodes that are indistinguishable by the DISCRIMINATOR, since the DISCRIMINATOR does not observe the correctness parameter. By computing regularized equilibrium strategies for this game, we obtain predictions that reflect a consensus between the GENERATOR and DISCRIMINATOR. strategies for such games, and have been successfully deployed in Poker (Brown & Sandholm, 2018; 2019), Stratego (Perolat et al., 2022), and Diplomacy (Bakhtin et al., 2023; FAIR et al., 2022; Jacob et al., 2022). Here, we show that they can also be applied to free-form language generation tasks. We call this game-theoretic approach to LM decoding EQUILIBRIUM-RANKING. Applied in 6 question answering benchmarks: MMLU (Hendrycks et al., 2020), ARC (Clark et al., 2018), RACE (Lai et al., 2017), HHH (Askell et al., 2021), TruthfulQA (Lin et al., 2022) and, GSM8K (Cobbe et al., 2021), EQUILIBRIUM-RANKING offers substantial improvements over existing generative, discriminative, and mixed decoding procedures. More generally, our results highlight the usefulness of the game-theoretic toolkit for formalizing and improving coherence in LMs. Improved coherence in turn leads to improved accuracy on factual tasks. 2 LANGUAGE MODEL CONSENSUS AS EQUILIBRIUM SEARCH We study the problem of obtaining correct output from a language model, which maps input strings x to output strings y according to some distribution PLM(y | x). While the techniques we present here are general, we focus in this paper on question answering problems consisting of a query x (In which of the following cities was Barack Obama born?) and a set of candidate answers Y (Honolulu, Chicago, . . . ) which may themselves have been sampled from the complete PLM(· | x). Given a set of candidates, we may use them with an LM in (at least) two ways: • Generatively, by supplying as input (i) the query x, (ii) the set of candidates Y, and (iii) a natural language prompt indicating that a correct answer is desired. In this case, the LM may be thought of as modeling a distribution PLM(y | x, correct), where the token correct denotes the fact that the model was prompted to generate a correct answer. • Discriminatively, by supplying as input (i) the query x and (ii) a possible candidate an- swer y ∈ Y, together with (iii) a prompt indicating that a correctness assessment v ∈ {correct, incorrect} is sought. In this case, the language model acts as a model of as modeling a distribution PLM(v | x, y) where v ∈ {correct, incorrect}. These two approaches are conceptually equivalent. But as noted in the introduction, current LMs may give very different answers when queried in these different ways: answers produced generatively might be assessed incorrect with high probability or vice-versa. Research on LMs has proposed 2 corr.incorr.Q: where was Barack Obama born? in Honoluluin Chicagoin NairobiI’m not sureGENERATORDISCRIMINATOREQUILIBRIUM-RANKINGREGULARIZED EQUILIBRIUM SEARCHNGD in Honoluluin Nairobi(1, 1)Dcorr.incorr.correctDDincorrectGin Honoluluin Chicagoin NairobiI’m not surein Honoluluin Chicagoin NairobiI’m not surecorr.incorr.corr.incorr.corr.incorr.(0, 0)(1, 1)(0, 0)(0, 0)(1, 1)(0, 0)(1, 1) Published as a conference paper at ICLR 2024 two broad solutions to this problem. Ensembling
