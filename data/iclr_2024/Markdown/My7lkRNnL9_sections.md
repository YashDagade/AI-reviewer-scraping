## Abstract

“Forward-only” algorithms, which train neural networks while avoiding a back- ward pass, have recently gained attention as a way of solving the biologically un- realistic aspects of backpropagation. Here, we first address compelling challenges related to the “forward-only” rules, which include reducing the performance gap with backpropagation and providing an analytical understanding of their dynam- ics. To this end, we show that the forward-only algorithm with top-down feedback is well-approximated by an “adaptive-feedback-alignment” algorithm, and we an- alytically track its performance during learning in a prototype high-dimensional setting. Then, we compare different versions of forward-only algorithms, focusing on the Forward-Forward and PEPITA frameworks, and we show that they share the same learning principles. Overall, our work unveils the connections between three key neuro-inspired learning rules, providing a link between “forward-only” algorithms, i.e., Forward-Forward and PEPITA, and an approximation of back- propagation, i.e., Feedback Alignment. 1

## Introduction

In machine learning (ML), the credit assignment (CA) problem refers to estimating how much each parameter of a neural network has contributed to the network’s output and how the parameter should be adjusted to decrease the network’s error. The most commonly used solution to CA is the Back- propagation algorithm (BP) (Rumelhart et al., 1995), which computes the update of each parameter as a derivative of the loss function. While this strategy is effective in training networks on com- plex tasks, it is problematic in at least two aspects. First, BP is not compatible with the known mechanisms of learning in the brain (Lillicrap et al., 2020). While the lack of biological plausibility does not necessarily represent an issue for ML, it may eventually help us understand how to address shortcomings of ML, such as the lack of continual learning and robustness, or offer insights into how learning operates in biological neural systems (Richards et al., 2019). Second, the backward pass of BP is a challenge for on-hardware implementations with limited resources, due to high memory and power requirements (Khacef et al., 2022; Kendall et al., 2020). These reasons motivated the development of alternative solutions to CA, relying on learning dy- namics that are more biologically realistic than BP. Among these, several algorithms modified the feedback path carrying the information on the error (Lillicrap et al., 2016a; Nøkland, 2016; Akrout et al., 2019; Clark et al., 2021) or the target (Lee et al., 2015; Frenkel et al., 2021) to each node, while maintaining the alternation between a forward and a backward pass. More recently, “forward-only” algorithms were developed, which consist of a family of training schemes that replace the backward pass with a second forward pass. These approaches include the Forward-Forward algorithm (FF) 1 Published as a conference paper at ICLR 2024 (Hinton, 2022) and the Present the Error to Perturb the Input To modulate the Activity learning rule (PEPITA) (Dellaferrera & Kreiman, 2022). Both algorithms present a clean input sample in the first forward pass (positive phase for FF, standard pass for PEPITA). In FF, the second forward pass con- sists in presenting the network with a corrupted data sample obtained by merging different samples with masks (negative phase). In PEPITA, instead, in the second forward pass the input is modulated through information about the error of the first forward pass (modulated pass). For simplicity, we will denote the first and second forward pass as clean and modulated pass, respectively, for both FF and PEPITA. These algorithms avoid the issues of weight transport, non-locality, freezing of activ- ity, and, partially, the update locking problem (Lillicrap et al., 2020). Furthermore, as they do not require precise knowledge of the gradients, nor any non-local information, they are well-suited for implementation in neuromorphic hardware. Indeed, the forward path can be treated as a black box during learning and it is sufficient to measure forward activations to compute the weight update. Figure 1: Different error transportations and WM configurations. Green arrows mark forward paths and orange arrows indicate error paths. (a) Feedback alignment (FA). (b) Present the Error to Perturb the Input To modulate Activity (PEPITA). (c) PEPITA with WM. (d) Forward-Forward (FF). Despite the promising results, the “forward-only” algorithms are still in their infancy. First, neither Hinton (2022) nor Dellaferrera & Kreiman (2022) present an analytical argument explaining why the algorithms work effectively. Second, they present some biologically unrealistic aspects. For instance, the FF requires the generation of corrupted data and their presentation in alternation with clean data, while PEPITA needs to retain information from the first forward pass until the second forward pass, therefore the algorithm is local in space but not local in time. Third, the algorithms present a performance gap when compared with other biologically inspired learning rules, such as feedback alignment (FA). Our work addresses the mentioned limitations of the “forward-only” al- gorithms, focusing on PEPITA, and compares the FF and the PEPITA frameworks. Here, we make the following contributions: (i) We show that PEPITA effectively implements “feedback-alignment” with an adaptive feedback (AF) matrix that depends on the upstream weights (Sec. 3.1); (ii) Focus- ing on PEPITA, we use the above result to analytically characterize the online learning dynamics of “forward-only” algorithms and explain the phenomenon of alignment of forward weights and top-down connections observed in original work (Sec. 3.2); (iii) We demonstrate that PEPITA can be applied to networks deeper than those tested by Dellaferrera & Kreiman (2022) (Sec. 4); (iv) We propose two strategies to extend the weight mirroring (WM) method to training schemes other than FA and show that PEPITA enhanced with WM achieves better alignment, accuracy, and convergence (Sec. 4); (v) We demonstrate that PEPITA can be approximated to a rule containing a sum of Heb- bian and anti-Hebbian terms, allowing space- and time-locality (Sec. 5.1 and Appendix B); (vi) We analytically compare the Hebbian approximation of PEPITA with the Forward-Forward algorithm (Hinton, 2022) with top-down feedback (Sec. 5.2). 2 BACKGROUND AND RELEVANT WORK 2.1 BIO-INSPIRED LEARNING ALGORITHMS Several training rules for artificial neural networks (ANNs) have been proposed to break the weight symmetry constraint required by BP (Lillicrap et al., 2016a; Nøkland, 2016; Liao et al., 2016; Nøkland & Eidnes, 2019; Frenkel et al., 2021; Hazan et al., 2018; Kohan et al., 2018; 2022; Meule- mans et al., 2021; Halvagal & Zenke, 2023; Journ´e et al., 2022; Launay et al., 2020). Furthermore, 2 a!𝑦−𝑊!FA𝑒𝑊"ℎ!𝑦𝐹!𝑥d!𝑦−𝑊!Forward-Forward𝑒𝑥𝑐𝑙𝑒𝑎𝑛=𝑥!"#𝑥𝑚𝑜𝑑=𝑥$%&𝑊"ℎ!𝑦b!𝑦−𝑊!PEPITA𝑒𝑥𝑐𝑙𝑒𝑎𝑛=𝑥𝑥𝑚𝑜𝑑=𝑥−𝐹𝑒𝑊"ℎ!𝑦𝐹𝐹=𝐹!𝐹"𝑥𝑥𝑥𝑥c!𝑦−𝑊!PEPITA+WM𝑒𝑥𝑐𝑙𝑒𝑎𝑛=𝑥𝑥𝑚𝑜𝑑=𝑥−𝐹𝑒𝑊"ℎ!𝑦−𝐹"𝐹!− Published as a conference paper at ICLR 2024 Table 1: Biological properties and test accuracy [%] on MNIST achieved by a non-exhaustive selec- tion of bio-inspired alternatives to BP. We report the highest accuracy documented in the referenced papers. Legend: ✗ = aspect not solved; ✓ = aspect fully solved; ∼ = aspect partially solved. LEARNING RULE FA DFA DRTP DTP-σ LRA RLRA EFP PEPITA FF PFF SP PEPITA+WM PEPIT-HEBBIAN PEPITA-TL REF. FORWARD ONLY LILLICRAP ET AL. (2016B) NØKLAND (2016) FRENKEL ET AL. (2021) ORORBIA & MALI (2019) ORORBIA & MALI (2019) ORORBIA ET AL. (2020) KOHAN ET AL. (2018) DELLAFERRERA & KREIMAN (2022) HINTON (2022) ORORBIA & MALI (2023B) KOHAN ET AL. (2022) OURS OURS OURS ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ WEIGHT TRANSPORT FREE ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ LOCAL ACTIVITY FREEZING UPDATE UNLOCKED MNIST ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ∼ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ∼ ✓ ✓ ✓ ✓ ✗ ∼ ✓ ✓ ✓ ∼ ∼ ✓ 98.74 98.55 98.52 97.62 98.03 98.18 98.24 98.29 98.84 98.66 98.62 98.42 98.05 92.78 several brain-inspired algorithms have been shown to train ANNs with local information in the frameworks of Predictive Coding (PC) (Rao & Ballard, 1999; van den Oord et al., 2019; Millidge et al., 2022; Salvatori et al., 2021; Ororbia & Mali, 2019; Ororbia et al., 2020) and neural generative coding (Ororbia & Kifer, 2022; Ororbia & Mali, 2023a). Table 1 reports properties of biological plausibility and accuracy metrics for these algorithms. The FA algorithm (Lillicrap et al., 2016a) replaces the transposed forward weights W in the feedback path with random, fixed (non-learning) weight matrices F to deliver error information to earlier layers, thereby solving the weight trans- port problem (Fig. 1a) (Lillicrap et al., 2016a). While FA achieves a performance close to BP on simple tasks such as MNIST (LeCun & Cortes, 2010) and CIFAR-10 (Krizhevsky et al., a) with rel- atively shallow networks, it fails to scale to complex tasks and architectures (Bartunov et al., 2018; Xiao et al., 2018; Moskovitz et al., 2018). Moreover, Hinton & McClelland (1987) proposed the Recirculation algorithm in which the output of an autoencoder is fed back into the network as a ”negative” example. Later, Baldi & Sadowski (2018) showed that Recirculation is a special case of adaptive FA. To improve FA performances, Akrout et al. (2019) proposed the Weight-Mirror (WM) algorithm, which is an approach to adjust the feedback weights in FA to improve their agreement, al- lowing to train neural networks with complex architectures (ResNet-18 and ResNet-50) on complex image recognition tasks (ImageNet). 2.2 THEORETICAL ANALYSES The theoretical study of online learning with 1-hidden-layer neural networks has brought consid- erable insights to statistical learning theory (Saad & Solla, 1995a;b; Riegler & Biehl, 1995; Goldt et al., 2020; Refinetti et al., 2021b). We leverage these works to understand learning with biologi- cally plausible algorithms. Through similar analysis, Refinetti et al. (2021a) analytically confirmed the previous results of Lillicrap et al. (2016b); Nøkland (2016); Frenkel et al. (2021) showing that the key to learning with direct feedback alignment (DFA) is the alignment between the network’s weights and the feedback matrices, which allows for the DFA gradient to be aligned to the BP gra- dient. The authors further show that the fixed nature of the feedback matrices induces a degeneracy- breaking effect where, out of many equally good solutions, a network trained with DFA converges to the one that maximizes the alignment between feedforward and feedback weights. This effect, however, imposes constraints on the structure of the feedback weights for learning, and possibly explains the difficulty of training convolutional neural networks with DFA. Bordelon & Pehlevan (2022) derived self-consistent equations for the learning curves of DFA and FA in infinite-width networks via a path integral formulation of the training dynamics and investigated the impact of biologically plausible rules on feature learning. 2.3 THE PEPITA LEARNING RULE Given a fully connected network with L layers, an input x, and one-hot encoded labels y (Fig. 1b), the learning rule proposed by Dellaferrera & Kreiman (2022) relies on a clean and a modulated 3 Published as a conference paper at ICLR 2024 forward pass through the network. During the clean pass, the hidden unit and output unit activations are computed as: h1 = σ1(W1x), hℓ = σℓ(Wℓhℓ−1) for 2 ≤ ℓ ≤ L, (1) where σl is the non-linearity at the output of the ℓth layer and Wℓ is the matrix of weights between layers ℓ − 1 and ℓ. During the modulated pass, the activations are computed as: 1 = σ1(W1(x − F e)), herr herr ℓ = σℓ(Wℓherr ℓ−1) for 2 ≤ ℓ ≤ L, (2) where y is the target output, e ≡ hL − y denotes the network error, and F is the fixed random matrix used to project the error on the input. We denote the output of the network either as hL or ˆy. After the two forward passes, the weights are updated according to the PEPITA learning rule: 1 ∆W1 = (h1 − herr ∆Wℓ = (hℓ − herr ℓ L−1)⊤. ∆WL = e(herr )(x − F e)⊤; )(herr ℓ−1)⊤ for 2 ≤ ℓ < L; (3) (4) (5) Note that, compared to the original paper, we changed the sign convention to read −F e rather than +F e in eqn. 2 and eqn. 3. Because the distribution of F entries is symmetric around zero, this has no consequence on the results. This change will appear useful when describing the WM applied to PEPITA in Section 4. Finally, the updates are applied depending on the chosen optimizer. For example, using stochastic gradient descent with learning rate η: W (t + 1) = W (t) − η∆W . The pseudocode detailing the algorithm is provided in Appendix A. 2.4 THE FORWARD-FORWARD ALGORITHM Analogously to PEPITA, the FF algorithm removes the need for a backward pass and relies on two forward passes (Fig. 1d). The clean pass and the modulated pass operate on real data and on appropriately distorted data, respectively. The latter is generated to exhibit very different long-range correlations but very similar short-range correlations. In practice, the modulated samples are hybrid images obtained by adding together one digit image times a mask with large regions of ones and zeros and a different digit image times the reverse of the mask. In the clean pass, the weights are updated to increase a “goodness” in the hidden layers (Hinton (2022) proposes to use the sum of the squared activities, or its negation), and in the modulated pass to decrease it. Similarly, the Predictive Forward Forward (PFF) integrates the local synaptic adaptation rule of FF based on a goodness measure and contrastive learning, with lateral competition and aspects of PC, such as its local error Hebbian manner of adjusting generative synaptic weights (Ororbia & Mali, 2023b). 3 THEORETICAL ANALYSIS OF THE LEARNING DYNAMICS OF PEPITA In this section, we present a theory for the “forward-only” learning frameworks, focusing specifically on PEPITA in two-layer networks. We propose a useful approximation of the PEPITA update, that we exploit to derive analytic expressions for the learning curves, and investigate its learning mechanisms in a prototype teacher-student setup. 3.1 TAYLOR EXPANSION AND ADAPTIVE FEEDBACK RULE First, we observe that the perturbation applied in the modulation pass is small compared to the input: ∥F e∥ ≪ ∥x∥. Indeed, in the experiments, the entries of the feedback matrix F are drawn with standard deviation σF = κ/ D, where D is the input dimension – typically large – and κ is a constant set by grid search, while the input entries are of order one. Thus, it is reasonable to Taylor-expand the presynaptic term h1 − herr , which results in the approximate update rule: √ 1 ∆W1 ≃ [(W1F e) ⊙ h′ 1] x⊤, (6) where we have used x instead of (x−F e) since the small perturbation has been found to be negligible for the performance (Dellaferrera & Kreiman, 2022). Eqn. 6 shows that PEPITA is effectively implementing a “DFA-like” update (equivalent to FA in two-layer networks), but using an AF matrix where the random term is modulated by the network weights. In light of this observation, it is natural to expect the alignment between W1F and W2. This simple approximation, which we call 4 Published as a conference paper at ICLR 2024 Adaptive Feedback Alignment (AFA), is actually very accurate, as we verify numerically in Fig. 2a, displaying experiments on MNIST, and in Fig. S3 of Appendix D for the CIFAR-10 and CIFAR-100 (Krizhevsky et al., b) datasets. AFA approximates PEPITA very accurately also in the teacher- student regression task depicted in Fig. 2b, analyzed in the next section. 3.2 ORDINARY DIFFERENTIAL EQUATIONS FOR ADAPTIVE FEEDBACK ALIGNMENT WITH ONLINE LEARNING FOR TEACHER-STUDENT REGRESSION To proceed in our theoretical analysis, it is useful to assume a generative model for the data. We fo- cus on the classic teacher-student setup (Gardner & Derrida, 1989; Seung et al., 1992; Watkin et al., 1993; Engel & Van den Broeck, 2001; Zdeborov´a & Krzakala, 2016). We consider D−dimensional standard Gaussian input vectors x ∼ N (0, ID), while the corresponding label y = ˜W2 ˜σ( ˜W1x) is generated by a two-layer teacher network with fixed random weights ˜W1, ˜W2 and activation func- tion ˜σ(·). The two-layer student network outputs a prediction ˆy = W2 σ(W1x) and is trained with the AFA rule and an online (or one-pass) protocol, i.e., employing a previously unseen example xµ, µ = 1, . . . , N , at each training step, where N is the number of samples. We characterize the dynamics of the mean-squared generalization error (7) 1 2 Ex Ex ϵg ≡ (cid:2)e2(cid:3) , e ≡ ˆy − y, (cid:2)(ˆy − y)2(cid:3) ≡ 1 2 in the infinite-dimensional limit of both input dimension D → ∞ and number of samples N → ∞, at a finite rate (or sample complexity) N/D ∼ OD(1) where the training time is t = µ/D. The hidden-layer size is of order O(1) in both teacher and student. We follow the derivation in the seminal works of Biehl & Schwarze (1995); Saad & Solla (1995d;c), which has been put on rigorous ground by Goldt et al. (2019), and extend it to include the time-evolution of the AF. As discussed in Appendix E, the dynamics of the error ϵg as a function of training time t is fully captured by the evolution of the AF matrix and a set of low-dimensional matrices encoding the teacher-student alignment. We derive a closed set of ordinary differential equations (ODEs) tracking these matrices, and we integrate them to obtain our theoretical predictions. Furthermore, in Appendix E.1, we perform an expansion at early training times t ≪ 1 to elucidate the alignment mechanism and the importance of the “teacher-feedback alignment”, i.e. ˜W2 ˜W1F , as well as the norm of ∥ ˜W1F ∥. For the sake of the discussion, we consider sigmoidal activations ˜σ(·) = σ(·) = erf(·), keeping in mind that the symmetry erf(−x) = −erf(x) induces a degeneracy of solutions. a b c d Figure 2: (a) Test accuracy as a function of epochs for experiments with the MNIST and a 1-hidden- layer network with (1024 hidden units, ReLU). Blue dots mark the “vanilla” PEPITA algorithm (without momentum) while purple crosses mark the AFA approximation (6). (b) Generalization error as a function of time for the experiments with PEPITA (blue dots) and AFA (purple crosses), and the theoretical curves (App. E) marked by full lines. Parameters: D = 500, lr = .05, erf activation, 2 hidden units in both teacher and student. (c) Alignment angle between the teacher and student second-layer weights (dark green), the student and a degenerate solution (light green), and the AF matrix and the student (orange) as a function of time. (d) Direction of the student, the AF and the teacher (and degenerate solutions). Different time shots are marked by vertical dashed lines in panel (c). Fig. 2b shows our theoretical prediction for the generalization error as a function of time, compared to numerical simulations of the PEPITA and AFA algorithms. We find excellent agreement between our infinite-dimensional theory and experiments, even at moderate system size (D = 500). Fig. 2c shows the dynamic changes in the alignment of the second-layer student weights W2 with different matrices: the second-layer teacher weights ˜W2, the second-layer teacher weights of the 5 Published as a conference paper at ICLR 2024 closest degenerate solution, and the AF weights W1F . We clearly observe that the error is stuck at a plateau until “adaptive-feedback alignment” happens (t ∼ 102). Fig. 2d depicts the directions of the two-dimensional vectors W2, W1F , and ˜W2 at three different time shots, marked by vertical dashed lines in Fig. 2c. It is crucial to notice that, in this case, the feedback also evolves in time, giving rise to a richer picture, that we further discuss in Appendix D. In the special case of Fig. 2c, while the direction of W1F is almost constant, its norm increases, speeding up the learning process, as shown in Fig. S4. Similar behavior was observed in Dellaferrera & Kreiman (2022). Notice that, in the case of one hidden layer, FA directly implies gradient alignment given that the weight updates become identical if the feedback matches the feed-forward weights. 4 TESTING PEPITA ON DEEPER NETWORKS, WITH WEIGHT MIRRORING, WEIGHT DECAY AND ACTIVATION NORMALIZATION The theoretical analysis above focuses on a two-layer network, as used in the proof-of-principle demonstration of PEPITA’s function in the original article. To the best of our knowledge, PEPITA has been tested so far only on two-layer networks. Here, we empirically show that the algorithm can be extended to train deeper networks. Furthermore, we propose three techniques that improve the performance of PEPITA and narrow the gap with BP. First, we enhance PEPITA with standard tech- niques used in ML, namely weight decay (WD) (Krogh & Hertz, 1991) and activation normalization (as in Hinton, 2022). In addition, we combine PEPITA with the WM algorithm (Akrout et al., 2019) to train the projection matrix F . Indeed, WM has been shown to greatly improve alignment between the forward and backward connections and the consequent accuracy of FA and, in Section 3.1, we have shown that PEPITA is closely related to FA. In order to apply WM to PEPITA, we propose a generalization of WM for learning rules where the dimensionalities of the feedback and feedfor- ward weights do not match, such as PEPITA, but also including DFA. In our work we focus on fully connected models, and leave the enhancement of convolutional networks for further exploration. Experiments were run on Nvidia V100 GPUs, using custom Python code available here. Figure 3: (a), (b) Test accuracy of fully connected networks with increasing depth trained with PEPITA on the CIFAR-10 dataset. (a) PEPITA uniform and PEPITA normal refer to the initialization of the weights and F (Sec. 4). “PEPITA-Hebbian” refers to the learning rule explained in Section 5.1. (b) Effect of weight decay. (c) Alignment angle between F (Eq. 2) and W1 · W2 during training with or without WM. PreM refers to pre-mirroring (Sec. 4). Hyperparameters are reported in Table S3. The plots indicate mean and standard deviation over 10 independent runs. 4.1
