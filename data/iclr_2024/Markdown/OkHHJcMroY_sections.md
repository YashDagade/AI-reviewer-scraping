## Abstract

Learning an accurate value function for a given policy is a critical step in solving reinforcement learning (RL) problems. So far, however, the convergence speed and sample complexity performances of most existing policy evaluation algorithms remain unsatisfactory, particularly with non-linear function approximation. This challenge motivates us to develop a new path-integrated primal-dual stochastic gradient (PILOT) method, that is able to achieve a fast convergence speed for RL policy evaluation with nonlinear function approximation. To further alleviate the periodic full gradient evaluation requirement, we further propose an enhanced method with an adaptive-batch adjustment called PILOT+. The main advantages of our

## Introduction

In recent years, reinforcement learning (RL) has achieved enormous successes in a large number of areas, including healthcare (Petersen et al., 2019; Raghu et al., 2017b), financial recommendation (Theocharous et al., 2015), ranking system (Wen et al., 2023), resources management (Mao et al., 2016) and robotics (Levine et al., 2016; Raghu et al., 2017a), to name just a few. In RL, an agent interacts with an environment and repeats the tasks of observing the current state, performing a policy- based action, receiving a reward, and transitioning to the next state. Upon collecting a trajectory of action-reward sample pairs, the agent updates its policy with the aim of maximizing its long-term accumulative reward. In this RL framework, a key step is the policy evaluation (PE) problem, which aims to learn the value function that estimates the expected long-term accumulative reward for a given policy. Value functions not only explicitly provide the agentâ€™s accumulative rewards, but are also able to update the current policy so that the agent can visit valuable states more frequently (Lagoudakis & Parr, 2003). Regarding PE, two of the most important performance metrics are convergence rate and sample complexity. First, since PE is a subroutine of an overall RL task, developing fast-converging PE algorithms is of critical importance to the overall efficiency of RL. Second, due to the challenges in collecting a large number of training samples (trajectories of state-action pairs) for PEs in RL, reducing the number of samples (i.e., sample complexity) can significantly alleviate the burden of data collection for solving PE problems. These two important aspects motivate us to pursue a fast-converging PE algorithm with a low sample complexity in this work. 1 Published as a conference paper at ICLR 2024 Among various algorithms for PE, one of the simplest and most effective
