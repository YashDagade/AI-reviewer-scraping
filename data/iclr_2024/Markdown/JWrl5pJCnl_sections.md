## Abstract

Foundation models have significantly advanced in various applications, including text-to-image generation, open-vocabulary segmentation, and natural language processing. This paper presents Instruct2Act, a framework that leverages Large Language Models (LLMs) to convert multi-modal instructions to sequen- tial actions for robotic manipulation tasks. Specifically, Instruct2Act uses LLMs to generate Python programs that form a comprehensive perception, plan- ning, and action loop for robotic tasks. It uses pre-defined APIs to access mul- tiple foundation models, with the Segment Anything Model (SAM) identifying potential objects and CLIP semantically classifying them. This approach com- bines the strengths of foundation models and robotic actions to transform com- plex high-level instructions into precise policy codes. Our approach is adaptable and versatile, capable of handling various instruction modalities and input types, and meeting specific task requirements. We validated the practicality and effi- ciency of our approach on robotic tasks including different tabletop and 6 De- gree of Freedom(DoF) manipulation scenarios in both simulation and real-world environments. Furthermore, our zero-shot method surpasses many state-of-the- art learning-based policies in several tasks. The code for our proposed approach is available at https://anonymous.4open.science/r/Instruct2Act, providing a solid benchmark for high-level robotic instruction tasks with diverse modality inputs. 1

## Introduction

Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020), LLaMA (Touvron et al., 2023), and ChatGPT have made unprecedented progress in generating human-like text and understanding natural language instructions. These models exhibit impressive zero-shot generalization abilities, having been trained on extensive corpora and refined through human feedback. Subsequent work, such as Visual ChatGPT (Wu et al., 2023a), incorporates a various visual foundation models to enable visual drawing and editing through a prompt manager. Similarly, VISPROG (Gupta & Kem- bhavi, 2022) introduces a neuro-symbolic approach for intricate visual tasks, including image un- derstanding, manipulation, and knowledge retrieval. Inspired by the immense potential of merging LLMs with multi-modality foundation models, we aim to develop comprehensive robotic manipu- lation systems. The question we pose is: Can we construct robotic systems akin to ChatGPT that support robotic manipulation, visual goal achievement, and visual reasoning? Developing a versatile robotic system that can perform complex tasks in dynamic environments is a significant challenge in robotics research. Such a system needs to perceive its environments, choose appropriate robotic skills, and sequence them accordingly to accomplish long-term objectives. This requires the integration of various technologies, including perception, planning, and control, to allow the robot to function autonomously in unstructured environments. Inspired by the LLMs’ impres- sive ability to generate simple Python programs from docstrings, CaP (Liang et al., 2022) directly generates the robot-centric policy code based on several in-context example language commands. However, its capabilities are limited to what the perception APIs can provide, and it struggles with interpreting longer and more complex commands due to the high precision requirements of the code. To address these challenges, we propose a novel approach that utilizes multi-modality foundation models and LLMs to simultaneously execute perceptual recognition, task planning, and low-level 1 Under review as a conference paper at ICLR 2024 (a) Robots are able to execute instructions that are provided as input in natural language. (b) Module examples utilized in Instruct2Act. The modules’ definitions are hierarchical and aligned with the robotic system design. Figure 1: A robotic task (a) is executed through the invocation of several modules (b) in Instruct2Act. control modules. Unlike existing
