## Abstract

This paper tackles a challenge in learning the long-term point trajectories in videos, like the Tracking Any Point (TAP) task. Fundamentally, the estimation of point-level motions is hindered by the significant uncertainty inherent in compre- hensive comparisons across the entire video frame. While existing models attempt to mitigate this issue by considering a regularized comparison space (e.g., the cost volumes), point-level motion remains highly noisy, often leading to failures on in- dividual points. To tackle the issue, our key idea is to jointly track multiple points within a given semantic object: since points in an object tend to move together on average, individual noise trajectories can be effectively marginalized, subse- quently obtaining fine-grained motion information. Specifically, we predict the object mask using point-prompted segmentation provided by Segment Anything Models (SAM) and enhance the performance of existing models through a sys- tematic two-stage procedure: (a) estimating the average motion of points within the object mask (predicted by SAM) as the initial estimate, and (b) refining this estimate to achieve point-level tracking. In stage (b), we actively generate fine- grained features around the initial estimate, preserving high-frequency details for precise tracking. Consequently, our method not only overcomes the failure modes seen in existing state-of-the-art

## Introduction

Finding point correspondences over the multiple views of a scene is a crucial challenge in handling visual data, playing a vital role in tasks understanding the geometry of the scene, e.g., 3D recon- struction ( Hartley & Zisserman , 2003 ). Specifically, when the scene features dynamically moving objects and backgrounds, i.e., video data, the problem becomes estimating dense point-level motion over the video frames. For example, optical flow ( Teed & Deng , 2020 ) has widely been adopted, where its goal is predicting the pixel-wise displacements between two frames, assuming that the ma- jority of the pixels are visible in both frames. Consequently, the optical flow is only reliable within short-term frames, and its application can be limited to videos comprising longer frames. To this end, video point tracking has recently emerged as a prominent next direction, overcoming the limitation of the previous approach. For example, the recent Tracking Any Point (TAP) task ( Doersch et al. , 2022 ) has attracted many research focus, which aims to predict the long-term trajectory of a given point, as well as the occlusion probability over the whole frames in a given video. However, tracking motions over the entire video entails comprehensive comparisons across the 3D spatial- temporal coordinates. In turn, the main challenge of point tracking is mostly regarding how to handle the computation burden and uncertainties arising from the large comparison space. For instance, the canonical design in state-of-the-art models ( Karaev et al. , 2023 ; Doersch et al. , 2023 ; Harley et al. , 2022 ) considers the regularized space, referred to as the cost volume ( Xu et al. , 2017 ), which basically represents the likelihood of the point’s location in low-resolution frames, producing a coarse-grained trajectory. Then, they employ refinement modules that smooth away 1 Under review as a conference paper at ICLR 2024 Figure 1: The illustration of the failure mode in point tracking. Even the state-of-the-art baseline, e.g., Doersch et al. ( 2023 ), can fall into the failure mode when the cost volume (i.e., likelihood of the point’s location) is significantly erroneous; for example, when the model fails to capture the high-frequency visual details to represent thin object parts, such as the dog’s ear. Our method mitigates this issue by referring to the estimated instance-level motion and focusing only on the region occupied by the instance to ensure the visual details required for accurate tracking. Indeed, we find that the cost volume in the baseline (left, red circles) is moving in the wrong direction to the query point, while our method (right, green circles) is correctly tracking the dog’s ear. the errors in the initial predictions, which are trained with datasets annotated for tracking, such as the Kubric ( Greff et al. , 2022 ) and PointOdyssey ( Zheng et al. , 2023 ). However, there exist failure modes in these
