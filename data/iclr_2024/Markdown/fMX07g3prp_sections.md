## Abstract

Neural Architecture Search (NAS) has risen to prominence as a pivotal tool for identifying optimal configurations for deep neural networks suited to particular tasks. However, the process of training and assessing numerous architectures in- troduces considerable computational overhead. One approach to mitigate this is through performance predictors, which offer a means to estimate an architecture’s potential without exhaustive training. Given that neural architectures fundamen- tally resemble directed acyclic graphs (DAGs), graph neural networks (GNNs) become an apparent choice for such predictive tasks. Nevertheless, the scarcity of training data can impact the precision of GNN-based predictors. To address this, we introduce a novel GNN predictor for NAS. This predictor renders neural architectures into vector representations by combining both the conventional and inverse graph views. Additionally, we incorporate a tailored feature loss within the GNN predictor to ensure efficient utilization of both types of representations. We subsequently assess our method’s efficacy through experiments on bench- mark datasets including NASBench-101, NASBench-201, and the DARTS search space, with a training data range of 50 to 400 samples. The results demonstrated a notable performance improvement, achieving an enhancement of 3%-16% in terms of prediction accuracy when compared to state-of-the-art GNN predictors across the board. The source code will be made publicly available. 1

## Introduction

Neural Architecture Search (NAS) plays a pivotal role in the automated generation of high- performing deep neural networks. Given a designated train and test dataset, NAS can be viewed as an optimization problem which aims to discover an architecture that maximizes accuracy and other performance metrics within a circumscribed search domain. Typically, NAS algorithms derive feed- back from the performance of numerous probed architectures, facilitating the generation of superior architectural configurations. A number of approaches have been adopted have been judiciously deployed to effectuate NAS tasks, including Reinforcement Learning (Tan et al., 2019), Bayesian Optimization (Kandasamy et al., 2018; White et al., 2021a), and Evolutionary Algorithms (Lu et al., 2019; Sun et al., 2020b), among others Zoph et al. (2018); Wu et al. (2019). However, a neural network’s performance is contingent on both its architecture and weight con- figurations. Evaluating these architectures demands rigorous training and validation on specific datasets, which often leads to high computational costs, sometimes equivalent to thousands of GPU days (Zoph et al., 2018; Ying et al., 2019). In response to this challenge, recent research has focused on approach to streamline evaluations. For example, the weight-sharing approach (Cai et al., 2020; Chu et al., 2021) trains super networks which encapsulate all potential architectures, thereby abbre- viating the training duration. A key advantage of super networks is the ability to inherit weights directly, obviating the need for redundant training. Another emerging approach involves the devel- opment of predictors (Liu et al., 2018; Wen et al., 2020; White et al., 2021a; Wei et al., 2022). Once calibrated on a curated subset of architectures, a predictor is expected to estimate the performance of architectures that have not been empirically tested. In the realm of NAS predictors, the representations of architectures are pivotal. Recognizing neural architectures’ inherent representation as Directed Acyclic Graphs (DAGs)(Ying et al., 2019), it is im- perative to harness such topological information. Some research initiatives have gravitated towards 1 Under review as a conference paper at ICLR 2024 Figure 1: Illustration of our GNN predictor based NAS framework. Given architectures from the search algorithm, they are assessed by the performance predictor and represented using both for- ward and reverse graph encodings. These are processed by two GIN encoders into feature vectors, which are then fed to MLPs for prediction. To enhance model effectiveness, two training losses are incorporated, targeting both features and predictions. serializing these DAGs(Sun et al., 2020a; White et al., 2021a). However, such serialized
