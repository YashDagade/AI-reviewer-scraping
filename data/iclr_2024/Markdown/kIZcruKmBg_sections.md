## Abstract

Physics-informed neural networks (PINNs) effectively embed physical principles into machine learning, but often struggle with complex or alternating geome- tries. We propose a novel method for integrating geometric transformations within PINNs to robustly accommodate geometric variations. Our method incorporates a diffeomorphism as a mapping of a reference domain and adapts the derivative computation of the physics-informed loss function. This generalizes the appli- cability of PINNs not only to smoothly deformed domains, but also to lower- dimensional manifolds and allows for direct shape optimization while training the network. We demonstrate the effectivity of our approach on several prob- lems: (i) Eikonal equation on Archimedean spiral, (ii) Poisson problem on surface manifold, (iii) Incompressible Stokes flow in deformed tube, and (iv) Shape opti- mization with Laplace operator. Through these examples, we demonstrate the en- hanced flexibility over traditional PINNs, especially under geometric variations. The proposed framework presents an outlook for training deep neural operators over parametrized geometries, paving the way for advanced modeling with PDEs on complex geometries in science and engineering. 1

## Introduction

Physics-informed neural networks (PINNs) (Raissi et al., 2019) are simple yet surprisingly pow- erful machine learning approaches to incorporate physical knowledge, in particular, formulated as partial differential equations (PDEs), into the training of neural networks. In the burgeoning field of physics-informed machine learning (Karniadakis et al., 2021), they play a significant role due to their versatile applicability in a wide range of problems in science and engineering, for instance, connecting measurement data and known physics in fluid dynamics (Raissi et al., 2020). Substantial challenges remain, unfortunately, and training PINNs is difficult in practice (Wang et al., 2023). A considerable degree of hyper-parameter tuning, coupled with precise weighting of compet- ing loss terms, is often indispensable to derive satisfactory solutions, irrespective of the challenges associated with determining an effective network architecture or finding a sufficient optimum (Raissi et al., 2019). Training physics-informed neural networks becomes especially challenging in the con- text of complex problems Krishnapriyan et al. (2021), and accurately adhering boundary conditions is tough (van der Meer et al., 2022). Distance functions exhibited potential in precisely enforcing boundary conditions (Sukumar & Sri- vastava, 2022), however, the idea is limited to rather simple geometries where distance functions can be constructed. Addressing the challenges posed by complex geometries, PhyGeoNet (Gao et al., 2021) first proposed the integration of a geometric mapping to accommodate a convolutional neural network architecture to unstructured domains. A geometric mapping has also been used for Fourier neural operators on transformed geometries (Li et al., 2022), but not in a physics-informed manner. Interestingly, hardly any approach of PINNs for manifolds has been proposed, e.g., for problems on surfaces in three-dimensional space, only few works like (Fang et al., 2021), (Costabal et al., 2022) and, to a limited extend, (Bonev et al., 2023) show attempts in this direction. Drawing upon and meticulously synthesizing prior concepts, we propose a novel yet straightforward approach to facilitate the application of physics-informed neural networks (PINNs) to complex or varying geometries, as well as for solving partial differential equations (PDEs) on manifolds. 1 Published as a conference paper at ICLR 2024 Our approach integrates a geometric transformation of a reference domain to represent the compu- tational domain, while concurrently adjusting the derivative computation of the physics-informed loss function. This engenders a latent representation of the solution to the PDE on the reference domain, yielding improved generalization properties across similar geometries and facilitating the implementation of exact boundary conditions. Such a formulation not only extends the applicability of PINNs to smoothly deformed domains, but also lower-dimensional manifolds, and extends its utility to free boundary problems or shape optimization. The paper is divided into three sections: Initially, we outline the formulation of PDEs on transformed geometries. Subsequently, we situate physics-informed neural networks within the realm of trans- formations and manifolds. In the final section, through four illustrative examples, we exhibit diverse applications of our novel approach, thereby unveiling exciting new horizons for physics-informed neural networks. 2 PROBLEM SETTING 2.1 DOMAIN AND TRANSFORMATION Let us consider a diffeomorphism φ, a differentiable function with differentiable inverse, with φ : Ωref → Ω, x (cid:55)→ y, (1) that maps an open and bounded m-dimensional domain Ωref ⊂ Rm to an m-dimensional manifold Ω ⊂ Rn embedded in n-dimensional Euclidean space. We will refer to Ωref as reference domain, for instance, a unit square as illustrated in Figure 1, and to Ω as computational domain, which is the domain where the subsequent problem will be posed. • x φ Ωref ⊂ Rm • y = φ(x) Ω ⊂ Rn Figure 1: A diffeomorphism φ smoothly transforms reference domain Ωref into Ω. 2.2 DIFFERENTIAL EQUATION Consider a generic scalar differential equation in Ω of the form Lu = f, u = g, in Ω, on ∂Ω, (2) (3) where L is an arbitrary continuous differential operator and f : Ω → R a source term. For simplicity, let us denote Dirichlet boundary conditions g : ∂Ω → R only. We assume that system (2)-(3) is well-posed and has a unique and smooth solution u. Incorporating the transformation φ into this system, we distinguish between the following two cases. 2.2.1 MANIFOLD: m < n In case of embedded manifolds, m < n, the system denoted by equations (2)-(3) needs to be formu- lated in terms of directional derivatives. This translates to expressing the system in local coordinates 2 Published as a conference paper at ICLR 2024 Lx(u ◦ φ) = f ◦ φ, u ◦ φ = g ◦ φ, in Ωref, on ∂Ωref, (4) (5) and provides a straightforward formulation for applying PINNs to problems on manifolds. Notably, without this formulation, solving PDEs on manifolds is not possible — just sampling points along the manifold falls short as this does not compute the directional derivatives along the manifold. Some examples using this formulation are presented in Section 4.1 and 4.2. 2.2.2 TRANSFORMATION: m = n In case of m = n, the differential operator can be understood as L = Ly with respect to global coor- dinates, facilitating the interpretation of Ω as a (parametrized) domain possessing constant measure. In this spirit, system (2)-(3) can be reformulated in terms of uref : Ωref → R on the reference domain Ly(uref ◦ φ−1) = f, uref ◦ φ−1 = g, in Ω = φ(Ωref), on ∂Ω = φ(∂Ωref), (6) (7) and offers a versatile way of representing solutions on varying geometries. As uref is defined on a latent representation of the geometry, Ωref, it changes smoothly with variations in φ. Moreover, it is beneficial for imposing exact Dirichlet boundary conditions, as they can be stated on a geometrically simple reference domain. We will demonstrate some applications of formulation (6)-(7) in Section 4.3 and 4.4, after outlining how both formulations can be put into the context of PINNs. 3 PHYSICS-INFORMED NEURAL NETWORKS Physics-informed neural networks are deep neural networks that encode physical effects by training with respect to a loss function incorporating the underlying partial differential equation (Raissi et al., 2019). To approximate the solution of a PDE, let us consider a neural network ˆu : Rn → R that maps coordinates y ∈ Ω to scalar values ˆu(y) ∈ R. If the network’s output depends smoothly on the input coordinates, a differential equation like (2)-(3) can be incorporated into the loss function to guide the update of the weights of the network. Advanced automatic differentiation techniques in modern machine learning frameworks make it feasible to perform the necessary computations, and the universal approximation theorem suggests that we can find a proper solution to our equation. More technically, sampling a set of N loss points yi within Ω and M boundary loss points zi on ∂Ω, we train the neural network with respect to the loss function M SE(ˆu) = 1 N N (cid:88) i=1 [Lˆu(yi) − f (yi)]2 + 1 M M (cid:88) i=1 [ˆu(zi) − g(zi)]2 , (8) that evaluates the differential operator L in a point-wise manner and, additionally, penalizes a devia- tion of our solution from the Dirichlet boundary conditions at the boundary. Provided that the neural network ˆu possesses sufficient expressiveness and a sufficient number of loss points is sampled, ˆu aspires to approximate the solution u to the PDE. This approach is simple, yet very powerful, as it not only allows the solution of a wide range of PDEs, but also easily incorporates (noisy) measurement data or can be used to solve inverse prob- lems, bridging the gap between physics-based solvers and data-driven machine learning (Karni- adakis et al., 2021). 3.1 EXACT BOUNDARY CONDITION WITH OUTPUT TRANSFORM Dirichlet boundary conditions can be imposed exactly by adding an output transform to the net- work’s outputs (Sukumar & Srivastava, 2022; Lu et al., 2021c). To make the approximate solution satisfy the boundary conditions, we construct the approximation ˆu as 3 Published as a conference paper at ICLR 2024 ˆu(y) = N (y)b(y) + g(y), y ∈ Ω, where N is the network output, and b : Ω → R is a smooth (distance) function satisfying b = 0 at the boundary ∂Ω and b > 0 within Ω. For a unit square, for instance, b(y) = q(y1)q(y2) with q(z) = 4z(1 − z) is a viable option. The expressivity of the neural network does not suffer from this output transform, but the approximation ˆu satisfies the Dirichlet boundary values g exactly. However, for complex geometries, formulating a distance function b can pose significant challenges or may even be infeasible. Instead, when writing the problem as a transformation of a simple refer- ence domain, the boundary conditions can be imposed in local coordinates, which makes it relatively easy to impose exact boundary conditions even on complex geometries as we will demonstrate in the following. 3.2 PINNS FOR MANIFOLDS In the manifold case, the PDE has to be rewritten in local coordinates, see formulation (4)-(5), defined over the reference domain. We plug in transformation φ, mapping from local to global coordinates, as an input feature transform to the network ˆu : Rn → R, which is defined in global coordinates. Then, we train the transformed network M(x; ˆu) = (ˆu ◦ φ)(x), x ∈ Ωref, on the reference domain Ωref, such that ˆu results in an approximation to the solution u of system (4)-(5). Remarkably, Dirichlet boundary conditions can exactly be imposed in this setting by adding an output transform that acts on the local domain. If bref : Ωref → R is a distance function on the reference domain, we can construct a transformed network as M(x; ˆu) = (ˆu ◦ φ)(x) bref(x) + (g ◦ φ)(x), x ∈ Ωref, (9) and M satisfies the Dirichlet conditions g on the boundary of the reference domain exactly. 3.3 PINNS FOR TRANSFORMATIONS In the case of transformations (n = m), we solve system (6)-(7) for the differential operator Ly with respect to global coordinates. For this purpose, we map (loss points from) the reference domain Ωref to global coordinates, and understand ˆu : Rn → R as a function in local coordinates. Formulation (6)-(7) motivates to put the transformed network M(y; ˆu) = (ˆu ◦ φ−1)(y), y ∈ φ(Ωref), into the differential operator, which implicitly encodes the derivatives of the transformation. Similar to above, Dirichlet boundary conditions on the reference domain can be imposed by M(y; ˆu) = (ˆu ◦ φ−1)(y) bref(φ−1(y)) + g(y), y ∈ φ(Ωref). In summary, training a neural network ˆu : Rn → R with respect to the loss function M SE(ˆu) = 1 N N (cid:88) i=1 [L (M(yi; ˆu)) − f (yi)]2 , yi = φ(xi), xi ∈ Ωref, (10) results in an approximation M to the solution of system (2)-(3) on Ω = φ(Ωref), and a latent space solution ˆu formulated in terms of local coordinates. This latent ˆu depends smoothly on the geometry transformations φ, and formulation (10) enables the solution of PDEs across complex transformed geometries with exact Dirichlet boundary conditions that are defined by distance functions on a simple reference domain. In the following section, we will demonstrate the use of both formulations in several examples. 4 Published as a conference paper at ICLR 2024 4 EXAMPLES We demonstrate the effectivity of our approach through several representative problems. Through these examples, we highlight the significantly enhanced flexibility offered by our method over traditional PINNs, particularly under geometric variations, unveiling exciting new capabilities for physics-informed neural networks. The list of examples includes two manifold and two transformation cases: (i) Eikonal equation on Archimedean spiral (ii) Poisson problem on surface manifold (iii) Incompressible Stokes flow in deformed tube (iv) Shape optimization with Laplace operator All examples have been implemented using DeepXDE (Lu et al., 2021b), and the full source code is provided in the supplementary material. Unless stated otherwise, we employ fully connected neural networks with 128 nodes in three hidden layers with tanh activation function. Optimization is carried out using PyTorch’s L-BFGS algorithm over 1000 steps. Figure 2: Eikonal equation on Archimedean spiral. 4.1 EIKONAL EQUATION ON ARCHIMEDEAN SPIRAL Our first example serves as a simple benchmark problem to evaluate the accuracy of our method. We consider a one-dimensional manifold Ω given by a mapping of Ωref = [0, 1] ⊂ R to an Archimedean spiral in R2. Choosing l = 3.5π, a = 0.1 and r(x) = ax, it can be written by (cid:18)r(lx) sin(lx) r(lx) cos(lx) ∈ Ω ⊂ R2. φ(x) = (cid:19) In this domain Ω, we solve the (one-dimensional) Eikonal equation ∇u = 1, u = 0, in Ω, on (0, 0), 5 Published as a conference paper at ICLR 2024 and use reformulation (4)-(5) to implement the directional derivatives of the problem, which reads ∇x(u ◦ φ) = 1, u ◦ φ = 0, in Ωref, on {0}. This problem can be easily implemented on the basis of (9) and the numerical result is depicted in Figure 2. The maximum value of the solution of this problem corresponds to the length of the spiral, as long as we parameterize the curve by arc length. The length of the spiral is analytically given by (l(cid:112)1 + l2) + log(l + (cid:112) 1 + l2) ≈ 6.225, a 2 and our transformed PINN finds the exact length with an error of ≈ 0.1%. Figure 3: Poisson problem on surface manifold. 4.2 POISSON PROBLEM ON SURFACE MANIFOLD The second example shows how a transformed PINN can solve PDEs on surface manifolds. As a reference PDE, we consider Poisson’s equation with uniform Dirichlet boundary conditions −∆u = f u = 0 in Ω, on ∂Ω. (11) (12) The manifold Ω is chosen as a part of a sphere, written in polar coordinates by φ(x1, x2) = (cid:33) (cid:32)sin(ψ) cos(θ) sin(ψ) cos(θ) cos(ψ) , ψ = x1 + ψ0, θ = x2 + θ0, parametrized over the reference domain Ωref = [0, 1]2. In our specific example, we choose f ≡ 1, ψ0 = 0.5 and θ0 = 1.0. The resulting manifold and the solution of the problem are depicted in Figure 3. We use reformulation (4)-(5) to express problem (11)-(12) in terms of local coordinates −∆x(u ◦ φ) = f ◦ φ, u ◦ φ = 0, in Ωref, on ∂Ωref. and solve this problem as described in Section 3.2 with strong boundary conditions. The function u : R3 → R is represented by a neural network that maps three-dimensional coordinates, but all derivatives are, due to the formulation in local coordinates, evaluated as directional derivatives along the manifold. To the best of our knowledge, this represents the first instance of a Poisson equation being solved on a manifold utilizing physics-informed neural networks. 6 Published as a conference paper at ICLR 2024 Figure 4: Incompressible Stokes flow in deformed tube. 4.3 INCOMPRESSIBLE STOKES FLOW IN DEFORMED TUBE The third example applies our approach to a physical problem that is ubiquitous in fluid dynamics. An incompressible, steady-state fluid flow can be described by the Stokes equations −∆u + ∇p = 0, div u = 0, in Ω, in Ω, (13) (14) where for Ω ⊂ R2 the unknowns are velocity u = (u, v) : Ω → R2 and pressure p : Ω → R. We examine a flow through a tube from left to right, characterized in terms of (local) Dirichlet boundary conditions on Ωref = [0, 1]2, specifically: u(x1, x2) = 4x2(1 − x2), v(x1, x2) = 0, p(x1, x2) = 0, on ∂Ωref, on ∂Ωref, on {1} × [0, 1]. In our example, we choose a deformed tube Ω that arises from the transformation φ(x1, x2) = (x1, (2x2 − 1)s(x1)) , s(x1) = 0.2 + 0.1 cos(3πx1). The resulting tube geometry and the corresponding flow field are depicted in Figure 4. As we elaborated in Section 3.3, the system defined by (13)-(14) can be solved on Ω = φ(Ωref) using exact boundary conditions imposed on the reference domain, and we train the network for 5000 steps. 7 Published as a conference paper at ICLR 2024 The numerical solution exhibits the anticipated pattern, wherein the fluid velocity significantly in- creases at the narrow segment of the tube, accompanied by a more rapid pressure decrement. It’s noteworthy that all boundary conditions are met exactly, obviating the need for hyper-parameters to strike a balance between inner and boundary loss. Figure 5: Shape optimization with Laplace operator. Initial configuration, one intermediate step, and final solution. The four black squares indicate the constrained points of the transformation. 4.4 SHAPE OPTIMIZATION WITH LAPLACE OPERATOR Our final example reveals novel capabilities engendered by our approach of representing computa- tional domains via transformations of a reference domain. Indeed, this methodology facilitates free boundary problems and shape optimization in a straightforward manner. Topology optimization has previously been executed using PINNs (Lu et al., 2021c), but with a quantity of interest for the inverse design problem embedded within the PDE problem. Let us represent the transformation φ itself by a neural network x (cid:55)→ y, ˆφ : Rm → Rn, which can be learned independently (or concurrently) during the training of a physics-informed neural network. Note that, in general, ˆφ will not be a diffeomorphism, but at least we can assure that it is smooth and differentiable. The newfound flexibility can be demonstrated in the following example, employing the Laplace operator. Having two neural networks ˆu : R2 → R and ˆφ : R → R, we solve problem −∆ˆu = 1, ˆu = 0, in Ω = ˆφ(Ωref), on ∂Ω = ˆφ(∂Ωref), (15) (16) 8 Published as a conference paper at ICLR 2024 where the computational domain Ω is a mapping of Ωref = [0, 1]2 via ˆφ. We impose Dirichlet conditions weakly by adding a penalization term to the loss, as in (8), and train both neural networks simultaneously to minimize this PINN loss. Additionally, we fix four points of domain Ω which imposes constraints to the transformation ˆφ, namely ˆφ(x) = x for x ∈ {(0, 0), (1, 0), (0, 1), (1, 1)}. In our implementation, both neural networks have a single hidden layer with 1024 nodes, and we execute training simultaneously until convergence which occurs after 18 steps of optimization. We do not introduce any explicit objective to the optimization, yet observe that the loss attains minimization when the domain manifests as a circle, as illustrated Figure 5. This outcome suggests that weakly imposed boundary conditions pose a distinctive challenge for PINNs in non-convex geometries. It shows how the parametrization of geometries via neural network can address an entirely new class problems, in particular, free boundary problems or shape optimization. 5
