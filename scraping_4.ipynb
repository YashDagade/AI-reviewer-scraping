{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Scraper for ICLR Papers Across Multiple Years via OpenReview\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import csv\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementNotInteractableException,\n",
    ")\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from pdfminer.high_level import extract_text\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import glob\n",
    "\n",
    "# %%\n",
    "# Setting up\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    filename=\"scraper.log\",\n",
    "    filemode=\"a\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# Also log to console\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "console.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://openreview.net\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; DataScraper/1.0; +https://yourdomain.com/)\"\n",
    "}\n",
    "BASE_DOWNLOAD_DIR = \"data/iclr\"  # Base directory for all years\n",
    "\n",
    "# Define metrics per year\n",
    "METRICS_BY_YEAR = {\n",
    "    \"2024\": [\"Soundness\", \"Presentation\", \"Contribution\", \"Rating\", \"Confidence\"],\n",
    "    \"2023\": [\"Correctness\", \"Technical Novelty And Significance\", \"Empirical Novelty And Significance\", \"Recommendation\", \"Confidence\"],\n",
    "    \"2022\": [\"Correctness\", \"Technical Novelty And Significance\", \"Empirical Novelty And Significance\", \"Recommendation\", \"Confidence\"],\n",
    "    \"2021\": [\"Rating\", \"Confidence\"]\n",
    "}\n",
    "\n",
    "# Ensure base directory exists\n",
    "os.makedirs(BASE_DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "def setup_selenium():\n",
    "    \"\"\"Sets up Selenium with headless Chrome.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    # Initialize Service with ChromeDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    \n",
    "    # Initialize WebDriver with Service and Options\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def fetch_html(driver, url, timeout=30):\n",
    "    \"\"\"Fetches the fully rendered HTML content of a given URL using Selenium.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Fetching URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until the main content is loaded\n",
    "        WebDriverWait(driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"note\"))\n",
    "        )\n",
    "\n",
    "        # Scroll to the bottom to ensure all lazy-loaded content is fetched\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for additional content to load\n",
    "\n",
    "        html = driver.page_source\n",
    "        logging.info(f\"Successfully fetched URL: {url}\")\n",
    "        return html\n",
    "    except TimeoutException:\n",
    "        logging.error(f\"Timeout while loading {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_paper_info(soup):\n",
    "    \"\"\"Parses the paper's BeautifulSoup object to extract metadata.\"\"\"\n",
    "    paper_info = {}\n",
    "\n",
    "    # Extract Title\n",
    "    title_tag = soup.find(\"h2\", class_=\"citation_title\")\n",
    "    paper_info[\"title\"] = title_tag.text.strip() if title_tag else \"N/A\"\n",
    "\n",
    "    # Extract Authors\n",
    "    authors_tag = soup.find(\"div\", class_=\"forum-authors\")\n",
    "    if authors_tag:\n",
    "        authors = [author.text.strip() for author in authors_tag.find_all(\"a\")]\n",
    "        paper_info[\"authors\"] = authors\n",
    "    else:\n",
    "        paper_info[\"authors\"] = []\n",
    "\n",
    "    # Extract Publication Date\n",
    "    pub_date_tag = soup.find(\"span\", class_=\"glyphicon-calendar\")\n",
    "    if pub_date_tag and pub_date_tag.parent:\n",
    "        dates_text = pub_date_tag.parent.text.strip()\n",
    "        publication_date = re.search(r\"Published:\\s*(.*?)(?:,|$)\", dates_text)\n",
    "        paper_info[\"publication_date\"] = (\n",
    "            publication_date.group(1) if publication_date else \"N/A\"\n",
    "        )\n",
    "    else:\n",
    "        paper_info[\"publication_date\"] = \"N/A\"\n",
    "\n",
    "    # Extract PDF URL\n",
    "    pdf_link_tag = soup.find(\"a\", class_=\"citation_pdf_url\")\n",
    "    if pdf_link_tag and \"href\" in pdf_link_tag.attrs:\n",
    "        pdf_url = pdf_link_tag[\"href\"]\n",
    "        if not pdf_url.startswith(\"http\"):\n",
    "            pdf_url = BASE_URL + pdf_url\n",
    "        paper_info[\"pdf_url\"] = pdf_url\n",
    "    else:\n",
    "        paper_info[\"pdf_url\"] = None\n",
    "\n",
    "    return paper_info\n",
    "\n",
    "def download_pdf(pdf_url, save_path):\n",
    "    \"\"\"Downloads the PDF from the given URL to the specified path.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Downloading PDF: {pdf_url}\")\n",
    "        response = requests.get(pdf_url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        logging.info(f\"Downloaded PDF to {save_path}\")\n",
    "        return True\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error downloading PDF from {pdf_url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_sections_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts the abstract and introduction from the PDF.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Extracting sections from PDF: {pdf_path}\")\n",
    "        text = extract_text(pdf_path)\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        # Improved regex patterns to accurately capture Abstract and Introduction\n",
    "        abstract_match = re.search(\n",
    "            r\"(?is)abstract\\s*(.*?)\\s*(?:(introduction|1\\.\\s*Introduction|2\\.\\s*Methods|methods|conclusion|related work|acknowledgments|references|$))\",\n",
    "            text\n",
    "        )\n",
    "        introduction_match = re.search(\n",
    "            r\"(?is)(introduction|1\\.\\s*Introduction)\\s*(.*?)\\s*(?:(conclusion|related work|methods|acknowledgments|references|2\\.\\s*Methods|$))\",\n",
    "            text\n",
    "        )\n",
    "\n",
    "        abstract = abstract_match.group(1).strip() if abstract_match else \"N/A\"\n",
    "        introduction = introduction_match.group(2).strip() if introduction_match else \"N/A\"\n",
    "\n",
    "        logging.info(f\"Extracted Abstract and Introduction from {pdf_path}\")\n",
    "        return abstract, introduction\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
    "        return \"N/A\", \"N/A\"\n",
    "\n",
    "def convert_to_markdown(text, header):\n",
    "    \"\"\"Converts plain text to Markdown with a specified header.\"\"\"\n",
    "    if text == \"N/A\":\n",
    "        markdown = f\"## {header}\\n\\nN/A\\n\"\n",
    "    else:\n",
    "        markdown = f\"## {header}\\n\\n{text}\\n\"\n",
    "    return markdown\n",
    "\n",
    "def save_markdown(content, filename):\n",
    "    \"\"\"Saves the given content to a Markdown file.\"\"\"\n",
    "    try:\n",
    "        path = os.path.join(filename)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "        logging.info(f\"Saved Markdown to {path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving Markdown file {filename}: {e}\")\n",
    "\n",
    "def extract_reviewer_responses(soup):\n",
    "    \"\"\"Extracts all notes (reviews, meta-reviews, decisions, comments) in order.\"\"\"\n",
    "    responses = []\n",
    "\n",
    "    # Find all notes (reviews, meta-reviews, decisions, comments)\n",
    "    note_divs = soup.find_all(\"div\", class_=\"note\", attrs={\"data-id\": re.compile(\".*\")})\n",
    "\n",
    "    for note in note_divs:\n",
    "        # Determine the type of note\n",
    "        invitation = note.find(\"span\", class_=\"invitation\")\n",
    "        if not invitation:\n",
    "            continue\n",
    "        invitation_type = invitation.text.strip()\n",
    "\n",
    "        # Extract author\n",
    "        signatures_span = note.find(\"span\", class_=\"signatures\")\n",
    "        author = \"N/A\"\n",
    "        if signatures_span:\n",
    "            author_tags = signatures_span.find_all(\"span\")\n",
    "            if author_tags:\n",
    "                author = author_tags[-1].text.strip()\n",
    "            else:\n",
    "                author = signatures_span.text.strip()\n",
    "\n",
    "        # Extract content fields\n",
    "        content_div = note.find(\"div\", class_=\"note-content\")\n",
    "        content_dict = {}\n",
    "        if content_div:\n",
    "            content_fields = content_div.find_all(\"div\", recursive=False)\n",
    "            for field in content_fields:\n",
    "                field_name_tag = field.find(\"strong\", class_=\"note-content-field\")\n",
    "                if not field_name_tag:\n",
    "                    continue\n",
    "                field_name = field_name_tag.text.strip(\":\").strip()\n",
    "                field_value_div = field.find(\"div\", class_=\"note-content-value\")\n",
    "                field_value_span = field.find(\"span\", class_=\"note-content-value\")\n",
    "                field_value = \"\"\n",
    "                if field_value_div:\n",
    "                    field_value = md(str(field_value_div)).strip()\n",
    "                elif field_value_span:\n",
    "                    field_value = md(str(field_value_span)).strip()\n",
    "                content_dict[field_name] = field_value\n",
    "\n",
    "        # Determine the note type based on presence of 'Soundness'\n",
    "        if \"Soundness\" in content_dict:\n",
    "            note_type = \"Official Review\"\n",
    "        else:\n",
    "            note_type = invitation_type  # Use the invitation type as the note type\n",
    "\n",
    "        # Append the note\n",
    "        responses.append(\n",
    "            {\"type\": note_type, \"author\": author, \"content\": content_dict}\n",
    "        )\n",
    "\n",
    "        # Handle nested comments (e.g., author responses)\n",
    "        nested_comments = note.find_all(\n",
    "            \"div\", class_=\"note\", attrs={\"data-id\": re.compile(\".*\")}\n",
    "        )\n",
    "        for comment in nested_comments:\n",
    "            comment_invitation = comment.find(\"span\", class_=\"invitation\")\n",
    "            if comment_invitation:\n",
    "                comment_type = comment_invitation.text.strip()\n",
    "                comment_author_span = comment.find(\"span\", class_=\"signatures\")\n",
    "                comment_author = \"N/A\"\n",
    "                if comment_author_span:\n",
    "                    author_tags = comment_author_span.find_all(\"span\")\n",
    "                    if author_tags:\n",
    "                        comment_author = author_tags[-1].text.strip()\n",
    "                    else:\n",
    "                        comment_author = comment_author_span.text.strip()\n",
    "\n",
    "                comment_content_div = comment.find(\"div\", class_=\"note-content\")\n",
    "                comment_content_dict = {}\n",
    "                if comment_content_div:\n",
    "                    content_fields = comment_content_div.find_all(\"div\", recursive=False)\n",
    "                    for field in content_fields:\n",
    "                        field_name_tag = field.find(\"strong\", class_=\"note-content-field\")\n",
    "                        if not field_name_tag:\n",
    "                            continue\n",
    "                        field_name = field_name_tag.text.strip(\":\").strip()\n",
    "                        field_value_div = field.find(\"div\", class_=\"note-content-value\")\n",
    "                        field_value_span = field.find(\"span\", class_=\"note-content-value\")\n",
    "                        field_value = \"\"\n",
    "                        if field_value_div:\n",
    "                            field_value = md(str(field_value_div)).strip()\n",
    "                        elif field_value_span:\n",
    "                            field_value = md(str(field_value_span)).strip()\n",
    "                        comment_content_dict[field_name] = field_value\n",
    "\n",
    "                responses.append(\n",
    "                    {\"type\": comment_type, \"author\": comment_author, \"content\": comment_content_dict}\n",
    "                )\n",
    "\n",
    "    return responses\n",
    "\n",
    "def save_reviewer_responses(responses, filename):\n",
    "    \"\"\"Saves reviewer responses to a Markdown file, maintaining the sequential order.\"\"\"\n",
    "    try:\n",
    "        content = f\"## Reviewer Responses\\n\\n\"\n",
    "        for idx, response in enumerate(responses, 1):\n",
    "            content += f\"### {response['type']} {idx}\\n\"\n",
    "            content += f\"**Author:** {response['author']}\\n\\n\"\n",
    "            for field_name, field_value in response[\"content\"].items():\n",
    "                content += f\"**{field_name}:**\\n{field_value}\\n\\n\"\n",
    "            content += \"\\n\"\n",
    "        save_markdown(content, filename)\n",
    "        logging.info(f\"Saved reviewer responses to {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving reviewer responses to {filename}: {e}\")\n",
    "\n",
    "def save_paper_metadata(paper_info, filename):\n",
    "    \"\"\"Saves paper metadata to a Markdown file.\"\"\"\n",
    "    try:\n",
    "        content = f\"# {paper_info['title']}\\n\\n\"\n",
    "        content += f\"**Authors:** {', '.join(paper_info['authors'])}\\n\\n\"\n",
    "        content += f\"**Publication Date:** {paper_info['publication_date']}\\n\\n\"\n",
    "        save_markdown(content, filename)\n",
    "        logging.info(f\"Saved metadata to {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving metadata to {filename}: {e}\")\n",
    "\n",
    "def scrape_paper(driver, year, paper_url):\n",
    "    \"\"\"Scrapes a single paper: downloads HTML and PDF.\"\"\"\n",
    "    logging.info(f\"Starting scraping for paper: {paper_url} (Year: {year})\")\n",
    "    html = fetch_html(driver, paper_url)\n",
    "    if not html:\n",
    "        logging.warning(f\"Failed to retrieve HTML for {paper_url}. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    paper_id_match = re.search(r\"id=(.+)\", paper_url)\n",
    "    paper_id = paper_id_match.group(1) if paper_id_match else \"unknown\"\n",
    "    \n",
    "    # Define directories for the year\n",
    "    year_download_dir = os.path.join(BASE_DOWNLOAD_DIR, f\"iclr_{year}\")\n",
    "    year_html_dir = os.path.join(year_download_dir, \"HTML\")\n",
    "    year_pdf_dir = os.path.join(year_download_dir, \"PDF\")\n",
    "    year_markdown_dir = os.path.join(year_download_dir, \"Markdown\")\n",
    "    year_images_dir = os.path.join(year_download_dir, \"Image\")\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    for directory in [year_download_dir, year_html_dir, year_pdf_dir, year_markdown_dir, year_images_dir]:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Save HTML\n",
    "    html_filename = os.path.join(year_html_dir, f\"{paper_id}.html\")\n",
    "    try:\n",
    "        with open(html_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html)\n",
    "        logging.info(f\"Saved HTML to {html_filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving HTML for {paper_url}: {e}\")\n",
    "    \n",
    "    # Parse paper info to get PDF URL\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    paper_info = parse_paper_info(soup)\n",
    "    \n",
    "    # Download PDF\n",
    "    if paper_info[\"pdf_url\"]:\n",
    "        pdf_filename = f\"{paper_id}.pdf\"\n",
    "        pdf_path = os.path.join(year_pdf_dir, pdf_filename)\n",
    "        success = download_pdf(paper_info[\"pdf_url\"], pdf_path)\n",
    "        if success:\n",
    "            logging.info(f\"Successfully scraped paper: {paper_id} (Year: {year})\")\n",
    "    else:\n",
    "        logging.warning(f\"No PDF URL found for {paper_url}.\")\n",
    "\n",
    "def parse_paper(paper_id, year):\n",
    "    \"\"\"Parses the scraped HTML and PDF to extract metadata, sections, and reviewer responses.\"\"\"\n",
    "    year_download_dir = os.path.join(BASE_DOWNLOAD_DIR, f\"iclr_{year}\")\n",
    "    html_filename = os.path.join(year_download_dir, \"HTML\", f\"{paper_id}.html\")\n",
    "    pdf_path = os.path.join(year_download_dir, \"PDF\", f\"{paper_id}.pdf\")\n",
    "    markdown_dir = os.path.join(year_download_dir, \"Markdown\")\n",
    "    \n",
    "    # Read HTML\n",
    "    try:\n",
    "        with open(html_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            html = f.read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading HTML file {html_filename}: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Parse paper info\n",
    "    paper_info = parse_paper_info(soup)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_filename = os.path.join(markdown_dir, f\"{paper_id}_metadata.md\")\n",
    "    save_paper_metadata(paper_info, metadata_filename)\n",
    "    \n",
    "    # Extract sections from PDF\n",
    "    if os.path.exists(pdf_path):\n",
    "        abstract, introduction = extract_sections_from_pdf(pdf_path)\n",
    "        abstract_md = convert_to_markdown(abstract, \"Abstract\")\n",
    "        introduction_md = convert_to_markdown(introduction, \"Introduction\")\n",
    "        combined_md = abstract_md + \"\\n\" + introduction_md\n",
    "        sections_filename = os.path.join(markdown_dir, f\"{paper_id}_sections.md\")\n",
    "        save_markdown(combined_md, sections_filename)\n",
    "    else:\n",
    "        logging.warning(f\"PDF not found for paper ID {paper_id}. Skipping section extraction.\")\n",
    "    \n",
    "    # Extract reviewer responses\n",
    "    responses = extract_reviewer_responses(soup)\n",
    "    if responses:\n",
    "        responses_filename = os.path.join(markdown_dir, f\"{paper_id}_responses.md\")\n",
    "        save_reviewer_responses(responses, responses_filename)\n",
    "    else:\n",
    "        logging.info(f\"No reviewer responses found for paper ID {paper_id}.\")\n",
    "    \n",
    "    logging.info(f\"Completed parsing for paper ID: {paper_id} (Year: {year})\")\n",
    "\n",
    "def aggregate_csv(csv_filename=\"decisions_and_scores.csv\"):\n",
    "    \"\"\"Aggregates decisions and scores from all *_responses.md files into a CSV.\"\"\"\n",
    "    csv_path = os.path.join(BASE_DOWNLOAD_DIR, csv_filename)\n",
    "    fieldnames = [\n",
    "        \"paperid\", \"title\", \"year\", \"decision\",\n",
    "        \"soundness\", \"presentation\", \"contribution\",\n",
    "        \"correctness\", \"technical_novelty_and_significance\",\n",
    "        \"empirical_novelty_and_significance\",\n",
    "        \"review_rating\", \"recommendation\",\n",
    "        \"confidence\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with open(csv_path, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            # Iterate over each year directory\n",
    "            for year in METRICS_BY_YEAR.keys():\n",
    "                year_download_dir = os.path.join(BASE_DOWNLOAD_DIR, f\"iclr_{year}\")\n",
    "                responses_files = glob.glob(os.path.join(year_download_dir, \"Markdown\", \"*_responses.md\"))\n",
    "                for resp_file in responses_files:\n",
    "                    paper_id = os.path.basename(resp_file).replace(\"_responses.md\", \"\")\n",
    "                    metadata_file = os.path.join(year_download_dir, \"Markdown\", f\"{paper_id}_metadata.md\")\n",
    "                    \n",
    "                    # Read metadata to get title\n",
    "                    try:\n",
    "                        with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                            metadata = f.read()\n",
    "                        title_match = re.search(r\"# (.+)\", metadata)\n",
    "                        title = title_match.group(1).strip() if title_match else \"N/A\"\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error reading metadata for {paper_id}: {e}\")\n",
    "                        title = \"N/A\"\n",
    "                    \n",
    "                    # Initialize metrics\n",
    "                    decision = \"N/A\"\n",
    "                    metrics = {\n",
    "                        \"soundness\": [],\n",
    "                        \"presentation\": [],\n",
    "                        \"contribution\": [],\n",
    "                        \"correctness\": [],\n",
    "                        \"technical_novelty_and_significance\": [],\n",
    "                        \"empirical_novelty_and_significance\": [],\n",
    "                        \"review_rating\": [],\n",
    "                        \"recommendation\": [],\n",
    "                        \"confidence\": []\n",
    "                    }\n",
    "                    \n",
    "                    # Read responses to get decision and reviews\n",
    "                    try:\n",
    "                        with open(resp_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                            responses_md = f.read()\n",
    "                        \n",
    "                        # Split the responses_md into sections based on headers\n",
    "                        sections = re.split(r\"^### \", responses_md, flags=re.MULTILINE)\n",
    "                        for section in sections:\n",
    "                            if not section.strip():\n",
    "                                continue\n",
    "                            header_match = re.match(r\"(\\w+.*?)\\n\", section)\n",
    "                            if header_match:\n",
    "                                header = header_match.group(1).strip()\n",
    "                                content = section[header_match.end():]\n",
    "                                if header.startswith(\"Decision\"):\n",
    "                                    # Extract decision\n",
    "                                    decision_match = re.search(r\"\\*\\*Decision:\\*\\*\\s*\\n*(.+?)(?:\\n\\n|\\Z)\", content, re.DOTALL)\n",
    "                                    decision = decision_match.group(1).strip() if decision_match else \"N/A\"\n",
    "                                elif header.startswith(\"Official Review\"):\n",
    "                                    # Extract review fields based on year\n",
    "                                    for metric in METRICS_BY_YEAR[year]:\n",
    "                                        metric_key = metric.lower().replace(\" \", \"_\").replace(\"and_\", \"_and_\")\n",
    "                                        pattern = rf\"\\*\\*{re.escape(metric)}:\\*\\*\\s*(\\d+)\"\n",
    "                                        match = re.search(pattern, content, re.IGNORECASE)\n",
    "                                        if match:\n",
    "                                            metrics[metric_key].append(match.group(1))\n",
    "                                        else:\n",
    "                                            metrics[metric_key].append(\"N/A\")\n",
    "                                else:\n",
    "                                    # Other types (Meta Review, Comments, etc.) are ignored for CSV\n",
    "                                    pass\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error reading responses for {paper_id}: {e}\")\n",
    "                    \n",
    "                    # Compile row data\n",
    "                    row = {\n",
    "                        \"paperid\": paper_id,\n",
    "                        \"title\": title,\n",
    "                        \"year\": year,\n",
    "                        \"decision\": decision,\n",
    "                        \"soundness\": str(metrics[\"soundness\"]) if metrics[\"soundness\"] else \"N/A\",\n",
    "                        \"presentation\": str(metrics[\"presentation\"]) if metrics[\"presentation\"] else \"N/A\",\n",
    "                        \"contribution\": str(metrics[\"contribution\"]) if metrics[\"contribution\"] else \"N/A\",\n",
    "                        \"correctness\": str(metrics[\"correctness\"]) if metrics[\"correctness\"] else \"N/A\",\n",
    "                        \"technical_novelty_and_significance\": str(metrics[\"technical_novelty_and_significance\"]) if metrics[\"technical_novelty_and_significance\"] else \"N/A\",\n",
    "                        \"empirical_novelty_and_significance\": str(metrics[\"empirical_novelty_and_significance\"]) if metrics[\"empirical_novelty_and_significance\"] else \"N/A\",\n",
    "                        \"review_rating\": str(metrics[\"review_rating\"]) if metrics[\"review_rating\"] else \"N/A\",\n",
    "                        \"recommendation\": str(metrics[\"recommendation\"]) if metrics[\"recommendation\"] else \"N/A\",\n",
    "                        \"confidence\": str(metrics[\"confidence\"]) if metrics[\"confidence\"] else \"N/A\"\n",
    "                    }\n",
    "                    \n",
    "                    # Write to CSV\n",
    "                    writer.writerow(row)\n",
    "        \n",
    "        logging.info(f\"Aggregated CSV saved to {csv_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating CSV file {csv_path}: {e}\")\n",
    "\n",
    "def process_papers_parallel_scrape(papers, max_workers=4):\n",
    "    \"\"\"Processes multiple papers in parallel for scraping.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for year, url in papers:\n",
    "            driver = setup_selenium()\n",
    "            future = executor.submit(scrape_paper, driver, year, url)\n",
    "            futures.append((future, driver))\n",
    "\n",
    "        for future, driver in futures:\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping a paper: {e}\")\n",
    "            finally:\n",
    "                driver.quit()\n",
    "\n",
    "def parse_paper_wrapper(papers, max_workers=4):\n",
    "    \"\"\"Parses all scraped papers in parallel.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for year, url in papers:\n",
    "            paper_id_match = re.search(r\"id=(.+)\", url)\n",
    "            paper_id = paper_id_match.group(1) if paper_id_match else \"unknown\"\n",
    "            year_download_dir = os.path.join(BASE_DOWNLOAD_DIR, f\"iclr_{year}\")\n",
    "            future = executor.submit(parse_paper, paper_id, year)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error parsing a paper: {e}\")\n",
    "\n",
    "def get_paper_urls_from_page(driver, page_url):\n",
    "    \"\"\"Extract all unique paper URLs from the given OpenReview page.\"\"\"\n",
    "    driver.get(page_url)\n",
    "    time.sleep(3)  # Give some time for page to load (adjust as necessary)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    paper_links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "    # Filter out URLs with '&noteId=' and ensure they contain 'forum?id='\n",
    "    paper_urls = [\n",
    "        link[\"href\"]\n",
    "        for link in paper_links\n",
    "        if \"forum?id=\" in link[\"href\"] and \"&noteId=\" not in link[\"href\"]\n",
    "    ]\n",
    "\n",
    "    return paper_urls\n",
    "\n",
    "def switch_to_tab_with_js(driver, tab_id):\n",
    "    \"\"\"Use JavaScript to switch to the desired tab to avoid click interception.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Switching to {tab_id} tab using JavaScript...\")\n",
    "        driver.execute_script(f\"document.querySelector('a[href=\\\"#{tab_id}\\\"]').click();\")\n",
    "        time.sleep(5)  # Allow time for the page to update after clicking the tab\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to switch to {tab_id} tab: {e}\")\n",
    "\n",
    "def scrape_all_pages(driver, year, base_url, tab_id):\n",
    "    \"\"\"Scrapes all pages from a single tab.\"\"\"\n",
    "    switch_to_tab_with_js(driver, tab_id)\n",
    "    \n",
    "    all_paper_urls = set()\n",
    "    page_count = 1\n",
    "\n",
    "    while True:\n",
    "        logging.info(f\"Scraping page {page_count} of {tab_id} tab for year {year}...\")\n",
    "        paper_urls = get_paper_urls_from_page(driver, base_url)\n",
    "        all_paper_urls.update(paper_urls)\n",
    "        \n",
    "        if not go_to_next_page(driver):  # Stop if there are no more pages\n",
    "            break\n",
    "        \n",
    "        page_count += 1\n",
    "\n",
    "    return all_paper_urls\n",
    "\n",
    "def go_to_next_page(driver):\n",
    "    try:\n",
    "        # Look for the \"Next\" button in the pagination section and click it\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, 'li.right-arrow > a'))\n",
    "        )\n",
    "        next_button.click()\n",
    "        time.sleep(3)  # Allow some time for the next page to load\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.info(\"No more pages to navigate.\")\n",
    "        return False\n",
    "\n",
    "def scrape_multiple_tabs(year, base_url, tabs):\n",
    "    \"\"\"Scrapes multiple tabs for a given year.\"\"\"\n",
    "    driver = setup_selenium()\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)  # Give the page some time to load\n",
    "\n",
    "    combined_paper_urls = set()\n",
    "    \n",
    "    for tab_id in tabs:\n",
    "        logging.info(f\"Scraping {tab_id} tab for year {year}...\")\n",
    "        paper_urls = scrape_all_pages(driver, year, base_url, tab_id)\n",
    "        combined_paper_urls.update(paper_urls)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    # Return a list of (year, formatted_url)\n",
    "    formatted_urls = [f\"https://openreview.net{url}\" for url in combined_paper_urls]\n",
    "    return [(year, url) for url in formatted_urls]\n",
    "\n",
    "def save_urls_to_file(filename, urls):\n",
    "    \"\"\"Saves the paper URLs to a text file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w') as f:\n",
    "            for url in urls:\n",
    "                f.write(f\"{url}\\n\")\n",
    "        logging.info(f\"Saved {len(urls)} URLs to {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving URLs to file {filename}: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate scraping and aggregation.\"\"\"\n",
    "    # Define the groups per year with their respective tabs\n",
    "    groups = [\n",
    "        {\n",
    "            \"year\": \"2024\",\n",
    "            \"base_url\": \"https://openreview.net/group?id=ICLR.cc/2024/Conference\",\n",
    "            \"tabs\": [\"accept-oral\", \"accept-spotlight\", \"accept-poster\", \"reject\"]\n",
    "        },\n",
    "        {\n",
    "            \"year\": \"2023\",\n",
    "            \"base_url\": \"https://openreview.net/group?id=ICLR.cc/2023/Conference\",\n",
    "            \"tabs\": [\"notable-top-25-\", \"poster\", \"submitted\"]\n",
    "        },\n",
    "        {\n",
    "            \"year\": \"2022\",\n",
    "            \"base_url\": \"https://openreview.net/group?id=ICLR.cc/2022/Conference\",\n",
    "            \"tabs\": [\"spotlight-submissions\", \"poster-submissions\", \"submitted-submissions\"]\n",
    "        },\n",
    "        {\n",
    "            \"year\": \"2021\",\n",
    "            \"base_url\": \"https://openreview.net/group?id=ICLR.cc/2021/Conference\",\n",
    "            \"tabs\": [\"submitted-submissions\", \"spotlight-presentations\", \"poster-presentations\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    all_papers = []\n",
    "\n",
    "    # Scrape paper URLs for each group\n",
    "    for group in groups:\n",
    "        year = group[\"year\"]\n",
    "        base_url = group[\"base_url\"]\n",
    "        tabs = group[\"tabs\"]\n",
    "        logging.info(f\"Starting URL scraping for year {year}...\")\n",
    "        papers = scrape_multiple_tabs(year, base_url, tabs)\n",
    "        all_papers.extend(papers)\n",
    "        save_urls_to_file(f\"{year}_paper_urls.txt\", [url for _, url in papers])\n",
    "        logging.info(f\"Completed URL scraping for year {year}.\")\n",
    "\n",
    "    logging.info(f\"Total papers to scrape: {len(all_papers)}\")\n",
    "\n",
    "    # Scrape all papers in parallel\n",
    "    logging.info(\"Starting paper scraping...\")\n",
    "    process_papers_parallel_scrape(all_papers, max_workers=8)\n",
    "    logging.info(\"Completed paper scraping.\")\n",
    "\n",
    "    # Parse all papers in parallel\n",
    "    logging.info(\"Starting paper parsing...\")\n",
    "    parse_paper_wrapper(all_papers, max_workers=8)\n",
    "    logging.info(\"Completed paper parsing.\")\n",
    "\n",
    "    # Aggregate all data into CSV\n",
    "    logging.info(\"Starting CSV aggregation...\")\n",
    "    aggregate_csv()\n",
    "    logging.info(\"Completed CSV aggregation.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
